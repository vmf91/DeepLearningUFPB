{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparando conjunto de treino e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields_full = ['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']\n",
    "fields_train = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']\n",
    "fields_test = ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processDataset(df):\n",
    "    # Insere os valores faltando em Age\n",
    "    mean_age = df['Age'].mean()\n",
    "    std_age = df['Age'].std() # desvio padrão\n",
    "\n",
    "    min_value = mean_age - std_age\n",
    "    max_value = mean_age + std_age\n",
    "\n",
    "    size = df['Age'].isnull().sum()\n",
    "    df.loc[df['Age'].isnull(), 'Age'] = np.random.randint(min_value, max_value, size=size)\n",
    "    df['Age'] = df['Age'].astype(int)\n",
    "\n",
    "    # Insere os valores faltando em Embarked\n",
    "    embarked_values = ['C', 'Q', 'S']\n",
    "    size = df['Embarked'].isnull().sum()\n",
    "    random_values = [embarked_values[i] for i in np.random.randint(0, 3, size=size)]\n",
    "    df.loc[df['Embarked'].isnull(), 'Embarked'] = random_values\n",
    "    \n",
    "    df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "    \n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'famale': 1})\n",
    "\n",
    "    # Faz o one hot encode em Sex, Embaked, SibSp e Parch, Pclass\n",
    "    # o  parâmetro'categories' garante que mesmo que o dado esteja faltando será considerado\n",
    "    sex = df['Sex'].astype(CategoricalDtype(categories=[0, 1]))\n",
    "    embarked = df['Embarked'].astype(CategoricalDtype(categories=[0, 1, 2]))\n",
    "    parch = df['Parch'].astype(CategoricalDtype(categories=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "    sibsp = df['SibSp'].astype(CategoricalDtype(categories=[0, 1, 2, 3, 4, 5, 6, 7, 8]))\n",
    "    pclass = df['Pclass'].astype(CategoricalDtype(categories=[1, 2, 3]))\n",
    "    \n",
    "    res = pd.concat([df,pd.get_dummies(sex, prefix='Sex')], axis=1)\n",
    "    res = pd.concat([res,pd.get_dummies(parch, prefix='Parch')], axis=1)\n",
    "    res = pd.concat([res,pd.get_dummies(sibsp, prefix='SibSp')], axis=1)\n",
    "    res = pd.concat([res,pd.get_dummies(embarked, prefix='Embarked')], axis=1)\n",
    "    res = pd.concat([res,pd.get_dummies(pclass, prefix='Pclass')], axis=1)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def clearColumns(df, cols):\n",
    "    df.drop(cols, inplace=True, axis='columns')\n",
    "    return df\n",
    "\n",
    "def addSurvivedCol(df_test, df_full):\n",
    "    survived = []\n",
    "    \n",
    "    names = df_test['Name']\n",
    "    \n",
    "    for name in names:\n",
    "        line = df_full.loc[df_full['Name'] == name]\n",
    "        survived.append(line['Survived'])\n",
    "    \n",
    "    df_test['Survived'] = survived\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "def correlation(df, fields, objective):\n",
    "    correlation_list = {}\n",
    "    for col in fields:\n",
    "        correl = np.corrcoef(df[col], df[objective])\n",
    "        correlation_list[col] = correl[0][1]\n",
    "\n",
    "    correlation_list = sorted(correlation_list.items(), key=lambda x:x[1], reverse=True)\n",
    "    for i in range(len(correlation_list)):\n",
    "        print(str(correlation_list[i][0]) + ': ' + str(correlation_list[i][1]))\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    one_hot = np.zeros([len(x), 2])\n",
    "    for i in enumerate(x):\n",
    "        one_hot[(i[0], i[1])] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('dataset.csv', usecols=fields_full)\n",
    "df_train = pd.read_csv('train.csv', usecols=fields_train)\n",
    "df_test = pd.read_csv('test.csv', usecols=fields_test)\n",
    "\n",
    "df_test = addSurvivedCol(df_test, df_full)\n",
    "\n",
    "df_train = processDataset(df_train)\n",
    "df_test = processDataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: nan\n",
      "Survived: 1.0\n",
      "Parch: 0.08162940708348372\n",
      "SibSp: -0.035322498885735645\n",
      "Age: -0.05872897651173179\n",
      "Embarked: -0.1676753138677212\n",
      "Pclass: -0.33848103596101475\n"
     ]
    }
   ],
   "source": [
    "correlation(df_train, fields_train, 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = clearColumns(df_train, ['Sex', 'Embarked', 'Pclass', 'Parch', 'SibSp'])\n",
    "df_test = clearColumns(df_test, ['Sex', 'Name', 'Embarked', 'Pclass', 'Parch', 'SibSp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separa os dados\n",
    "data_X = df_train.drop(['Survived'], inplace=False, axis='columns').values.tolist()\n",
    "data_y = df_train['Survived'].values.tolist()\n",
    "\n",
    "data_X, data_y = shuffle(data_X, data_y, random_state=0)\n",
    "\n",
    "data_y = one_hot_encode(data_y)\n",
    "\n",
    "train_X, val_X = data_X[:800], data_X[800:]\n",
    "train_y, val_y = data_y[:800], data_y[800:]\n",
    "\n",
    "test_X = df_test.drop(['Survived'], inplace=False, axis='columns').values.tolist()\n",
    "test_y = df_test['Survived'].values.tolist()\n",
    "test_y = one_hot_encode(test_y)\n",
    "\n",
    "# Converte tudo pra NpArray\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "val_X = np.array(val_X)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "# exibe os dados do dataframe\n",
    "#print(df_train.head(10))\n",
    "\n",
    "# exibe os dados para treino\n",
    "#for i in range(2):\n",
    "    #print(train_X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 0 | Batch 0 | Train Loss: 1835.8243 | Validation Loss: 147.45451\n",
      "Epoch 1 | Batch 0 | Train Loss: 1322.666 | Validation Loss: 173.45422\n",
      "Epoch 2 | Batch 0 | Train Loss: 1182.425 | Validation Loss: 164.11896\n",
      "Epoch 3 | Batch 0 | Train Loss: 1194.6082 | Validation Loss: 131.54875\n",
      "Epoch 4 | Batch 0 | Train Loss: 1302.7336 | Validation Loss: 168.70319\n",
      "Epoch 5 | Batch 0 | Train Loss: 1283.8256 | Validation Loss: 190.69809\n",
      "Epoch 6 | Batch 0 | Train Loss: 965.92053 | Validation Loss: 112.20842\n",
      "Epoch 7 | Batch 0 | Train Loss: 1118.678 | Validation Loss: 97.530945\n",
      "Epoch 8 | Batch 0 | Train Loss: 1131.5442 | Validation Loss: 169.80016\n",
      "Epoch 9 | Batch 0 | Train Loss: 1004.70953 | Validation Loss: 159.18777\n",
      "Epoch 10 | Batch 0 | Train Loss: 932.26587 | Validation Loss: 125.640686\n",
      "Epoch 11 | Batch 0 | Train Loss: 900.48615 | Validation Loss: 161.85599\n",
      "Epoch 12 | Batch 0 | Train Loss: 1009.4071 | Validation Loss: 105.30832\n",
      "Epoch 13 | Batch 0 | Train Loss: 838.15393 | Validation Loss: 127.277725\n",
      "Epoch 14 | Batch 0 | Train Loss: 906.7062 | Validation Loss: 117.55484\n",
      "Epoch 15 | Batch 0 | Train Loss: 950.25586 | Validation Loss: 93.943665\n",
      "Epoch 16 | Batch 0 | Train Loss: 911.2654 | Validation Loss: 78.13425\n",
      "Epoch 17 | Batch 0 | Train Loss: 868.1864 | Validation Loss: 127.05493\n",
      "Epoch 18 | Batch 0 | Train Loss: 817.2488 | Validation Loss: 88.04788\n",
      "Epoch 19 | Batch 0 | Train Loss: 796.8683 | Validation Loss: 77.16614\n",
      "Epoch 20 | Batch 0 | Train Loss: 678.5443 | Validation Loss: 97.7569\n",
      "Epoch 21 | Batch 0 | Train Loss: 653.4116 | Validation Loss: 90.53178\n",
      "Epoch 22 | Batch 0 | Train Loss: 669.9054 | Validation Loss: 94.10747\n",
      "Epoch 23 | Batch 0 | Train Loss: 648.1931 | Validation Loss: 66.12753\n",
      "Epoch 24 | Batch 0 | Train Loss: 591.1039 | Validation Loss: 75.962135\n",
      "Epoch 25 | Batch 0 | Train Loss: 516.49866 | Validation Loss: 62.111366\n",
      "Epoch 26 | Batch 0 | Train Loss: 564.41077 | Validation Loss: 64.59063\n",
      "Epoch 27 | Batch 0 | Train Loss: 512.2471 | Validation Loss: 77.249214\n",
      "Epoch 28 | Batch 0 | Train Loss: 477.8429 | Validation Loss: 64.25225\n",
      "Epoch 29 | Batch 0 | Train Loss: 465.19537 | Validation Loss: 60.853146\n",
      "Epoch 30 | Batch 0 | Train Loss: 431.3584 | Validation Loss: 68.05825\n",
      "Epoch 31 | Batch 0 | Train Loss: 480.18433 | Validation Loss: 62.753456\n",
      "Epoch 32 | Batch 0 | Train Loss: 409.0251 | Validation Loss: 49.212097\n",
      "Epoch 33 | Batch 0 | Train Loss: 414.03705 | Validation Loss: 46.023674\n",
      "Epoch 34 | Batch 0 | Train Loss: 402.80457 | Validation Loss: 43.112892\n",
      "Epoch 35 | Batch 0 | Train Loss: 373.65686 | Validation Loss: 51.79979\n",
      "Epoch 36 | Batch 0 | Train Loss: 391.06873 | Validation Loss: 47.400925\n",
      "Epoch 37 | Batch 0 | Train Loss: 355.26947 | Validation Loss: 42.12008\n",
      "Epoch 38 | Batch 0 | Train Loss: 325.64813 | Validation Loss: 43.62624\n",
      "Epoch 39 | Batch 0 | Train Loss: 312.78082 | Validation Loss: 44.9551\n",
      "Epoch 40 | Batch 0 | Train Loss: 309.81146 | Validation Loss: 44.63776\n",
      "Epoch 41 | Batch 0 | Train Loss: 316.64706 | Validation Loss: 46.763725\n",
      "Epoch 42 | Batch 0 | Train Loss: 290.49326 | Validation Loss: 40.745945\n",
      "Epoch 43 | Batch 0 | Train Loss: 295.49384 | Validation Loss: 31.829641\n",
      "Epoch 44 | Batch 0 | Train Loss: 274.96954 | Validation Loss: 34.615505\n",
      "Epoch 45 | Batch 0 | Train Loss: 266.417 | Validation Loss: 36.975403\n",
      "Epoch 46 | Batch 0 | Train Loss: 266.18524 | Validation Loss: 32.292923\n",
      "Epoch 47 | Batch 0 | Train Loss: 251.52457 | Validation Loss: 28.167904\n",
      "Epoch 48 | Batch 0 | Train Loss: 244.48093 | Validation Loss: 33.46149\n",
      "Epoch 49 | Batch 0 | Train Loss: 242.63176 | Validation Loss: 29.644241\n",
      "Epoch 50 | Batch 0 | Train Loss: 227.06711 | Validation Loss: 28.433979\n",
      "Epoch 51 | Batch 0 | Train Loss: 221.55244 | Validation Loss: 29.546196\n",
      "Epoch 52 | Batch 0 | Train Loss: 223.14462 | Validation Loss: 31.281109\n",
      "Epoch 53 | Batch 0 | Train Loss: 237.16644 | Validation Loss: 24.074554\n",
      "Epoch 54 | Batch 0 | Train Loss: 212.17166 | Validation Loss: 26.802307\n",
      "Epoch 55 | Batch 0 | Train Loss: 224.8605 | Validation Loss: 23.329605\n",
      "Epoch 56 | Batch 0 | Train Loss: 228.46317 | Validation Loss: 25.277096\n",
      "Epoch 57 | Batch 0 | Train Loss: 218.99411 | Validation Loss: 27.220703\n",
      "Epoch 58 | Batch 0 | Train Loss: 218.27316 | Validation Loss: 24.644423\n",
      "Epoch 59 | Batch 0 | Train Loss: 217.64038 | Validation Loss: 27.726274\n",
      "Epoch 60 | Batch 0 | Train Loss: 214.0614 | Validation Loss: 30.85228\n",
      "Epoch 61 | Batch 0 | Train Loss: 207.40704 | Validation Loss: 23.58434\n",
      "Epoch 62 | Batch 0 | Train Loss: 210.78882 | Validation Loss: 27.607313\n",
      "Epoch 63 | Batch 0 | Train Loss: 204.4614 | Validation Loss: 26.919912\n",
      "Epoch 64 | Batch 0 | Train Loss: 214.18677 | Validation Loss: 25.815468\n",
      "Epoch 65 | Batch 0 | Train Loss: 202.27298 | Validation Loss: 25.578005\n",
      "Epoch 66 | Batch 0 | Train Loss: 217.2781 | Validation Loss: 27.78568\n",
      "Epoch 67 | Batch 0 | Train Loss: 211.7197 | Validation Loss: 23.99242\n",
      "Epoch 68 | Batch 0 | Train Loss: 209.97777 | Validation Loss: 25.326595\n",
      "Epoch 69 | Batch 0 | Train Loss: 196.11575 | Validation Loss: 28.126984\n",
      "Epoch 70 | Batch 0 | Train Loss: 192.60654 | Validation Loss: 25.294355\n",
      "Epoch 71 | Batch 0 | Train Loss: 194.08743 | Validation Loss: 23.701893\n",
      "Epoch 72 | Batch 0 | Train Loss: 196.09216 | Validation Loss: 28.020668\n",
      "Epoch 73 | Batch 0 | Train Loss: 201.23169 | Validation Loss: 23.159674\n",
      "Epoch 74 | Batch 0 | Train Loss: 198.31552 | Validation Loss: 24.845951\n",
      "Epoch 75 | Batch 0 | Train Loss: 192.8609 | Validation Loss: 28.96735\n",
      "Epoch 76 | Batch 0 | Train Loss: 200.04669 | Validation Loss: 24.765621\n",
      "Epoch 77 | Batch 0 | Train Loss: 195.12003 | Validation Loss: 24.789139\n",
      "Epoch 78 | Batch 0 | Train Loss: 202.38542 | Validation Loss: 23.57196\n",
      "Epoch 79 | Batch 0 | Train Loss: 206.39093 | Validation Loss: 22.388844\n",
      "Epoch 80 | Batch 0 | Train Loss: 198.04248 | Validation Loss: 23.457851\n",
      "Epoch 81 | Batch 0 | Train Loss: 183.60754 | Validation Loss: 25.977163\n",
      "Epoch 82 | Batch 0 | Train Loss: 191.0729 | Validation Loss: 26.717594\n",
      "Epoch 83 | Batch 0 | Train Loss: 201.6865 | Validation Loss: 23.421589\n",
      "Epoch 84 | Batch 0 | Train Loss: 196.17813 | Validation Loss: 28.250656\n",
      "Epoch 85 | Batch 0 | Train Loss: 198.8618 | Validation Loss: 22.69123\n",
      "Epoch 86 | Batch 0 | Train Loss: 195.11955 | Validation Loss: 22.12513\n",
      "Epoch 87 | Batch 0 | Train Loss: 184.0032 | Validation Loss: 25.381784\n",
      "Epoch 88 | Batch 0 | Train Loss: 189.55757 | Validation Loss: 26.275158\n",
      "Epoch 89 | Batch 0 | Train Loss: 183.85095 | Validation Loss: 22.570768\n",
      "Epoch 90 | Batch 0 | Train Loss: 184.29997 | Validation Loss: 23.827805\n",
      "Epoch 91 | Batch 0 | Train Loss: 189.14383 | Validation Loss: 26.59769\n",
      "Epoch 92 | Batch 0 | Train Loss: 180.47815 | Validation Loss: 21.90621\n",
      "Epoch 93 | Batch 0 | Train Loss: 190.08769 | Validation Loss: 25.73949\n",
      "Epoch 94 | Batch 0 | Train Loss: 182.00269 | Validation Loss: 23.349394\n",
      "Epoch 95 | Batch 0 | Train Loss: 183.47787 | Validation Loss: 22.504162\n",
      "Epoch 96 | Batch 0 | Train Loss: 185.00923 | Validation Loss: 22.240086\n",
      "Epoch 97 | Batch 0 | Train Loss: 185.35883 | Validation Loss: 24.071903\n",
      "Epoch 98 | Batch 0 | Train Loss: 188.5734 | Validation Loss: 24.117031\n",
      "Epoch 99 | Batch 0 | Train Loss: 181.35886 | Validation Loss: 26.55293\n",
      "Epoch 100 | Batch 0 | Train Loss: 180.107 | Validation Loss: 24.36924\n",
      "Epoch 101 | Batch 0 | Train Loss: 195.23277 | Validation Loss: 25.00784\n",
      "Epoch 102 | Batch 0 | Train Loss: 186.72278 | Validation Loss: 23.40386\n",
      "Epoch 103 | Batch 0 | Train Loss: 187.65656 | Validation Loss: 23.417412\n",
      "Epoch 104 | Batch 0 | Train Loss: 189.19104 | Validation Loss: 18.304466\n",
      "Epoch 105 | Batch 0 | Train Loss: 181.35226 | Validation Loss: 23.072216\n",
      "Epoch 106 | Batch 0 | Train Loss: 179.6047 | Validation Loss: 25.894997\n",
      "Epoch 107 | Batch 0 | Train Loss: 188.72586 | Validation Loss: 20.896479\n",
      "Epoch 108 | Batch 0 | Train Loss: 177.63174 | Validation Loss: 23.654099\n",
      "Epoch 109 | Batch 0 | Train Loss: 173.93124 | Validation Loss: 22.69879\n",
      "Epoch 110 | Batch 0 | Train Loss: 184.22014 | Validation Loss: 21.959652\n",
      "Epoch 111 | Batch 0 | Train Loss: 182.18448 | Validation Loss: 21.731485\n",
      "Epoch 112 | Batch 0 | Train Loss: 183.6084 | Validation Loss: 25.238197\n",
      "Epoch 113 | Batch 0 | Train Loss: 177.1708 | Validation Loss: 22.964962\n",
      "Epoch 114 | Batch 0 | Train Loss: 180.04059 | Validation Loss: 27.193035\n",
      "Epoch 115 | Batch 0 | Train Loss: 180.98932 | Validation Loss: 21.50995\n",
      "Epoch 116 | Batch 0 | Train Loss: 179.48749 | Validation Loss: 23.684534\n",
      "Epoch 117 | Batch 0 | Train Loss: 182.37732 | Validation Loss: 22.864902\n",
      "Epoch 118 | Batch 0 | Train Loss: 176.43887 | Validation Loss: 23.438982\n",
      "Epoch 119 | Batch 0 | Train Loss: 186.22672 | Validation Loss: 21.774702\n",
      "Epoch 120 | Batch 0 | Train Loss: 184.59242 | Validation Loss: 20.514902\n",
      "Epoch 121 | Batch 0 | Train Loss: 175.45258 | Validation Loss: 22.669094\n",
      "Epoch 122 | Batch 0 | Train Loss: 172.54405 | Validation Loss: 21.164753\n",
      "Epoch 123 | Batch 0 | Train Loss: 175.12553 | Validation Loss: 21.403555\n",
      "Epoch 124 | Batch 0 | Train Loss: 179.78336 | Validation Loss: 20.519281\n",
      "Epoch 125 | Batch 0 | Train Loss: 174.20831 | Validation Loss: 22.603294\n",
      "Epoch 126 | Batch 0 | Train Loss: 177.10953 | Validation Loss: 22.109535\n",
      "Epoch 127 | Batch 0 | Train Loss: 173.82303 | Validation Loss: 19.326183\n",
      "Epoch 128 | Batch 0 | Train Loss: 174.26672 | Validation Loss: 26.418354\n",
      "Epoch 129 | Batch 0 | Train Loss: 177.15979 | Validation Loss: 23.916113\n",
      "Epoch 130 | Batch 0 | Train Loss: 174.0296 | Validation Loss: 20.31429\n",
      "Epoch 131 | Batch 0 | Train Loss: 178.33986 | Validation Loss: 23.162157\n",
      "Epoch 132 | Batch 0 | Train Loss: 176.72052 | Validation Loss: 25.865725\n",
      "Epoch 133 | Batch 0 | Train Loss: 172.7521 | Validation Loss: 24.445133\n",
      "Epoch 134 | Batch 0 | Train Loss: 176.10248 | Validation Loss: 21.764534\n",
      "Epoch 135 | Batch 0 | Train Loss: 174.2681 | Validation Loss: 20.746418\n",
      "Epoch 136 | Batch 0 | Train Loss: 175.25319 | Validation Loss: 23.369102\n",
      "Epoch 137 | Batch 0 | Train Loss: 174.42184 | Validation Loss: 23.072227\n",
      "Epoch 138 | Batch 0 | Train Loss: 175.61337 | Validation Loss: 25.639591\n",
      "Epoch 139 | Batch 0 | Train Loss: 171.67116 | Validation Loss: 21.801407\n",
      "Epoch 140 | Batch 0 | Train Loss: 171.82361 | Validation Loss: 22.874012\n",
      "Epoch 141 | Batch 0 | Train Loss: 167.36076 | Validation Loss: 22.939312\n",
      "Epoch 142 | Batch 0 | Train Loss: 174.75153 | Validation Loss: 23.252039\n",
      "Epoch 143 | Batch 0 | Train Loss: 174.52179 | Validation Loss: 20.580667\n",
      "Epoch 144 | Batch 0 | Train Loss: 174.33209 | Validation Loss: 22.229834\n",
      "Epoch 145 | Batch 0 | Train Loss: 175.35635 | Validation Loss: 22.21868\n",
      "Epoch 146 | Batch 0 | Train Loss: 176.17531 | Validation Loss: 24.776123\n",
      "Epoch 147 | Batch 0 | Train Loss: 170.48643 | Validation Loss: 22.99185\n",
      "Epoch 148 | Batch 0 | Train Loss: 164.25635 | Validation Loss: 23.747831\n",
      "Epoch 149 | Batch 0 | Train Loss: 167.77394 | Validation Loss: 26.093483\n",
      "Epoch 150 | Batch 0 | Train Loss: 165.55754 | Validation Loss: 23.759604\n",
      "Epoch 151 | Batch 0 | Train Loss: 169.77042 | Validation Loss: 20.588522\n",
      "Epoch 152 | Batch 0 | Train Loss: 169.11615 | Validation Loss: 25.869434\n",
      "Epoch 153 | Batch 0 | Train Loss: 173.94754 | Validation Loss: 22.18188\n",
      "Epoch 154 | Batch 0 | Train Loss: 175.1706 | Validation Loss: 24.512693\n",
      "Epoch 155 | Batch 0 | Train Loss: 172.3746 | Validation Loss: 22.823189\n",
      "Epoch 156 | Batch 0 | Train Loss: 168.52832 | Validation Loss: 25.010918\n",
      "Epoch 157 | Batch 0 | Train Loss: 174.75925 | Validation Loss: 22.71962\n",
      "Epoch 158 | Batch 0 | Train Loss: 167.5668 | Validation Loss: 23.481806\n",
      "Epoch 159 | Batch 0 | Train Loss: 174.09244 | Validation Loss: 24.312962\n",
      "Epoch 160 | Batch 0 | Train Loss: 169.01636 | Validation Loss: 22.827396\n",
      "Epoch 161 | Batch 0 | Train Loss: 171.74675 | Validation Loss: 22.800442\n",
      "Epoch 162 | Batch 0 | Train Loss: 173.78165 | Validation Loss: 23.717146\n",
      "Epoch 163 | Batch 0 | Train Loss: 167.48627 | Validation Loss: 22.099167\n",
      "Epoch 164 | Batch 0 | Train Loss: 168.55618 | Validation Loss: 20.742386\n",
      "Epoch 165 | Batch 0 | Train Loss: 171.09183 | Validation Loss: 24.22424\n",
      "Epoch 166 | Batch 0 | Train Loss: 173.77571 | Validation Loss: 22.741085\n",
      "Epoch 167 | Batch 0 | Train Loss: 172.7811 | Validation Loss: 25.11987\n",
      "Epoch 168 | Batch 0 | Train Loss: 166.21713 | Validation Loss: 24.75256\n",
      "Epoch 169 | Batch 0 | Train Loss: 170.2348 | Validation Loss: 23.52317\n",
      "Epoch 170 | Batch 0 | Train Loss: 173.18242 | Validation Loss: 19.896301\n",
      "Epoch 171 | Batch 0 | Train Loss: 170.58894 | Validation Loss: 22.508553\n",
      "Epoch 172 | Batch 0 | Train Loss: 165.61276 | Validation Loss: 22.937809\n",
      "Epoch 173 | Batch 0 | Train Loss: 168.94617 | Validation Loss: 21.856651\n",
      "Epoch 174 | Batch 0 | Train Loss: 168.48132 | Validation Loss: 22.342176\n",
      "Epoch 175 | Batch 0 | Train Loss: 174.05057 | Validation Loss: 21.881012\n",
      "Epoch 176 | Batch 0 | Train Loss: 169.54047 | Validation Loss: 22.144913\n",
      "Epoch 177 | Batch 0 | Train Loss: 164.63966 | Validation Loss: 19.81152\n",
      "Epoch 178 | Batch 0 | Train Loss: 168.33066 | Validation Loss: 25.02705\n",
      "Epoch 179 | Batch 0 | Train Loss: 169.8884 | Validation Loss: 24.491922\n",
      "Epoch 180 | Batch 0 | Train Loss: 171.22461 | Validation Loss: 22.758953\n",
      "Epoch 181 | Batch 0 | Train Loss: 165.66711 | Validation Loss: 24.198027\n",
      "Epoch 182 | Batch 0 | Train Loss: 166.53429 | Validation Loss: 26.84985\n",
      "Epoch 183 | Batch 0 | Train Loss: 172.93506 | Validation Loss: 22.941252\n",
      "Epoch 184 | Batch 0 | Train Loss: 169.24731 | Validation Loss: 23.149569\n",
      "Epoch 185 | Batch 0 | Train Loss: 166.03876 | Validation Loss: 21.733944\n",
      "Epoch 186 | Batch 0 | Train Loss: 163.5145 | Validation Loss: 21.746325\n",
      "Epoch 187 | Batch 0 | Train Loss: 175.84033 | Validation Loss: 24.577393\n",
      "Epoch 188 | Batch 0 | Train Loss: 173.73715 | Validation Loss: 22.714258\n",
      "Epoch 189 | Batch 0 | Train Loss: 166.86218 | Validation Loss: 22.277159\n",
      "Epoch 190 | Batch 0 | Train Loss: 166.10966 | Validation Loss: 24.869213\n",
      "Epoch 191 | Batch 0 | Train Loss: 170.72699 | Validation Loss: 25.099371\n",
      "Epoch 192 | Batch 0 | Train Loss: 167.562 | Validation Loss: 22.741516\n",
      "Epoch 193 | Batch 0 | Train Loss: 164.63998 | Validation Loss: 24.344604\n",
      "Epoch 194 | Batch 0 | Train Loss: 166.28452 | Validation Loss: 20.5375\n",
      "Epoch 195 | Batch 0 | Train Loss: 164.86801 | Validation Loss: 20.549578\n",
      "Epoch 196 | Batch 0 | Train Loss: 165.92947 | Validation Loss: 25.432972\n",
      "Epoch 197 | Batch 0 | Train Loss: 166.23526 | Validation Loss: 20.979649\n",
      "Epoch 198 | Batch 0 | Train Loss: 165.20764 | Validation Loss: 23.918415\n",
      "Epoch 199 | Batch 0 | Train Loss: 164.60901 | Validation Loss: 25.398731\n",
      "Epoch 200 | Batch 0 | Train Loss: 161.395 | Validation Loss: 24.582058\n",
      "Epoch 201 | Batch 0 | Train Loss: 162.68109 | Validation Loss: 20.838108\n",
      "Epoch 202 | Batch 0 | Train Loss: 166.83646 | Validation Loss: 23.257843\n",
      "Epoch 203 | Batch 0 | Train Loss: 166.99411 | Validation Loss: 24.046307\n",
      "Epoch 204 | Batch 0 | Train Loss: 163.8771 | Validation Loss: 21.516869\n",
      "Epoch 205 | Batch 0 | Train Loss: 167.87567 | Validation Loss: 21.976557\n",
      "Epoch 206 | Batch 0 | Train Loss: 167.03818 | Validation Loss: 22.633057\n",
      "Epoch 207 | Batch 0 | Train Loss: 167.81743 | Validation Loss: 21.387724\n",
      "Epoch 208 | Batch 0 | Train Loss: 164.85522 | Validation Loss: 25.347095\n",
      "Epoch 209 | Batch 0 | Train Loss: 166.6343 | Validation Loss: 23.49096\n",
      "Epoch 210 | Batch 0 | Train Loss: 163.73958 | Validation Loss: 24.114876\n",
      "Epoch 211 | Batch 0 | Train Loss: 166.42752 | Validation Loss: 22.572226\n",
      "Epoch 212 | Batch 0 | Train Loss: 161.91399 | Validation Loss: 23.460297\n",
      "Epoch 213 | Batch 0 | Train Loss: 165.26337 | Validation Loss: 23.642593\n",
      "Epoch 214 | Batch 0 | Train Loss: 166.76118 | Validation Loss: 23.247417\n",
      "Epoch 215 | Batch 0 | Train Loss: 172.55641 | Validation Loss: 23.252676\n",
      "Epoch 216 | Batch 0 | Train Loss: 164.01785 | Validation Loss: 20.461615\n",
      "Epoch 217 | Batch 0 | Train Loss: 163.78023 | Validation Loss: 21.643879\n",
      "Epoch 218 | Batch 0 | Train Loss: 169.05986 | Validation Loss: 21.38675\n",
      "Epoch 219 | Batch 0 | Train Loss: 169.51978 | Validation Loss: 27.554495\n",
      "Epoch 220 | Batch 0 | Train Loss: 162.37596 | Validation Loss: 22.743338\n",
      "Epoch 221 | Batch 0 | Train Loss: 163.86273 | Validation Loss: 22.0509\n",
      "Epoch 222 | Batch 0 | Train Loss: 163.44952 | Validation Loss: 22.265976\n",
      "Epoch 223 | Batch 0 | Train Loss: 162.23894 | Validation Loss: 21.470572\n",
      "Epoch 224 | Batch 0 | Train Loss: 168.36232 | Validation Loss: 23.189579\n",
      "Epoch 225 | Batch 0 | Train Loss: 160.03392 | Validation Loss: 22.709164\n",
      "Epoch 226 | Batch 0 | Train Loss: 159.79758 | Validation Loss: 24.089928\n",
      "Epoch 227 | Batch 0 | Train Loss: 162.7881 | Validation Loss: 23.323435\n",
      "Epoch 228 | Batch 0 | Train Loss: 165.37733 | Validation Loss: 23.491116\n",
      "Epoch 229 | Batch 0 | Train Loss: 170.49597 | Validation Loss: 21.115288\n",
      "Epoch 230 | Batch 0 | Train Loss: 163.30219 | Validation Loss: 21.644308\n",
      "Epoch 231 | Batch 0 | Train Loss: 162.70657 | Validation Loss: 20.582657\n",
      "Epoch 232 | Batch 0 | Train Loss: 164.4578 | Validation Loss: 22.418768\n",
      "Epoch 233 | Batch 0 | Train Loss: 175.20218 | Validation Loss: 24.53041\n",
      "Epoch 234 | Batch 0 | Train Loss: 160.63211 | Validation Loss: 23.049389\n",
      "Epoch 235 | Batch 0 | Train Loss: 159.6727 | Validation Loss: 24.400324\n",
      "Epoch 236 | Batch 0 | Train Loss: 163.55582 | Validation Loss: 23.678791\n",
      "Epoch 237 | Batch 0 | Train Loss: 169.05365 | Validation Loss: 28.157452\n",
      "Epoch 238 | Batch 0 | Train Loss: 165.31311 | Validation Loss: 21.534222\n",
      "Epoch 239 | Batch 0 | Train Loss: 157.9047 | Validation Loss: 23.170391\n",
      "Epoch 240 | Batch 0 | Train Loss: 165.22314 | Validation Loss: 21.214249\n",
      "Epoch 241 | Batch 0 | Train Loss: 159.72531 | Validation Loss: 19.921362\n",
      "Epoch 242 | Batch 0 | Train Loss: 162.76447 | Validation Loss: 25.10193\n",
      "Epoch 243 | Batch 0 | Train Loss: 154.95444 | Validation Loss: 19.531399\n",
      "Epoch 244 | Batch 0 | Train Loss: 161.86774 | Validation Loss: 23.604214\n",
      "Epoch 245 | Batch 0 | Train Loss: 162.72925 | Validation Loss: 20.943602\n",
      "Epoch 246 | Batch 0 | Train Loss: 158.69022 | Validation Loss: 21.526955\n",
      "Epoch 247 | Batch 0 | Train Loss: 162.77399 | Validation Loss: 23.390682\n",
      "Epoch 248 | Batch 0 | Train Loss: 163.52332 | Validation Loss: 24.320246\n",
      "Epoch 249 | Batch 0 | Train Loss: 167.52533 | Validation Loss: 23.689838\n",
      "Epoch 250 | Batch 0 | Train Loss: 160.72055 | Validation Loss: 23.108934\n",
      "Epoch 251 | Batch 0 | Train Loss: 172.98277 | Validation Loss: 24.42057\n",
      "Epoch 252 | Batch 0 | Train Loss: 164.55757 | Validation Loss: 25.728111\n",
      "Epoch 253 | Batch 0 | Train Loss: 159.01772 | Validation Loss: 24.079506\n",
      "Epoch 254 | Batch 0 | Train Loss: 163.01083 | Validation Loss: 21.57671\n",
      "Epoch 255 | Batch 0 | Train Loss: 163.97678 | Validation Loss: 21.584534\n",
      "Epoch 256 | Batch 0 | Train Loss: 156.92343 | Validation Loss: 24.586433\n",
      "Epoch 257 | Batch 0 | Train Loss: 163.61362 | Validation Loss: 24.623043\n",
      "Epoch 258 | Batch 0 | Train Loss: 163.5178 | Validation Loss: 22.535522\n",
      "Epoch 259 | Batch 0 | Train Loss: 159.33656 | Validation Loss: 20.868874\n",
      "Epoch 260 | Batch 0 | Train Loss: 161.26822 | Validation Loss: 22.579702\n",
      "Epoch 261 | Batch 0 | Train Loss: 162.81946 | Validation Loss: 26.146423\n",
      "Epoch 262 | Batch 0 | Train Loss: 163.7905 | Validation Loss: 24.863455\n",
      "Epoch 263 | Batch 0 | Train Loss: 164.43646 | Validation Loss: 20.594042\n",
      "Epoch 264 | Batch 0 | Train Loss: 157.2651 | Validation Loss: 25.529154\n",
      "Epoch 265 | Batch 0 | Train Loss: 163.78561 | Validation Loss: 21.21451\n",
      "Epoch 266 | Batch 0 | Train Loss: 163.55737 | Validation Loss: 23.720959\n",
      "Epoch 267 | Batch 0 | Train Loss: 167.21803 | Validation Loss: 24.76316\n",
      "Epoch 268 | Batch 0 | Train Loss: 159.1106 | Validation Loss: 23.030275\n",
      "Epoch 269 | Batch 0 | Train Loss: 156.42638 | Validation Loss: 21.413998\n",
      "Epoch 270 | Batch 0 | Train Loss: 163.03517 | Validation Loss: 23.10849\n",
      "Epoch 271 | Batch 0 | Train Loss: 160.2722 | Validation Loss: 25.264933\n",
      "Epoch 272 | Batch 0 | Train Loss: 163.4314 | Validation Loss: 22.497597\n",
      "Epoch 273 | Batch 0 | Train Loss: 167.75455 | Validation Loss: 26.087624\n",
      "Epoch 274 | Batch 0 | Train Loss: 160.151 | Validation Loss: 24.485806\n",
      "Epoch 275 | Batch 0 | Train Loss: 154.76146 | Validation Loss: 20.018597\n",
      "Epoch 276 | Batch 0 | Train Loss: 154.96814 | Validation Loss: 21.809885\n",
      "Epoch 277 | Batch 0 | Train Loss: 163.75835 | Validation Loss: 22.735872\n",
      "Epoch 278 | Batch 0 | Train Loss: 163.20932 | Validation Loss: 22.059101\n",
      "Epoch 279 | Batch 0 | Train Loss: 160.65918 | Validation Loss: 25.296053\n",
      "Epoch 280 | Batch 0 | Train Loss: 164.0359 | Validation Loss: 26.372353\n",
      "Epoch 281 | Batch 0 | Train Loss: 163.13795 | Validation Loss: 21.850754\n",
      "Epoch 282 | Batch 0 | Train Loss: 161.46378 | Validation Loss: 24.80945\n",
      "Epoch 283 | Batch 0 | Train Loss: 162.64243 | Validation Loss: 23.729439\n",
      "Epoch 284 | Batch 0 | Train Loss: 163.31168 | Validation Loss: 21.74557\n",
      "Epoch 285 | Batch 0 | Train Loss: 158.10309 | Validation Loss: 25.450693\n",
      "Epoch 286 | Batch 0 | Train Loss: 156.37021 | Validation Loss: 25.413998\n",
      "Epoch 287 | Batch 0 | Train Loss: 158.96841 | Validation Loss: 22.27591\n",
      "Epoch 288 | Batch 0 | Train Loss: 154.55615 | Validation Loss: 25.843042\n",
      "Epoch 289 | Batch 0 | Train Loss: 163.3889 | Validation Loss: 20.943289\n",
      "Epoch 290 | Batch 0 | Train Loss: 161.19995 | Validation Loss: 25.502678\n",
      "Epoch 291 | Batch 0 | Train Loss: 157.69284 | Validation Loss: 21.649834\n",
      "Epoch 292 | Batch 0 | Train Loss: 162.71823 | Validation Loss: 23.661118\n",
      "Epoch 293 | Batch 0 | Train Loss: 159.25468 | Validation Loss: 26.225864\n",
      "Epoch 294 | Batch 0 | Train Loss: 157.99835 | Validation Loss: 27.210865\n",
      "Epoch 295 | Batch 0 | Train Loss: 156.6188 | Validation Loss: 25.101358\n",
      "Epoch 296 | Batch 0 | Train Loss: 160.45676 | Validation Loss: 22.460123\n",
      "Epoch 297 | Batch 0 | Train Loss: 158.06189 | Validation Loss: 26.116142\n",
      "Epoch 298 | Batch 0 | Train Loss: 157.65172 | Validation Loss: 26.137882\n",
      "Epoch 299 | Batch 0 | Train Loss: 157.84966 | Validation Loss: 22.460262\n",
      "Epoch 300 | Batch 0 | Train Loss: 164.2028 | Validation Loss: 22.995113\n",
      "Epoch 301 | Batch 0 | Train Loss: 166.96399 | Validation Loss: 20.899658\n",
      "Epoch 302 | Batch 0 | Train Loss: 158.55292 | Validation Loss: 25.42078\n",
      "Epoch 303 | Batch 0 | Train Loss: 157.78105 | Validation Loss: 21.457329\n",
      "Epoch 304 | Batch 0 | Train Loss: 163.43384 | Validation Loss: 22.901987\n",
      "Epoch 305 | Batch 0 | Train Loss: 159.37311 | Validation Loss: 19.55343\n",
      "Epoch 306 | Batch 0 | Train Loss: 155.61383 | Validation Loss: 22.449165\n",
      "Epoch 307 | Batch 0 | Train Loss: 159.22545 | Validation Loss: 25.1398\n",
      "Epoch 308 | Batch 0 | Train Loss: 160.45215 | Validation Loss: 23.199705\n",
      "Epoch 309 | Batch 0 | Train Loss: 157.51126 | Validation Loss: 23.796711\n",
      "Epoch 310 | Batch 0 | Train Loss: 161.51285 | Validation Loss: 22.511831\n",
      "Epoch 311 | Batch 0 | Train Loss: 164.8173 | Validation Loss: 24.14419\n",
      "Epoch 312 | Batch 0 | Train Loss: 157.98373 | Validation Loss: 24.62492\n",
      "Epoch 313 | Batch 0 | Train Loss: 164.11313 | Validation Loss: 21.17453\n",
      "Epoch 314 | Batch 0 | Train Loss: 158.60912 | Validation Loss: 21.838022\n",
      "Epoch 315 | Batch 0 | Train Loss: 158.61542 | Validation Loss: 24.07681\n",
      "Epoch 316 | Batch 0 | Train Loss: 160.23816 | Validation Loss: 21.082155\n",
      "Epoch 317 | Batch 0 | Train Loss: 159.24112 | Validation Loss: 23.61947\n",
      "Epoch 318 | Batch 0 | Train Loss: 157.57202 | Validation Loss: 19.516708\n",
      "Epoch 319 | Batch 0 | Train Loss: 150.97934 | Validation Loss: 26.24749\n",
      "Epoch 320 | Batch 0 | Train Loss: 164.8154 | Validation Loss: 24.512772\n",
      "Epoch 321 | Batch 0 | Train Loss: 160.5845 | Validation Loss: 23.76477\n",
      "Epoch 322 | Batch 0 | Train Loss: 164.00848 | Validation Loss: 22.356289\n",
      "Epoch 323 | Batch 0 | Train Loss: 160.91956 | Validation Loss: 21.85298\n",
      "Epoch 324 | Batch 0 | Train Loss: 152.35808 | Validation Loss: 19.55369\n",
      "Epoch 325 | Batch 0 | Train Loss: 156.07178 | Validation Loss: 19.806122\n",
      "Epoch 326 | Batch 0 | Train Loss: 159.08392 | Validation Loss: 24.989576\n",
      "Epoch 327 | Batch 0 | Train Loss: 158.77292 | Validation Loss: 21.864695\n",
      "Epoch 328 | Batch 0 | Train Loss: 156.96521 | Validation Loss: 21.60292\n",
      "Epoch 329 | Batch 0 | Train Loss: 159.04498 | Validation Loss: 27.449326\n",
      "Epoch 330 | Batch 0 | Train Loss: 159.50871 | Validation Loss: 21.999008\n",
      "Epoch 331 | Batch 0 | Train Loss: 157.92825 | Validation Loss: 22.003435\n",
      "Epoch 332 | Batch 0 | Train Loss: 157.99913 | Validation Loss: 24.934803\n",
      "Epoch 333 | Batch 0 | Train Loss: 161.92628 | Validation Loss: 21.352346\n",
      "Epoch 334 | Batch 0 | Train Loss: 159.72122 | Validation Loss: 24.38391\n",
      "Epoch 335 | Batch 0 | Train Loss: 155.914 | Validation Loss: 24.315493\n",
      "Epoch 336 | Batch 0 | Train Loss: 158.57693 | Validation Loss: 21.160831\n",
      "Epoch 337 | Batch 0 | Train Loss: 161.87881 | Validation Loss: 24.175644\n",
      "Epoch 338 | Batch 0 | Train Loss: 156.13995 | Validation Loss: 21.98534\n",
      "Epoch 339 | Batch 0 | Train Loss: 171.47464 | Validation Loss: 23.980734\n",
      "Epoch 340 | Batch 0 | Train Loss: 157.67755 | Validation Loss: 22.961689\n",
      "Epoch 341 | Batch 0 | Train Loss: 157.58354 | Validation Loss: 24.265823\n",
      "Epoch 342 | Batch 0 | Train Loss: 160.37265 | Validation Loss: 24.56783\n",
      "Epoch 343 | Batch 0 | Train Loss: 161.41527 | Validation Loss: 23.75019\n",
      "Epoch 344 | Batch 0 | Train Loss: 163.4336 | Validation Loss: 25.863636\n",
      "Epoch 345 | Batch 0 | Train Loss: 160.05098 | Validation Loss: 23.170656\n",
      "Epoch 346 | Batch 0 | Train Loss: 153.10265 | Validation Loss: 24.831245\n",
      "Epoch 347 | Batch 0 | Train Loss: 162.23627 | Validation Loss: 24.769611\n",
      "Epoch 348 | Batch 0 | Train Loss: 157.71725 | Validation Loss: 24.484596\n",
      "Epoch 349 | Batch 0 | Train Loss: 156.51553 | Validation Loss: 26.090641\n",
      "Epoch 350 | Batch 0 | Train Loss: 156.97461 | Validation Loss: 23.193096\n",
      "Epoch 351 | Batch 0 | Train Loss: 158.82428 | Validation Loss: 21.278421\n",
      "Epoch 352 | Batch 0 | Train Loss: 155.24347 | Validation Loss: 22.891838\n",
      "Epoch 353 | Batch 0 | Train Loss: 166.1608 | Validation Loss: 22.929733\n",
      "Epoch 354 | Batch 0 | Train Loss: 153.54001 | Validation Loss: 21.321266\n",
      "Epoch 355 | Batch 0 | Train Loss: 160.73576 | Validation Loss: 24.781115\n",
      "Epoch 356 | Batch 0 | Train Loss: 155.15327 | Validation Loss: 24.87244\n",
      "Epoch 357 | Batch 0 | Train Loss: 158.55661 | Validation Loss: 22.06004\n",
      "Epoch 358 | Batch 0 | Train Loss: 156.8779 | Validation Loss: 25.07072\n",
      "Epoch 359 | Batch 0 | Train Loss: 152.24028 | Validation Loss: 22.694662\n",
      "Epoch 360 | Batch 0 | Train Loss: 161.44626 | Validation Loss: 26.376514\n",
      "Epoch 361 | Batch 0 | Train Loss: 157.12785 | Validation Loss: 21.108192\n",
      "Epoch 362 | Batch 0 | Train Loss: 158.58191 | Validation Loss: 22.917152\n",
      "Epoch 363 | Batch 0 | Train Loss: 158.36609 | Validation Loss: 23.958237\n",
      "Epoch 364 | Batch 0 | Train Loss: 151.2735 | Validation Loss: 26.878677\n",
      "Epoch 365 | Batch 0 | Train Loss: 163.3525 | Validation Loss: 23.89067\n",
      "Epoch 366 | Batch 0 | Train Loss: 156.92496 | Validation Loss: 23.061596\n",
      "Epoch 367 | Batch 0 | Train Loss: 161.99533 | Validation Loss: 27.100595\n",
      "Epoch 368 | Batch 0 | Train Loss: 154.4434 | Validation Loss: 22.21717\n",
      "Epoch 369 | Batch 0 | Train Loss: 154.3796 | Validation Loss: 26.72762\n",
      "Epoch 370 | Batch 0 | Train Loss: 155.78679 | Validation Loss: 26.405903\n",
      "Epoch 371 | Batch 0 | Train Loss: 162.47044 | Validation Loss: 24.113974\n",
      "Epoch 372 | Batch 0 | Train Loss: 168.22568 | Validation Loss: 24.541216\n",
      "Epoch 373 | Batch 0 | Train Loss: 163.60643 | Validation Loss: 22.635403\n",
      "Epoch 374 | Batch 0 | Train Loss: 156.81844 | Validation Loss: 23.273912\n",
      "Epoch 375 | Batch 0 | Train Loss: 156.46823 | Validation Loss: 26.011122\n",
      "Epoch 376 | Batch 0 | Train Loss: 153.40308 | Validation Loss: 28.357174\n",
      "Epoch 377 | Batch 0 | Train Loss: 159.02551 | Validation Loss: 27.902773\n",
      "Epoch 378 | Batch 0 | Train Loss: 156.07458 | Validation Loss: 21.26741\n",
      "Epoch 379 | Batch 0 | Train Loss: 162.32994 | Validation Loss: 22.270742\n",
      "Epoch 380 | Batch 0 | Train Loss: 163.18518 | Validation Loss: 26.999435\n",
      "Epoch 381 | Batch 0 | Train Loss: 155.34996 | Validation Loss: 28.041595\n",
      "Epoch 382 | Batch 0 | Train Loss: 157.37877 | Validation Loss: 26.278843\n",
      "Epoch 383 | Batch 0 | Train Loss: 158.03337 | Validation Loss: 27.025246\n",
      "Epoch 384 | Batch 0 | Train Loss: 156.37025 | Validation Loss: 21.37674\n",
      "Epoch 385 | Batch 0 | Train Loss: 156.88928 | Validation Loss: 22.50497\n",
      "Epoch 386 | Batch 0 | Train Loss: 157.63286 | Validation Loss: 23.741253\n",
      "Epoch 387 | Batch 0 | Train Loss: 161.12701 | Validation Loss: 26.200588\n",
      "Epoch 388 | Batch 0 | Train Loss: 160.91191 | Validation Loss: 22.01516\n",
      "Epoch 389 | Batch 0 | Train Loss: 158.46799 | Validation Loss: 27.604626\n",
      "Epoch 390 | Batch 0 | Train Loss: 160.97182 | Validation Loss: 21.807983\n",
      "Epoch 391 | Batch 0 | Train Loss: 158.04408 | Validation Loss: 26.922195\n",
      "Epoch 392 | Batch 0 | Train Loss: 161.75638 | Validation Loss: 22.651672\n",
      "Epoch 393 | Batch 0 | Train Loss: 155.47894 | Validation Loss: 26.828753\n",
      "Epoch 394 | Batch 0 | Train Loss: 155.34872 | Validation Loss: 26.437672\n",
      "Epoch 395 | Batch 0 | Train Loss: 158.01602 | Validation Loss: 22.087122\n",
      "Epoch 396 | Batch 0 | Train Loss: 160.50984 | Validation Loss: 19.346165\n",
      "Epoch 397 | Batch 0 | Train Loss: 158.26619 | Validation Loss: 23.872917\n",
      "Epoch 398 | Batch 0 | Train Loss: 153.68706 | Validation Loss: 24.962479\n",
      "Epoch 399 | Batch 0 | Train Loss: 156.99321 | Validation Loss: 24.398245\n",
      "Epoch 400 | Batch 0 | Train Loss: 153.10231 | Validation Loss: 25.431255\n",
      "Epoch 401 | Batch 0 | Train Loss: 155.37543 | Validation Loss: 25.66523\n",
      "Epoch 402 | Batch 0 | Train Loss: 158.30353 | Validation Loss: 22.873138\n",
      "Epoch 403 | Batch 0 | Train Loss: 160.10522 | Validation Loss: 23.519043\n",
      "Epoch 404 | Batch 0 | Train Loss: 151.8425 | Validation Loss: 24.661009\n",
      "Epoch 405 | Batch 0 | Train Loss: 153.3057 | Validation Loss: 28.619736\n",
      "Epoch 406 | Batch 0 | Train Loss: 158.84831 | Validation Loss: 23.183092\n",
      "Epoch 407 | Batch 0 | Train Loss: 159.44054 | Validation Loss: 21.305803\n",
      "Epoch 408 | Batch 0 | Train Loss: 153.32428 | Validation Loss: 21.746414\n",
      "Epoch 409 | Batch 0 | Train Loss: 150.21194 | Validation Loss: 25.5822\n",
      "Epoch 410 | Batch 0 | Train Loss: 153.39113 | Validation Loss: 25.02886\n",
      "Epoch 411 | Batch 0 | Train Loss: 159.95317 | Validation Loss: 21.908836\n",
      "Epoch 412 | Batch 0 | Train Loss: 159.10599 | Validation Loss: 23.892464\n",
      "Epoch 413 | Batch 0 | Train Loss: 158.08356 | Validation Loss: 24.84724\n",
      "Epoch 414 | Batch 0 | Train Loss: 162.281 | Validation Loss: 26.049652\n",
      "Epoch 415 | Batch 0 | Train Loss: 161.32898 | Validation Loss: 23.70911\n",
      "Epoch 416 | Batch 0 | Train Loss: 165.4379 | Validation Loss: 27.857586\n",
      "Epoch 417 | Batch 0 | Train Loss: 159.35118 | Validation Loss: 23.687784\n",
      "Epoch 418 | Batch 0 | Train Loss: 154.24902 | Validation Loss: 24.87857\n",
      "Epoch 419 | Batch 0 | Train Loss: 156.86096 | Validation Loss: 21.71479\n",
      "Epoch 420 | Batch 0 | Train Loss: 156.6457 | Validation Loss: 21.186382\n",
      "Epoch 421 | Batch 0 | Train Loss: 156.73729 | Validation Loss: 22.407883\n",
      "Epoch 422 | Batch 0 | Train Loss: 156.60928 | Validation Loss: 23.228012\n",
      "Epoch 423 | Batch 0 | Train Loss: 155.37427 | Validation Loss: 26.607635\n",
      "Epoch 424 | Batch 0 | Train Loss: 158.40668 | Validation Loss: 25.612064\n",
      "Epoch 425 | Batch 0 | Train Loss: 160.57657 | Validation Loss: 22.4231\n",
      "Epoch 426 | Batch 0 | Train Loss: 154.14328 | Validation Loss: 24.021503\n",
      "Epoch 427 | Batch 0 | Train Loss: 156.92198 | Validation Loss: 25.487082\n",
      "Epoch 428 | Batch 0 | Train Loss: 156.42114 | Validation Loss: 22.310612\n",
      "Epoch 429 | Batch 0 | Train Loss: 157.381 | Validation Loss: 23.710075\n",
      "Epoch 430 | Batch 0 | Train Loss: 159.24408 | Validation Loss: 24.843504\n",
      "Epoch 431 | Batch 0 | Train Loss: 153.51006 | Validation Loss: 23.586306\n",
      "Epoch 432 | Batch 0 | Train Loss: 151.5715 | Validation Loss: 25.256536\n",
      "Epoch 433 | Batch 0 | Train Loss: 155.45628 | Validation Loss: 28.011162\n",
      "Epoch 434 | Batch 0 | Train Loss: 152.88882 | Validation Loss: 23.708204\n",
      "Epoch 435 | Batch 0 | Train Loss: 157.37234 | Validation Loss: 21.772966\n",
      "Epoch 436 | Batch 0 | Train Loss: 147.39105 | Validation Loss: 21.266233\n",
      "Epoch 437 | Batch 0 | Train Loss: 156.21295 | Validation Loss: 23.502808\n",
      "Epoch 438 | Batch 0 | Train Loss: 157.70877 | Validation Loss: 24.95544\n",
      "Epoch 439 | Batch 0 | Train Loss: 150.27126 | Validation Loss: 27.624336\n",
      "Epoch 440 | Batch 0 | Train Loss: 159.6319 | Validation Loss: 24.782866\n",
      "Epoch 441 | Batch 0 | Train Loss: 158.66704 | Validation Loss: 27.006699\n",
      "Epoch 442 | Batch 0 | Train Loss: 161.73526 | Validation Loss: 25.29587\n",
      "Epoch 443 | Batch 0 | Train Loss: 153.46304 | Validation Loss: 29.448336\n",
      "Epoch 444 | Batch 0 | Train Loss: 155.49115 | Validation Loss: 24.150509\n",
      "Epoch 445 | Batch 0 | Train Loss: 155.31665 | Validation Loss: 27.491642\n",
      "Epoch 446 | Batch 0 | Train Loss: 156.98474 | Validation Loss: 22.387648\n",
      "Epoch 447 | Batch 0 | Train Loss: 153.24414 | Validation Loss: 23.586657\n",
      "Epoch 448 | Batch 0 | Train Loss: 154.38147 | Validation Loss: 26.836735\n",
      "Epoch 449 | Batch 0 | Train Loss: 157.94446 | Validation Loss: 23.089375\n",
      "Epoch 450 | Batch 0 | Train Loss: 157.20485 | Validation Loss: 21.799538\n",
      "Epoch 451 | Batch 0 | Train Loss: 152.14539 | Validation Loss: 26.962461\n",
      "Epoch 452 | Batch 0 | Train Loss: 156.4514 | Validation Loss: 27.400927\n",
      "Epoch 453 | Batch 0 | Train Loss: 157.3435 | Validation Loss: 25.64474\n",
      "Epoch 454 | Batch 0 | Train Loss: 158.97636 | Validation Loss: 26.042685\n",
      "Epoch 455 | Batch 0 | Train Loss: 157.16959 | Validation Loss: 24.370426\n",
      "Epoch 456 | Batch 0 | Train Loss: 155.70886 | Validation Loss: 24.887177\n",
      "Epoch 457 | Batch 0 | Train Loss: 157.02335 | Validation Loss: 25.185421\n",
      "Epoch 458 | Batch 0 | Train Loss: 154.66382 | Validation Loss: 25.842445\n",
      "Epoch 459 | Batch 0 | Train Loss: 157.97327 | Validation Loss: 22.87701\n",
      "Epoch 460 | Batch 0 | Train Loss: 156.86192 | Validation Loss: 20.544987\n",
      "Epoch 461 | Batch 0 | Train Loss: 154.09117 | Validation Loss: 24.605803\n",
      "Epoch 462 | Batch 0 | Train Loss: 155.25664 | Validation Loss: 28.534023\n",
      "Epoch 463 | Batch 0 | Train Loss: 158.20291 | Validation Loss: 26.007767\n",
      "Epoch 464 | Batch 0 | Train Loss: 155.81772 | Validation Loss: 21.846828\n",
      "Epoch 465 | Batch 0 | Train Loss: 156.70416 | Validation Loss: 25.219933\n",
      "Epoch 466 | Batch 0 | Train Loss: 157.74997 | Validation Loss: 26.332172\n",
      "Epoch 467 | Batch 0 | Train Loss: 151.69922 | Validation Loss: 25.946627\n",
      "Epoch 468 | Batch 0 | Train Loss: 153.20988 | Validation Loss: 25.981445\n",
      "Epoch 469 | Batch 0 | Train Loss: 154.65732 | Validation Loss: 27.797373\n",
      "Epoch 470 | Batch 0 | Train Loss: 158.42642 | Validation Loss: 27.029694\n",
      "Epoch 471 | Batch 0 | Train Loss: 153.36209 | Validation Loss: 21.85347\n",
      "Epoch 472 | Batch 0 | Train Loss: 152.88834 | Validation Loss: 24.977604\n",
      "Epoch 473 | Batch 0 | Train Loss: 159.11975 | Validation Loss: 20.338634\n",
      "Epoch 474 | Batch 0 | Train Loss: 150.67853 | Validation Loss: 23.258259\n",
      "Epoch 475 | Batch 0 | Train Loss: 153.0704 | Validation Loss: 25.01678\n",
      "Epoch 476 | Batch 0 | Train Loss: 155.46194 | Validation Loss: 26.548851\n",
      "Epoch 477 | Batch 0 | Train Loss: 158.24976 | Validation Loss: 22.236303\n",
      "Epoch 478 | Batch 0 | Train Loss: 161.66327 | Validation Loss: 26.31849\n",
      "Epoch 479 | Batch 0 | Train Loss: 152.70415 | Validation Loss: 23.237022\n",
      "Epoch 480 | Batch 0 | Train Loss: 154.40097 | Validation Loss: 24.668278\n",
      "Epoch 481 | Batch 0 | Train Loss: 158.0479 | Validation Loss: 24.2755\n",
      "Epoch 482 | Batch 0 | Train Loss: 153.64473 | Validation Loss: 24.179588\n",
      "Epoch 483 | Batch 0 | Train Loss: 149.69632 | Validation Loss: 24.481133\n",
      "Epoch 484 | Batch 0 | Train Loss: 150.47589 | Validation Loss: 26.136106\n",
      "Epoch 485 | Batch 0 | Train Loss: 152.24637 | Validation Loss: 27.66806\n",
      "Epoch 486 | Batch 0 | Train Loss: 156.1218 | Validation Loss: 24.014824\n",
      "Epoch 487 | Batch 0 | Train Loss: 160.83582 | Validation Loss: 23.13203\n",
      "Epoch 488 | Batch 0 | Train Loss: 155.43718 | Validation Loss: 23.603222\n",
      "Epoch 489 | Batch 0 | Train Loss: 157.14879 | Validation Loss: 28.280788\n",
      "Epoch 490 | Batch 0 | Train Loss: 152.61183 | Validation Loss: 28.309914\n",
      "Epoch 491 | Batch 0 | Train Loss: 153.88518 | Validation Loss: 23.773705\n",
      "Epoch 492 | Batch 0 | Train Loss: 154.87485 | Validation Loss: 25.05497\n",
      "Epoch 493 | Batch 0 | Train Loss: 158.61281 | Validation Loss: 24.110168\n",
      "Epoch 494 | Batch 0 | Train Loss: 159.76862 | Validation Loss: 29.230305\n",
      "Epoch 495 | Batch 0 | Train Loss: 157.66283 | Validation Loss: 25.010958\n",
      "Epoch 496 | Batch 0 | Train Loss: 153.6926 | Validation Loss: 24.956593\n",
      "Epoch 497 | Batch 0 | Train Loss: 152.08162 | Validation Loss: 24.018162\n",
      "Epoch 498 | Batch 0 | Train Loss: 152.22177 | Validation Loss: 28.504066\n",
      "Epoch 499 | Batch 0 | Train Loss: 151.91756 | Validation Loss: 28.829292\n",
      "Epoch 500 | Batch 0 | Train Loss: 153.84857 | Validation Loss: 24.997532\n",
      "Epoch 501 | Batch 0 | Train Loss: 157.02512 | Validation Loss: 20.675043\n",
      "Epoch 502 | Batch 0 | Train Loss: 152.27914 | Validation Loss: 25.548248\n",
      "Epoch 503 | Batch 0 | Train Loss: 152.812 | Validation Loss: 26.07446\n",
      "Epoch 504 | Batch 0 | Train Loss: 154.47284 | Validation Loss: 27.029966\n",
      "Epoch 505 | Batch 0 | Train Loss: 157.86269 | Validation Loss: 28.743526\n",
      "Epoch 506 | Batch 0 | Train Loss: 156.16792 | Validation Loss: 26.013428\n",
      "Epoch 507 | Batch 0 | Train Loss: 145.63098 | Validation Loss: 26.974731\n",
      "Epoch 508 | Batch 0 | Train Loss: 155.60999 | Validation Loss: 26.018349\n",
      "Epoch 509 | Batch 0 | Train Loss: 150.98973 | Validation Loss: 23.702835\n",
      "Epoch 510 | Batch 0 | Train Loss: 150.6402 | Validation Loss: 25.374294\n",
      "Epoch 511 | Batch 0 | Train Loss: 159.22284 | Validation Loss: 23.872375\n",
      "Epoch 512 | Batch 0 | Train Loss: 153.33928 | Validation Loss: 22.131691\n",
      "Epoch 513 | Batch 0 | Train Loss: 159.13206 | Validation Loss: 26.524632\n",
      "Epoch 514 | Batch 0 | Train Loss: 151.25269 | Validation Loss: 27.628212\n",
      "Epoch 515 | Batch 0 | Train Loss: 154.22647 | Validation Loss: 26.727337\n",
      "Epoch 516 | Batch 0 | Train Loss: 154.18466 | Validation Loss: 25.152885\n",
      "Epoch 517 | Batch 0 | Train Loss: 154.39255 | Validation Loss: 22.688171\n",
      "Epoch 518 | Batch 0 | Train Loss: 153.16864 | Validation Loss: 22.566944\n",
      "Epoch 519 | Batch 0 | Train Loss: 155.9639 | Validation Loss: 21.944378\n",
      "Epoch 520 | Batch 0 | Train Loss: 150.16435 | Validation Loss: 24.712418\n",
      "Epoch 521 | Batch 0 | Train Loss: 156.31332 | Validation Loss: 20.908306\n",
      "Epoch 522 | Batch 0 | Train Loss: 151.58032 | Validation Loss: 24.459482\n",
      "Epoch 523 | Batch 0 | Train Loss: 154.16962 | Validation Loss: 25.564724\n",
      "Epoch 524 | Batch 0 | Train Loss: 152.76045 | Validation Loss: 23.969337\n",
      "Epoch 525 | Batch 0 | Train Loss: 159.90672 | Validation Loss: 25.35111\n",
      "Epoch 526 | Batch 0 | Train Loss: 155.89755 | Validation Loss: 25.843227\n",
      "Epoch 527 | Batch 0 | Train Loss: 153.82843 | Validation Loss: 26.873661\n",
      "Epoch 528 | Batch 0 | Train Loss: 152.26392 | Validation Loss: 23.640474\n",
      "Epoch 529 | Batch 0 | Train Loss: 158.38335 | Validation Loss: 23.951063\n",
      "Epoch 530 | Batch 0 | Train Loss: 156.88315 | Validation Loss: 25.092241\n",
      "Epoch 531 | Batch 0 | Train Loss: 155.61598 | Validation Loss: 28.17493\n",
      "Epoch 532 | Batch 0 | Train Loss: 153.19933 | Validation Loss: 22.596146\n",
      "Epoch 533 | Batch 0 | Train Loss: 155.51962 | Validation Loss: 22.71677\n",
      "Epoch 534 | Batch 0 | Train Loss: 157.44113 | Validation Loss: 25.784866\n",
      "Epoch 535 | Batch 0 | Train Loss: 155.07132 | Validation Loss: 24.137836\n",
      "Epoch 536 | Batch 0 | Train Loss: 149.08041 | Validation Loss: 24.832256\n",
      "Epoch 537 | Batch 0 | Train Loss: 151.25484 | Validation Loss: 25.94453\n",
      "Epoch 538 | Batch 0 | Train Loss: 154.11858 | Validation Loss: 25.499409\n",
      "Epoch 539 | Batch 0 | Train Loss: 152.19708 | Validation Loss: 23.763138\n",
      "Epoch 540 | Batch 0 | Train Loss: 154.203 | Validation Loss: 22.630451\n",
      "Epoch 541 | Batch 0 | Train Loss: 158.63132 | Validation Loss: 22.462437\n",
      "Epoch 542 | Batch 0 | Train Loss: 158.42082 | Validation Loss: 24.12045\n",
      "Epoch 543 | Batch 0 | Train Loss: 154.2659 | Validation Loss: 24.985863\n",
      "Epoch 544 | Batch 0 | Train Loss: 149.99579 | Validation Loss: 24.94846\n",
      "Epoch 545 | Batch 0 | Train Loss: 152.51892 | Validation Loss: 26.899372\n",
      "Epoch 546 | Batch 0 | Train Loss: 152.66673 | Validation Loss: 25.716953\n",
      "Epoch 547 | Batch 0 | Train Loss: 152.07639 | Validation Loss: 22.239948\n",
      "Epoch 548 | Batch 0 | Train Loss: 151.86234 | Validation Loss: 19.184464\n",
      "Epoch 549 | Batch 0 | Train Loss: 158.3945 | Validation Loss: 27.80971\n",
      "Epoch 550 | Batch 0 | Train Loss: 153.95795 | Validation Loss: 26.736088\n",
      "Epoch 551 | Batch 0 | Train Loss: 154.36444 | Validation Loss: 28.538685\n",
      "Epoch 552 | Batch 0 | Train Loss: 151.84293 | Validation Loss: 24.156818\n",
      "Epoch 553 | Batch 0 | Train Loss: 158.7366 | Validation Loss: 24.090271\n",
      "Epoch 554 | Batch 0 | Train Loss: 154.38666 | Validation Loss: 22.179256\n",
      "Epoch 555 | Batch 0 | Train Loss: 155.94789 | Validation Loss: 27.615128\n",
      "Epoch 556 | Batch 0 | Train Loss: 153.54285 | Validation Loss: 29.263233\n",
      "Epoch 557 | Batch 0 | Train Loss: 152.40364 | Validation Loss: 27.272545\n",
      "Epoch 558 | Batch 0 | Train Loss: 147.93198 | Validation Loss: 23.173431\n",
      "Epoch 559 | Batch 0 | Train Loss: 151.90492 | Validation Loss: 25.223566\n",
      "Epoch 560 | Batch 0 | Train Loss: 150.2111 | Validation Loss: 23.467262\n",
      "Epoch 561 | Batch 0 | Train Loss: 153.51822 | Validation Loss: 28.277802\n",
      "Epoch 562 | Batch 0 | Train Loss: 153.43787 | Validation Loss: 25.42143\n",
      "Epoch 563 | Batch 0 | Train Loss: 152.89795 | Validation Loss: 23.986889\n",
      "Epoch 564 | Batch 0 | Train Loss: 148.94191 | Validation Loss: 23.655575\n",
      "Epoch 565 | Batch 0 | Train Loss: 158.78024 | Validation Loss: 25.760426\n",
      "Epoch 566 | Batch 0 | Train Loss: 149.134 | Validation Loss: 27.8138\n",
      "Epoch 567 | Batch 0 | Train Loss: 155.23235 | Validation Loss: 27.31056\n",
      "Epoch 568 | Batch 0 | Train Loss: 151.4849 | Validation Loss: 23.978676\n",
      "Epoch 569 | Batch 0 | Train Loss: 154.34738 | Validation Loss: 26.976353\n",
      "Epoch 570 | Batch 0 | Train Loss: 154.85735 | Validation Loss: 28.735863\n",
      "Epoch 571 | Batch 0 | Train Loss: 151.24353 | Validation Loss: 29.782036\n",
      "Epoch 572 | Batch 0 | Train Loss: 154.1168 | Validation Loss: 22.198036\n",
      "Epoch 573 | Batch 0 | Train Loss: 147.5145 | Validation Loss: 29.013872\n",
      "Epoch 574 | Batch 0 | Train Loss: 150.65787 | Validation Loss: 28.767408\n",
      "Epoch 575 | Batch 0 | Train Loss: 154.7063 | Validation Loss: 28.193253\n",
      "Epoch 576 | Batch 0 | Train Loss: 153.66719 | Validation Loss: 26.423132\n",
      "Epoch 577 | Batch 0 | Train Loss: 151.7771 | Validation Loss: 22.479027\n",
      "Epoch 578 | Batch 0 | Train Loss: 155.5752 | Validation Loss: 26.917751\n",
      "Epoch 579 | Batch 0 | Train Loss: 154.3162 | Validation Loss: 22.738895\n",
      "Epoch 580 | Batch 0 | Train Loss: 153.73264 | Validation Loss: 24.128407\n",
      "Epoch 581 | Batch 0 | Train Loss: 155.10495 | Validation Loss: 26.465157\n",
      "Epoch 582 | Batch 0 | Train Loss: 152.3359 | Validation Loss: 23.632595\n",
      "Epoch 583 | Batch 0 | Train Loss: 148.94814 | Validation Loss: 27.68926\n",
      "Epoch 584 | Batch 0 | Train Loss: 148.08281 | Validation Loss: 23.231503\n",
      "Epoch 585 | Batch 0 | Train Loss: 154.84145 | Validation Loss: 22.293472\n",
      "Epoch 586 | Batch 0 | Train Loss: 150.76846 | Validation Loss: 26.266056\n",
      "Epoch 587 | Batch 0 | Train Loss: 155.24014 | Validation Loss: 27.079887\n",
      "Epoch 588 | Batch 0 | Train Loss: 151.1253 | Validation Loss: 27.690357\n",
      "Epoch 589 | Batch 0 | Train Loss: 153.97528 | Validation Loss: 24.62994\n",
      "Epoch 590 | Batch 0 | Train Loss: 148.30585 | Validation Loss: 32.449104\n",
      "Epoch 591 | Batch 0 | Train Loss: 152.31517 | Validation Loss: 28.761322\n",
      "Epoch 592 | Batch 0 | Train Loss: 149.92212 | Validation Loss: 30.732374\n",
      "Epoch 593 | Batch 0 | Train Loss: 155.38922 | Validation Loss: 26.031368\n",
      "Epoch 594 | Batch 0 | Train Loss: 157.48587 | Validation Loss: 27.072197\n",
      "Epoch 595 | Batch 0 | Train Loss: 148.04062 | Validation Loss: 23.819723\n",
      "Epoch 596 | Batch 0 | Train Loss: 151.57953 | Validation Loss: 29.45475\n",
      "Epoch 597 | Batch 0 | Train Loss: 151.47678 | Validation Loss: 23.175915\n",
      "Epoch 598 | Batch 0 | Train Loss: 151.43842 | Validation Loss: 29.201462\n",
      "Epoch 599 | Batch 0 | Train Loss: 151.0127 | Validation Loss: 30.11762\n",
      "Epoch 600 | Batch 0 | Train Loss: 151.34497 | Validation Loss: 27.800184\n",
      "Epoch 601 | Batch 0 | Train Loss: 152.10725 | Validation Loss: 28.77346\n",
      "Epoch 602 | Batch 0 | Train Loss: 150.13535 | Validation Loss: 26.775673\n",
      "Epoch 603 | Batch 0 | Train Loss: 149.00143 | Validation Loss: 27.587128\n",
      "Epoch 604 | Batch 0 | Train Loss: 155.04199 | Validation Loss: 25.013714\n",
      "Epoch 605 | Batch 0 | Train Loss: 151.69507 | Validation Loss: 25.83627\n",
      "Epoch 606 | Batch 0 | Train Loss: 157.12929 | Validation Loss: 27.259441\n",
      "Epoch 607 | Batch 0 | Train Loss: 152.77155 | Validation Loss: 21.784939\n",
      "Epoch 608 | Batch 0 | Train Loss: 157.23016 | Validation Loss: 27.956966\n",
      "Epoch 609 | Batch 0 | Train Loss: 152.51288 | Validation Loss: 25.304712\n",
      "Epoch 610 | Batch 0 | Train Loss: 157.01067 | Validation Loss: 29.075195\n",
      "Epoch 611 | Batch 0 | Train Loss: 155.22433 | Validation Loss: 23.620571\n",
      "Epoch 612 | Batch 0 | Train Loss: 148.1706 | Validation Loss: 27.675663\n",
      "Epoch 613 | Batch 0 | Train Loss: 157.56836 | Validation Loss: 30.597355\n",
      "Epoch 614 | Batch 0 | Train Loss: 155.3023 | Validation Loss: 21.689379\n",
      "Epoch 615 | Batch 0 | Train Loss: 154.22408 | Validation Loss: 26.679308\n",
      "Epoch 616 | Batch 0 | Train Loss: 158.68683 | Validation Loss: 26.519047\n",
      "Epoch 617 | Batch 0 | Train Loss: 155.79701 | Validation Loss: 21.680948\n",
      "Epoch 618 | Batch 0 | Train Loss: 153.32338 | Validation Loss: 26.426899\n",
      "Epoch 619 | Batch 0 | Train Loss: 155.24612 | Validation Loss: 28.19881\n",
      "Epoch 620 | Batch 0 | Train Loss: 155.35208 | Validation Loss: 29.793102\n",
      "Epoch 621 | Batch 0 | Train Loss: 151.83054 | Validation Loss: 30.825655\n",
      "Epoch 622 | Batch 0 | Train Loss: 148.75282 | Validation Loss: 23.19865\n",
      "Epoch 623 | Batch 0 | Train Loss: 150.4588 | Validation Loss: 26.110622\n",
      "Epoch 624 | Batch 0 | Train Loss: 148.03357 | Validation Loss: 27.419388\n",
      "Epoch 625 | Batch 0 | Train Loss: 152.23112 | Validation Loss: 24.54561\n",
      "Epoch 626 | Batch 0 | Train Loss: 152.8876 | Validation Loss: 26.67852\n",
      "Epoch 627 | Batch 0 | Train Loss: 152.19876 | Validation Loss: 24.350815\n",
      "Epoch 628 | Batch 0 | Train Loss: 150.84903 | Validation Loss: 27.594963\n",
      "Epoch 629 | Batch 0 | Train Loss: 154.54074 | Validation Loss: 28.182173\n",
      "Epoch 630 | Batch 0 | Train Loss: 157.88095 | Validation Loss: 29.44336\n",
      "Epoch 631 | Batch 0 | Train Loss: 153.0195 | Validation Loss: 30.499115\n",
      "Epoch 632 | Batch 0 | Train Loss: 155.31938 | Validation Loss: 25.32707\n",
      "Epoch 633 | Batch 0 | Train Loss: 148.93086 | Validation Loss: 24.061264\n",
      "Epoch 634 | Batch 0 | Train Loss: 160.40294 | Validation Loss: 27.46299\n",
      "Epoch 635 | Batch 0 | Train Loss: 147.80934 | Validation Loss: 28.805939\n",
      "Epoch 636 | Batch 0 | Train Loss: 150.54156 | Validation Loss: 27.352306\n",
      "Epoch 637 | Batch 0 | Train Loss: 153.52379 | Validation Loss: 25.302967\n",
      "Epoch 638 | Batch 0 | Train Loss: 152.3978 | Validation Loss: 26.217415\n",
      "Epoch 639 | Batch 0 | Train Loss: 151.45334 | Validation Loss: 25.368923\n",
      "Epoch 640 | Batch 0 | Train Loss: 150.0854 | Validation Loss: 25.93356\n",
      "Epoch 641 | Batch 0 | Train Loss: 157.21355 | Validation Loss: 23.608074\n",
      "Epoch 642 | Batch 0 | Train Loss: 151.47849 | Validation Loss: 24.053799\n",
      "Epoch 643 | Batch 0 | Train Loss: 156.52487 | Validation Loss: 27.498964\n",
      "Epoch 644 | Batch 0 | Train Loss: 152.43427 | Validation Loss: 27.359505\n",
      "Epoch 645 | Batch 0 | Train Loss: 152.16473 | Validation Loss: 30.476276\n",
      "Epoch 646 | Batch 0 | Train Loss: 155.73566 | Validation Loss: 25.749582\n",
      "Epoch 647 | Batch 0 | Train Loss: 156.6266 | Validation Loss: 24.774517\n",
      "Epoch 648 | Batch 0 | Train Loss: 154.10349 | Validation Loss: 23.522346\n",
      "Epoch 649 | Batch 0 | Train Loss: 148.5303 | Validation Loss: 27.033878\n",
      "Epoch 650 | Batch 0 | Train Loss: 156.8033 | Validation Loss: 27.84298\n",
      "Epoch 651 | Batch 0 | Train Loss: 152.65479 | Validation Loss: 24.712261\n",
      "Epoch 652 | Batch 0 | Train Loss: 154.70216 | Validation Loss: 25.039833\n",
      "Epoch 653 | Batch 0 | Train Loss: 154.46799 | Validation Loss: 27.336786\n",
      "Epoch 654 | Batch 0 | Train Loss: 149.82309 | Validation Loss: 24.908548\n",
      "Epoch 655 | Batch 0 | Train Loss: 149.93893 | Validation Loss: 25.380013\n",
      "Epoch 656 | Batch 0 | Train Loss: 149.76826 | Validation Loss: 25.841162\n",
      "Epoch 657 | Batch 0 | Train Loss: 154.53273 | Validation Loss: 23.441383\n",
      "Epoch 658 | Batch 0 | Train Loss: 151.09528 | Validation Loss: 26.975838\n",
      "Epoch 659 | Batch 0 | Train Loss: 151.89908 | Validation Loss: 26.850052\n",
      "Epoch 660 | Batch 0 | Train Loss: 151.513 | Validation Loss: 27.17336\n",
      "Epoch 661 | Batch 0 | Train Loss: 153.37717 | Validation Loss: 25.214165\n",
      "Epoch 662 | Batch 0 | Train Loss: 143.83347 | Validation Loss: 25.53692\n",
      "Epoch 663 | Batch 0 | Train Loss: 152.85027 | Validation Loss: 23.022043\n",
      "Epoch 664 | Batch 0 | Train Loss: 150.06778 | Validation Loss: 29.043236\n",
      "Epoch 665 | Batch 0 | Train Loss: 151.98724 | Validation Loss: 26.614193\n",
      "Epoch 666 | Batch 0 | Train Loss: 149.95985 | Validation Loss: 27.190136\n",
      "Epoch 667 | Batch 0 | Train Loss: 152.66936 | Validation Loss: 23.331377\n",
      "Epoch 668 | Batch 0 | Train Loss: 153.17604 | Validation Loss: 26.989792\n",
      "Epoch 669 | Batch 0 | Train Loss: 149.19385 | Validation Loss: 25.877026\n",
      "Epoch 670 | Batch 0 | Train Loss: 152.26915 | Validation Loss: 28.136135\n",
      "Epoch 671 | Batch 0 | Train Loss: 154.98166 | Validation Loss: 26.666195\n",
      "Epoch 672 | Batch 0 | Train Loss: 153.4336 | Validation Loss: 26.282438\n",
      "Epoch 673 | Batch 0 | Train Loss: 150.60696 | Validation Loss: 24.645279\n",
      "Epoch 674 | Batch 0 | Train Loss: 152.83914 | Validation Loss: 26.006939\n",
      "Epoch 675 | Batch 0 | Train Loss: 153.20045 | Validation Loss: 22.66362\n",
      "Epoch 676 | Batch 0 | Train Loss: 153.09338 | Validation Loss: 24.3573\n",
      "Epoch 677 | Batch 0 | Train Loss: 146.53648 | Validation Loss: 21.297894\n",
      "Epoch 678 | Batch 0 | Train Loss: 150.12326 | Validation Loss: 24.271534\n",
      "Epoch 679 | Batch 0 | Train Loss: 148.017 | Validation Loss: 27.438038\n",
      "Epoch 680 | Batch 0 | Train Loss: 150.50891 | Validation Loss: 30.069027\n",
      "Epoch 681 | Batch 0 | Train Loss: 151.83179 | Validation Loss: 28.90076\n",
      "Epoch 682 | Batch 0 | Train Loss: 147.21307 | Validation Loss: 24.0993\n",
      "Epoch 683 | Batch 0 | Train Loss: 145.56111 | Validation Loss: 25.536304\n",
      "Epoch 684 | Batch 0 | Train Loss: 155.81236 | Validation Loss: 28.558022\n",
      "Epoch 685 | Batch 0 | Train Loss: 148.83282 | Validation Loss: 26.343853\n",
      "Epoch 686 | Batch 0 | Train Loss: 156.42255 | Validation Loss: 27.616829\n",
      "Epoch 687 | Batch 0 | Train Loss: 154.30438 | Validation Loss: 30.207138\n",
      "Epoch 688 | Batch 0 | Train Loss: 145.65892 | Validation Loss: 27.351126\n",
      "Epoch 689 | Batch 0 | Train Loss: 149.3583 | Validation Loss: 26.214443\n",
      "Epoch 690 | Batch 0 | Train Loss: 148.81412 | Validation Loss: 24.738937\n",
      "Epoch 691 | Batch 0 | Train Loss: 154.51076 | Validation Loss: 25.60941\n",
      "Epoch 692 | Batch 0 | Train Loss: 151.32414 | Validation Loss: 27.917788\n",
      "Epoch 693 | Batch 0 | Train Loss: 151.5185 | Validation Loss: 25.004211\n",
      "Epoch 694 | Batch 0 | Train Loss: 144.24487 | Validation Loss: 24.269814\n",
      "Epoch 695 | Batch 0 | Train Loss: 151.36176 | Validation Loss: 30.485905\n",
      "Epoch 696 | Batch 0 | Train Loss: 155.8422 | Validation Loss: 23.82465\n",
      "Epoch 697 | Batch 0 | Train Loss: 157.53131 | Validation Loss: 25.433168\n",
      "Epoch 698 | Batch 0 | Train Loss: 147.71725 | Validation Loss: 27.256168\n",
      "Epoch 699 | Batch 0 | Train Loss: 152.23517 | Validation Loss: 25.710026\n",
      "Epoch 700 | Batch 0 | Train Loss: 147.68639 | Validation Loss: 26.636284\n",
      "Epoch 701 | Batch 0 | Train Loss: 152.54977 | Validation Loss: 31.870499\n",
      "Epoch 702 | Batch 0 | Train Loss: 151.50891 | Validation Loss: 25.912395\n",
      "Epoch 703 | Batch 0 | Train Loss: 151.45341 | Validation Loss: 26.093946\n",
      "Epoch 704 | Batch 0 | Train Loss: 153.45139 | Validation Loss: 25.788443\n",
      "Epoch 705 | Batch 0 | Train Loss: 150.18259 | Validation Loss: 29.894936\n",
      "Epoch 706 | Batch 0 | Train Loss: 150.35654 | Validation Loss: 28.310013\n",
      "Epoch 707 | Batch 0 | Train Loss: 152.03242 | Validation Loss: 24.519875\n",
      "Epoch 708 | Batch 0 | Train Loss: 160.07758 | Validation Loss: 27.10754\n",
      "Epoch 709 | Batch 0 | Train Loss: 144.68683 | Validation Loss: 24.801182\n",
      "Epoch 710 | Batch 0 | Train Loss: 151.86797 | Validation Loss: 26.735811\n",
      "Epoch 711 | Batch 0 | Train Loss: 154.12375 | Validation Loss: 24.15075\n",
      "Epoch 712 | Batch 0 | Train Loss: 151.89542 | Validation Loss: 23.968067\n",
      "Epoch 713 | Batch 0 | Train Loss: 148.03787 | Validation Loss: 26.33582\n",
      "Epoch 714 | Batch 0 | Train Loss: 152.26886 | Validation Loss: 31.10285\n",
      "Epoch 715 | Batch 0 | Train Loss: 152.82516 | Validation Loss: 26.026443\n",
      "Epoch 716 | Batch 0 | Train Loss: 150.37411 | Validation Loss: 31.68266\n",
      "Epoch 717 | Batch 0 | Train Loss: 147.97736 | Validation Loss: 26.178108\n",
      "Epoch 718 | Batch 0 | Train Loss: 153.50171 | Validation Loss: 28.89787\n",
      "Epoch 719 | Batch 0 | Train Loss: 152.15468 | Validation Loss: 28.629196\n",
      "Epoch 720 | Batch 0 | Train Loss: 148.55548 | Validation Loss: 32.337097\n",
      "Epoch 721 | Batch 0 | Train Loss: 152.91388 | Validation Loss: 26.999998\n",
      "Epoch 722 | Batch 0 | Train Loss: 146.79707 | Validation Loss: 26.024693\n",
      "Epoch 723 | Batch 0 | Train Loss: 148.93282 | Validation Loss: 25.62249\n",
      "Epoch 724 | Batch 0 | Train Loss: 150.59174 | Validation Loss: 28.38947\n",
      "Epoch 725 | Batch 0 | Train Loss: 148.89955 | Validation Loss: 23.425154\n",
      "Epoch 726 | Batch 0 | Train Loss: 150.53378 | Validation Loss: 29.452984\n",
      "Epoch 727 | Batch 0 | Train Loss: 149.85796 | Validation Loss: 20.23964\n",
      "Epoch 728 | Batch 0 | Train Loss: 146.49539 | Validation Loss: 21.261444\n",
      "Epoch 729 | Batch 0 | Train Loss: 153.6253 | Validation Loss: 23.606884\n",
      "Epoch 730 | Batch 0 | Train Loss: 154.72586 | Validation Loss: 27.65525\n",
      "Epoch 731 | Batch 0 | Train Loss: 149.44272 | Validation Loss: 25.966661\n",
      "Epoch 732 | Batch 0 | Train Loss: 149.53693 | Validation Loss: 27.229988\n",
      "Epoch 733 | Batch 0 | Train Loss: 151.68265 | Validation Loss: 28.154917\n",
      "Epoch 734 | Batch 0 | Train Loss: 146.48744 | Validation Loss: 24.999184\n",
      "Epoch 735 | Batch 0 | Train Loss: 153.94788 | Validation Loss: 30.156998\n",
      "Epoch 736 | Batch 0 | Train Loss: 149.33977 | Validation Loss: 26.404795\n",
      "Epoch 737 | Batch 0 | Train Loss: 153.90799 | Validation Loss: 24.992294\n",
      "Epoch 738 | Batch 0 | Train Loss: 145.61237 | Validation Loss: 24.28253\n",
      "Epoch 739 | Batch 0 | Train Loss: 150.45116 | Validation Loss: 31.099148\n",
      "Epoch 740 | Batch 0 | Train Loss: 149.51373 | Validation Loss: 24.508709\n",
      "Epoch 741 | Batch 0 | Train Loss: 147.51083 | Validation Loss: 25.116371\n",
      "Epoch 742 | Batch 0 | Train Loss: 152.62436 | Validation Loss: 27.609268\n",
      "Epoch 743 | Batch 0 | Train Loss: 154.92853 | Validation Loss: 28.66502\n",
      "Epoch 744 | Batch 0 | Train Loss: 152.57568 | Validation Loss: 24.766088\n",
      "Epoch 745 | Batch 0 | Train Loss: 151.21799 | Validation Loss: 25.179585\n",
      "Epoch 746 | Batch 0 | Train Loss: 148.30016 | Validation Loss: 33.112053\n",
      "Epoch 747 | Batch 0 | Train Loss: 151.89713 | Validation Loss: 27.164341\n",
      "Epoch 748 | Batch 0 | Train Loss: 153.06854 | Validation Loss: 26.121143\n",
      "Epoch 749 | Batch 0 | Train Loss: 149.55243 | Validation Loss: 30.523937\n",
      "Epoch 750 | Batch 0 | Train Loss: 152.2389 | Validation Loss: 27.74123\n",
      "Epoch 751 | Batch 0 | Train Loss: 148.02374 | Validation Loss: 26.212215\n",
      "Epoch 752 | Batch 0 | Train Loss: 151.60416 | Validation Loss: 26.584728\n",
      "Epoch 753 | Batch 0 | Train Loss: 145.88315 | Validation Loss: 28.378386\n",
      "Epoch 754 | Batch 0 | Train Loss: 154.39355 | Validation Loss: 29.360052\n",
      "Epoch 755 | Batch 0 | Train Loss: 151.35655 | Validation Loss: 26.113623\n",
      "Epoch 756 | Batch 0 | Train Loss: 147.06853 | Validation Loss: 23.149607\n",
      "Epoch 757 | Batch 0 | Train Loss: 146.54208 | Validation Loss: 25.448254\n",
      "Epoch 758 | Batch 0 | Train Loss: 146.84203 | Validation Loss: 25.67041\n",
      "Epoch 759 | Batch 0 | Train Loss: 149.08966 | Validation Loss: 22.35239\n",
      "Epoch 760 | Batch 0 | Train Loss: 152.65462 | Validation Loss: 29.001522\n",
      "Epoch 761 | Batch 0 | Train Loss: 148.0863 | Validation Loss: 26.451912\n",
      "Epoch 762 | Batch 0 | Train Loss: 147.71748 | Validation Loss: 34.74055\n",
      "Epoch 763 | Batch 0 | Train Loss: 155.07144 | Validation Loss: 27.51365\n",
      "Epoch 764 | Batch 0 | Train Loss: 152.85507 | Validation Loss: 24.637276\n",
      "Epoch 765 | Batch 0 | Train Loss: 146.5712 | Validation Loss: 28.951946\n",
      "Epoch 766 | Batch 0 | Train Loss: 151.81926 | Validation Loss: 28.379356\n",
      "Epoch 767 | Batch 0 | Train Loss: 156.03636 | Validation Loss: 26.081429\n",
      "Epoch 768 | Batch 0 | Train Loss: 154.4624 | Validation Loss: 24.9515\n",
      "Epoch 769 | Batch 0 | Train Loss: 153.70271 | Validation Loss: 28.15982\n",
      "Epoch 770 | Batch 0 | Train Loss: 151.44339 | Validation Loss: 30.82967\n",
      "Epoch 771 | Batch 0 | Train Loss: 153.25143 | Validation Loss: 30.89785\n",
      "Epoch 772 | Batch 0 | Train Loss: 150.99385 | Validation Loss: 26.724766\n",
      "Epoch 773 | Batch 0 | Train Loss: 150.38422 | Validation Loss: 27.068056\n",
      "Epoch 774 | Batch 0 | Train Loss: 144.68765 | Validation Loss: 30.409714\n",
      "Epoch 775 | Batch 0 | Train Loss: 151.55902 | Validation Loss: 32.493866\n",
      "Epoch 776 | Batch 0 | Train Loss: 151.11136 | Validation Loss: 32.173553\n",
      "Epoch 777 | Batch 0 | Train Loss: 149.87599 | Validation Loss: 29.434898\n",
      "Epoch 778 | Batch 0 | Train Loss: 151.64374 | Validation Loss: 32.513184\n",
      "Epoch 779 | Batch 0 | Train Loss: 157.86906 | Validation Loss: 30.0126\n",
      "Epoch 780 | Batch 0 | Train Loss: 156.13072 | Validation Loss: 23.68179\n",
      "Epoch 781 | Batch 0 | Train Loss: 151.61276 | Validation Loss: 29.063496\n",
      "Epoch 782 | Batch 0 | Train Loss: 153.91583 | Validation Loss: 30.809448\n",
      "Epoch 783 | Batch 0 | Train Loss: 153.48097 | Validation Loss: 28.043226\n",
      "Epoch 784 | Batch 0 | Train Loss: 152.67563 | Validation Loss: 28.319904\n",
      "Epoch 785 | Batch 0 | Train Loss: 152.29749 | Validation Loss: 23.185963\n",
      "Epoch 786 | Batch 0 | Train Loss: 148.81412 | Validation Loss: 26.106037\n",
      "Epoch 787 | Batch 0 | Train Loss: 151.94653 | Validation Loss: 33.191673\n",
      "Epoch 788 | Batch 0 | Train Loss: 149.6663 | Validation Loss: 22.93561\n",
      "Epoch 789 | Batch 0 | Train Loss: 147.92664 | Validation Loss: 25.413239\n",
      "Epoch 790 | Batch 0 | Train Loss: 153.90312 | Validation Loss: 28.894188\n",
      "Epoch 791 | Batch 0 | Train Loss: 149.88306 | Validation Loss: 29.179386\n",
      "Epoch 792 | Batch 0 | Train Loss: 150.13293 | Validation Loss: 28.232784\n",
      "Epoch 793 | Batch 0 | Train Loss: 149.39941 | Validation Loss: 24.817757\n",
      "Epoch 794 | Batch 0 | Train Loss: 150.62962 | Validation Loss: 27.770674\n",
      "Epoch 795 | Batch 0 | Train Loss: 143.67099 | Validation Loss: 26.617939\n",
      "Epoch 796 | Batch 0 | Train Loss: 148.69855 | Validation Loss: 28.157454\n",
      "Epoch 797 | Batch 0 | Train Loss: 155.19589 | Validation Loss: 33.804657\n",
      "Epoch 798 | Batch 0 | Train Loss: 146.94427 | Validation Loss: 24.425718\n",
      "Epoch 799 | Batch 0 | Train Loss: 155.3808 | Validation Loss: 25.607803\n",
      "Epoch 800 | Batch 0 | Train Loss: 148.31172 | Validation Loss: 24.115032\n",
      "Epoch 801 | Batch 0 | Train Loss: 148.37372 | Validation Loss: 29.661907\n",
      "Epoch 802 | Batch 0 | Train Loss: 154.14703 | Validation Loss: 30.935114\n",
      "Epoch 803 | Batch 0 | Train Loss: 156.89041 | Validation Loss: 24.931005\n",
      "Epoch 804 | Batch 0 | Train Loss: 151.36548 | Validation Loss: 28.64725\n",
      "Epoch 805 | Batch 0 | Train Loss: 148.77225 | Validation Loss: 26.347477\n",
      "Epoch 806 | Batch 0 | Train Loss: 151.3688 | Validation Loss: 25.9873\n",
      "Epoch 807 | Batch 0 | Train Loss: 152.84349 | Validation Loss: 27.930834\n",
      "Epoch 808 | Batch 0 | Train Loss: 152.43704 | Validation Loss: 31.429928\n",
      "Epoch 809 | Batch 0 | Train Loss: 152.99414 | Validation Loss: 26.908222\n",
      "Epoch 810 | Batch 0 | Train Loss: 154.79816 | Validation Loss: 28.07637\n",
      "Epoch 811 | Batch 0 | Train Loss: 145.77704 | Validation Loss: 27.806072\n",
      "Epoch 812 | Batch 0 | Train Loss: 147.25192 | Validation Loss: 28.434952\n",
      "Epoch 813 | Batch 0 | Train Loss: 153.59946 | Validation Loss: 29.01435\n",
      "Epoch 814 | Batch 0 | Train Loss: 148.63898 | Validation Loss: 28.247028\n",
      "Epoch 815 | Batch 0 | Train Loss: 147.6301 | Validation Loss: 25.744007\n",
      "Epoch 816 | Batch 0 | Train Loss: 151.15656 | Validation Loss: 25.797775\n",
      "Epoch 817 | Batch 0 | Train Loss: 145.9008 | Validation Loss: 30.319622\n",
      "Epoch 818 | Batch 0 | Train Loss: 154.2247 | Validation Loss: 21.900764\n",
      "Epoch 819 | Batch 0 | Train Loss: 148.05098 | Validation Loss: 34.810196\n",
      "Epoch 820 | Batch 0 | Train Loss: 151.81865 | Validation Loss: 27.522118\n",
      "Epoch 821 | Batch 0 | Train Loss: 151.33015 | Validation Loss: 30.722664\n",
      "Epoch 822 | Batch 0 | Train Loss: 148.23567 | Validation Loss: 26.447784\n",
      "Epoch 823 | Batch 0 | Train Loss: 156.23126 | Validation Loss: 29.684517\n",
      "Epoch 824 | Batch 0 | Train Loss: 148.97708 | Validation Loss: 30.798977\n",
      "Epoch 825 | Batch 0 | Train Loss: 149.77016 | Validation Loss: 32.169014\n",
      "Epoch 826 | Batch 0 | Train Loss: 152.21497 | Validation Loss: 30.378784\n",
      "Epoch 827 | Batch 0 | Train Loss: 150.07672 | Validation Loss: 25.556362\n",
      "Epoch 828 | Batch 0 | Train Loss: 149.26297 | Validation Loss: 28.873974\n",
      "Epoch 829 | Batch 0 | Train Loss: 154.3655 | Validation Loss: 25.474115\n",
      "Epoch 830 | Batch 0 | Train Loss: 147.65195 | Validation Loss: 31.168392\n",
      "Epoch 831 | Batch 0 | Train Loss: 150.2108 | Validation Loss: 31.699924\n",
      "Epoch 832 | Batch 0 | Train Loss: 151.26227 | Validation Loss: 33.709267\n",
      "Epoch 833 | Batch 0 | Train Loss: 146.78716 | Validation Loss: 26.806692\n",
      "Epoch 834 | Batch 0 | Train Loss: 151.27637 | Validation Loss: 30.870045\n",
      "Epoch 835 | Batch 0 | Train Loss: 150.01494 | Validation Loss: 31.75939\n",
      "Epoch 836 | Batch 0 | Train Loss: 146.92764 | Validation Loss: 32.908775\n",
      "Epoch 837 | Batch 0 | Train Loss: 145.5835 | Validation Loss: 30.663757\n",
      "Epoch 838 | Batch 0 | Train Loss: 160.58594 | Validation Loss: 25.857868\n",
      "Epoch 839 | Batch 0 | Train Loss: 147.13579 | Validation Loss: 29.184309\n",
      "Epoch 840 | Batch 0 | Train Loss: 148.14417 | Validation Loss: 27.240189\n",
      "Epoch 841 | Batch 0 | Train Loss: 150.7534 | Validation Loss: 28.176392\n",
      "Epoch 842 | Batch 0 | Train Loss: 152.60692 | Validation Loss: 30.740713\n",
      "Epoch 843 | Batch 0 | Train Loss: 147.83632 | Validation Loss: 29.612616\n",
      "Epoch 844 | Batch 0 | Train Loss: 153.97163 | Validation Loss: 32.228676\n",
      "Epoch 845 | Batch 0 | Train Loss: 148.42496 | Validation Loss: 31.70216\n",
      "Epoch 846 | Batch 0 | Train Loss: 153.51192 | Validation Loss: 29.085262\n",
      "Epoch 847 | Batch 0 | Train Loss: 147.93327 | Validation Loss: 30.656372\n",
      "Epoch 848 | Batch 0 | Train Loss: 144.42976 | Validation Loss: 27.767887\n",
      "Epoch 849 | Batch 0 | Train Loss: 145.56348 | Validation Loss: 25.433979\n",
      "Epoch 850 | Batch 0 | Train Loss: 152.07564 | Validation Loss: 25.235062\n",
      "Epoch 851 | Batch 0 | Train Loss: 148.64499 | Validation Loss: 25.602852\n",
      "Epoch 852 | Batch 0 | Train Loss: 152.95016 | Validation Loss: 27.765282\n",
      "Epoch 853 | Batch 0 | Train Loss: 149.12149 | Validation Loss: 31.008682\n",
      "Epoch 854 | Batch 0 | Train Loss: 151.2838 | Validation Loss: 24.512846\n",
      "Epoch 855 | Batch 0 | Train Loss: 147.7799 | Validation Loss: 31.078304\n",
      "Epoch 856 | Batch 0 | Train Loss: 149.93521 | Validation Loss: 28.319748\n",
      "Epoch 857 | Batch 0 | Train Loss: 152.89511 | Validation Loss: 28.480858\n",
      "Epoch 858 | Batch 0 | Train Loss: 149.54265 | Validation Loss: 31.29773\n",
      "Epoch 859 | Batch 0 | Train Loss: 145.93536 | Validation Loss: 28.899578\n",
      "Epoch 860 | Batch 0 | Train Loss: 158.9083 | Validation Loss: 27.049732\n",
      "Epoch 861 | Batch 0 | Train Loss: 152.53876 | Validation Loss: 34.168373\n",
      "Epoch 862 | Batch 0 | Train Loss: 147.23766 | Validation Loss: 27.131104\n",
      "Epoch 863 | Batch 0 | Train Loss: 148.60785 | Validation Loss: 33.031273\n",
      "Epoch 864 | Batch 0 | Train Loss: 143.33862 | Validation Loss: 27.195074\n",
      "Epoch 865 | Batch 0 | Train Loss: 151.3709 | Validation Loss: 27.661083\n",
      "Epoch 866 | Batch 0 | Train Loss: 153.84523 | Validation Loss: 28.4334\n",
      "Epoch 867 | Batch 0 | Train Loss: 150.51581 | Validation Loss: 28.31166\n",
      "Epoch 868 | Batch 0 | Train Loss: 145.89178 | Validation Loss: 25.05315\n",
      "Epoch 869 | Batch 0 | Train Loss: 154.66068 | Validation Loss: 25.480026\n",
      "Epoch 870 | Batch 0 | Train Loss: 153.89551 | Validation Loss: 29.695656\n",
      "Epoch 871 | Batch 0 | Train Loss: 150.1287 | Validation Loss: 27.330713\n",
      "Epoch 872 | Batch 0 | Train Loss: 149.42598 | Validation Loss: 29.93451\n",
      "Epoch 873 | Batch 0 | Train Loss: 144.45467 | Validation Loss: 31.8573\n",
      "Epoch 874 | Batch 0 | Train Loss: 148.43217 | Validation Loss: 28.584415\n",
      "Epoch 875 | Batch 0 | Train Loss: 146.60458 | Validation Loss: 31.145832\n",
      "Epoch 876 | Batch 0 | Train Loss: 148.48665 | Validation Loss: 22.778435\n",
      "Epoch 877 | Batch 0 | Train Loss: 146.8836 | Validation Loss: 28.236656\n",
      "Epoch 878 | Batch 0 | Train Loss: 147.47507 | Validation Loss: 25.320385\n",
      "Epoch 879 | Batch 0 | Train Loss: 149.25026 | Validation Loss: 30.362425\n",
      "Epoch 880 | Batch 0 | Train Loss: 150.93256 | Validation Loss: 26.158682\n",
      "Epoch 881 | Batch 0 | Train Loss: 151.50748 | Validation Loss: 31.303333\n",
      "Epoch 882 | Batch 0 | Train Loss: 152.43297 | Validation Loss: 23.473825\n",
      "Epoch 883 | Batch 0 | Train Loss: 145.68475 | Validation Loss: 32.806927\n",
      "Epoch 884 | Batch 0 | Train Loss: 153.56337 | Validation Loss: 26.929758\n",
      "Epoch 885 | Batch 0 | Train Loss: 148.73297 | Validation Loss: 34.92685\n",
      "Epoch 886 | Batch 0 | Train Loss: 148.87483 | Validation Loss: 25.901075\n",
      "Epoch 887 | Batch 0 | Train Loss: 150.14996 | Validation Loss: 33.48639\n",
      "Epoch 888 | Batch 0 | Train Loss: 146.59842 | Validation Loss: 32.97329\n",
      "Epoch 889 | Batch 0 | Train Loss: 142.36966 | Validation Loss: 29.055773\n",
      "Epoch 890 | Batch 0 | Train Loss: 146.72398 | Validation Loss: 30.284098\n",
      "Epoch 891 | Batch 0 | Train Loss: 148.5035 | Validation Loss: 28.249588\n",
      "Epoch 892 | Batch 0 | Train Loss: 155.21497 | Validation Loss: 26.028557\n",
      "Epoch 893 | Batch 0 | Train Loss: 145.71442 | Validation Loss: 31.070332\n",
      "Epoch 894 | Batch 0 | Train Loss: 148.09991 | Validation Loss: 30.579762\n",
      "Epoch 895 | Batch 0 | Train Loss: 151.7742 | Validation Loss: 31.973286\n",
      "Epoch 896 | Batch 0 | Train Loss: 150.83018 | Validation Loss: 32.37283\n",
      "Epoch 897 | Batch 0 | Train Loss: 149.79143 | Validation Loss: 31.814415\n",
      "Epoch 898 | Batch 0 | Train Loss: 150.16118 | Validation Loss: 29.406338\n",
      "Epoch 899 | Batch 0 | Train Loss: 145.03696 | Validation Loss: 25.337498\n",
      "Epoch 900 | Batch 0 | Train Loss: 144.65424 | Validation Loss: 28.452126\n",
      "Epoch 901 | Batch 0 | Train Loss: 147.24748 | Validation Loss: 23.995708\n",
      "Epoch 902 | Batch 0 | Train Loss: 151.90344 | Validation Loss: 33.14499\n",
      "Epoch 903 | Batch 0 | Train Loss: 148.37035 | Validation Loss: 29.652033\n",
      "Epoch 904 | Batch 0 | Train Loss: 148.95583 | Validation Loss: 31.26331\n",
      "Epoch 905 | Batch 0 | Train Loss: 147.85963 | Validation Loss: 21.582836\n",
      "Epoch 906 | Batch 0 | Train Loss: 155.00279 | Validation Loss: 31.952946\n",
      "Epoch 907 | Batch 0 | Train Loss: 146.51474 | Validation Loss: 27.639679\n",
      "Epoch 908 | Batch 0 | Train Loss: 147.71715 | Validation Loss: 32.199463\n",
      "Epoch 909 | Batch 0 | Train Loss: 147.90953 | Validation Loss: 31.218601\n",
      "Epoch 910 | Batch 0 | Train Loss: 152.84874 | Validation Loss: 21.186485\n",
      "Epoch 911 | Batch 0 | Train Loss: 148.76874 | Validation Loss: 30.92016\n",
      "Epoch 912 | Batch 0 | Train Loss: 151.87923 | Validation Loss: 29.289162\n",
      "Epoch 913 | Batch 0 | Train Loss: 147.39032 | Validation Loss: 29.861336\n",
      "Epoch 914 | Batch 0 | Train Loss: 154.36044 | Validation Loss: 31.399158\n",
      "Epoch 915 | Batch 0 | Train Loss: 145.69345 | Validation Loss: 24.577461\n",
      "Epoch 916 | Batch 0 | Train Loss: 146.80812 | Validation Loss: 27.098455\n",
      "Epoch 917 | Batch 0 | Train Loss: 145.61882 | Validation Loss: 27.837944\n",
      "Epoch 918 | Batch 0 | Train Loss: 147.32831 | Validation Loss: 19.542751\n",
      "Epoch 919 | Batch 0 | Train Loss: 150.76154 | Validation Loss: 29.606941\n",
      "Epoch 920 | Batch 0 | Train Loss: 144.5812 | Validation Loss: 26.416054\n",
      "Epoch 921 | Batch 0 | Train Loss: 151.0198 | Validation Loss: 28.492176\n",
      "Epoch 922 | Batch 0 | Train Loss: 146.9267 | Validation Loss: 24.056942\n",
      "Epoch 923 | Batch 0 | Train Loss: 147.80627 | Validation Loss: 29.462343\n",
      "Epoch 924 | Batch 0 | Train Loss: 144.70724 | Validation Loss: 31.55291\n",
      "Epoch 925 | Batch 0 | Train Loss: 148.87602 | Validation Loss: 35.659935\n",
      "Epoch 926 | Batch 0 | Train Loss: 146.452 | Validation Loss: 32.76615\n",
      "Epoch 927 | Batch 0 | Train Loss: 153.73116 | Validation Loss: 26.925617\n",
      "Epoch 928 | Batch 0 | Train Loss: 148.92137 | Validation Loss: 32.907715\n",
      "Epoch 929 | Batch 0 | Train Loss: 150.55775 | Validation Loss: 30.973286\n",
      "Epoch 930 | Batch 0 | Train Loss: 149.81888 | Validation Loss: 26.922455\n",
      "Epoch 931 | Batch 0 | Train Loss: 146.02873 | Validation Loss: 26.717304\n",
      "Epoch 932 | Batch 0 | Train Loss: 149.14276 | Validation Loss: 30.306463\n",
      "Epoch 933 | Batch 0 | Train Loss: 152.55203 | Validation Loss: 29.858498\n",
      "Epoch 934 | Batch 0 | Train Loss: 147.2012 | Validation Loss: 31.867382\n",
      "Epoch 935 | Batch 0 | Train Loss: 149.5072 | Validation Loss: 31.492985\n",
      "Epoch 936 | Batch 0 | Train Loss: 150.31546 | Validation Loss: 31.837189\n",
      "Epoch 937 | Batch 0 | Train Loss: 146.76616 | Validation Loss: 24.507385\n",
      "Epoch 938 | Batch 0 | Train Loss: 146.96432 | Validation Loss: 26.321926\n",
      "Epoch 939 | Batch 0 | Train Loss: 145.08528 | Validation Loss: 30.573647\n",
      "Epoch 940 | Batch 0 | Train Loss: 149.96082 | Validation Loss: 33.25154\n",
      "Epoch 941 | Batch 0 | Train Loss: 153.54231 | Validation Loss: 31.678413\n",
      "Epoch 942 | Batch 0 | Train Loss: 150.89368 | Validation Loss: 33.746693\n",
      "Epoch 943 | Batch 0 | Train Loss: 144.76756 | Validation Loss: 28.005684\n",
      "Epoch 944 | Batch 0 | Train Loss: 151.36426 | Validation Loss: 29.44838\n",
      "Epoch 945 | Batch 0 | Train Loss: 149.93169 | Validation Loss: 36.865314\n",
      "Epoch 946 | Batch 0 | Train Loss: 148.20679 | Validation Loss: 25.482323\n",
      "Epoch 947 | Batch 0 | Train Loss: 147.82631 | Validation Loss: 33.6964\n",
      "Epoch 948 | Batch 0 | Train Loss: 148.76909 | Validation Loss: 28.7762\n",
      "Epoch 949 | Batch 0 | Train Loss: 148.97284 | Validation Loss: 27.064066\n",
      "Epoch 950 | Batch 0 | Train Loss: 144.13593 | Validation Loss: 26.446995\n",
      "Epoch 951 | Batch 0 | Train Loss: 147.97781 | Validation Loss: 31.676025\n",
      "Epoch 952 | Batch 0 | Train Loss: 146.37254 | Validation Loss: 26.56863\n",
      "Epoch 953 | Batch 0 | Train Loss: 145.10945 | Validation Loss: 27.852413\n",
      "Epoch 954 | Batch 0 | Train Loss: 146.23059 | Validation Loss: 28.285763\n",
      "Epoch 955 | Batch 0 | Train Loss: 142.80536 | Validation Loss: 27.64317\n",
      "Epoch 956 | Batch 0 | Train Loss: 150.83267 | Validation Loss: 28.722433\n",
      "Epoch 957 | Batch 0 | Train Loss: 148.9345 | Validation Loss: 34.234882\n",
      "Epoch 958 | Batch 0 | Train Loss: 152.1697 | Validation Loss: 23.862057\n",
      "Epoch 959 | Batch 0 | Train Loss: 154.90018 | Validation Loss: 26.483871\n",
      "Epoch 960 | Batch 0 | Train Loss: 152.65279 | Validation Loss: 31.84436\n",
      "Epoch 961 | Batch 0 | Train Loss: 152.75926 | Validation Loss: 25.409817\n",
      "Epoch 962 | Batch 0 | Train Loss: 151.20049 | Validation Loss: 29.550423\n",
      "Epoch 963 | Batch 0 | Train Loss: 156.1629 | Validation Loss: 26.034752\n",
      "Epoch 964 | Batch 0 | Train Loss: 145.84587 | Validation Loss: 26.924986\n",
      "Epoch 965 | Batch 0 | Train Loss: 149.50562 | Validation Loss: 28.13007\n",
      "Epoch 966 | Batch 0 | Train Loss: 145.69586 | Validation Loss: 27.754553\n",
      "Epoch 967 | Batch 0 | Train Loss: 144.98743 | Validation Loss: 27.297426\n",
      "Epoch 968 | Batch 0 | Train Loss: 150.2602 | Validation Loss: 29.336143\n",
      "Epoch 969 | Batch 0 | Train Loss: 144.43735 | Validation Loss: 29.967123\n",
      "Epoch 970 | Batch 0 | Train Loss: 151.72733 | Validation Loss: 27.043886\n",
      "Epoch 971 | Batch 0 | Train Loss: 147.69687 | Validation Loss: 31.036861\n",
      "Epoch 972 | Batch 0 | Train Loss: 154.23936 | Validation Loss: 33.3122\n",
      "Epoch 973 | Batch 0 | Train Loss: 147.71143 | Validation Loss: 34.36666\n",
      "Epoch 974 | Batch 0 | Train Loss: 148.87871 | Validation Loss: 34.22578\n",
      "Epoch 975 | Batch 0 | Train Loss: 151.46182 | Validation Loss: 25.32141\n",
      "Epoch 976 | Batch 0 | Train Loss: 147.70819 | Validation Loss: 31.901634\n",
      "Epoch 977 | Batch 0 | Train Loss: 146.81607 | Validation Loss: 28.98052\n",
      "Epoch 978 | Batch 0 | Train Loss: 148.34357 | Validation Loss: 26.790035\n",
      "Epoch 979 | Batch 0 | Train Loss: 153.06458 | Validation Loss: 30.8709\n",
      "Epoch 980 | Batch 0 | Train Loss: 152.83249 | Validation Loss: 32.670063\n",
      "Epoch 981 | Batch 0 | Train Loss: 145.82358 | Validation Loss: 26.507708\n",
      "Epoch 982 | Batch 0 | Train Loss: 144.97339 | Validation Loss: 22.074816\n",
      "Epoch 983 | Batch 0 | Train Loss: 151.44312 | Validation Loss: 29.337614\n",
      "Epoch 984 | Batch 0 | Train Loss: 149.29599 | Validation Loss: 28.713402\n",
      "Epoch 985 | Batch 0 | Train Loss: 144.98022 | Validation Loss: 25.106735\n",
      "Epoch 986 | Batch 0 | Train Loss: 145.63722 | Validation Loss: 33.217953\n",
      "Epoch 987 | Batch 0 | Train Loss: 149.54742 | Validation Loss: 32.888763\n",
      "Epoch 988 | Batch 0 | Train Loss: 144.8797 | Validation Loss: 30.857246\n",
      "Epoch 989 | Batch 0 | Train Loss: 145.13756 | Validation Loss: 28.116535\n",
      "Epoch 990 | Batch 0 | Train Loss: 143.76187 | Validation Loss: 27.39505\n",
      "Epoch 991 | Batch 0 | Train Loss: 148.49478 | Validation Loss: 29.314775\n",
      "Epoch 992 | Batch 0 | Train Loss: 141.8984 | Validation Loss: 27.846771\n",
      "Epoch 993 | Batch 0 | Train Loss: 150.14816 | Validation Loss: 26.579346\n",
      "Epoch 994 | Batch 0 | Train Loss: 147.09676 | Validation Loss: 22.010406\n",
      "Epoch 995 | Batch 0 | Train Loss: 148.44336 | Validation Loss: 26.788315\n",
      "Epoch 996 | Batch 0 | Train Loss: 151.49576 | Validation Loss: 29.001083\n",
      "Epoch 997 | Batch 0 | Train Loss: 147.70187 | Validation Loss: 37.357056\n",
      "Epoch 998 | Batch 0 | Train Loss: 148.15399 | Validation Loss: 31.571644\n",
      "Epoch 999 | Batch 0 | Train Loss: 150.55301 | Validation Loss: 28.216497\n",
      "Model saved in path: ./model/my_test_model-1000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAv4AAAH0CAYAAABFFilCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4C+EEEIIIcQIIIG/EEIIIYQQI8D/A9J9jv2u6iJNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efdf222f710>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 383
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch = 1000\n",
    "interval = 100\n",
    "batch_size = 10\n",
    "n_batches = train_X.shape[0]//batch_size\n",
    "\n",
    "# Layer's sizes\n",
    "x_size = train_X.shape[1]   # Number of input nodes: 3 features and 1 bias\n",
    "h_size = 56                 # Number of hidden nodes\n",
    "y_size = train_y.shape[1]   # Number of outcomes (8 classes)\n",
    "\n",
    "# Symbols\n",
    "X = tf.placeholder(tf.float32, shape=[None, x_size], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, y_size], name='y')\n",
    "\n",
    "# Weight initializations\n",
    "w1 = tf.Variable(tf.random_normal(shape=(x_size, h_size)))\n",
    "b1 = tf.Variable(tf.random_normal(shape=[h_size]))\n",
    "w2 = tf.Variable(tf.random_normal(shape=(h_size, y_size)))\n",
    "b2 = tf.Variable(tf.random_normal(shape=[y_size]))\n",
    "\n",
    "# Operations\n",
    "hidden_output = tf.nn.sigmoid(tf.add(tf.matmul(X, w1), b1))\n",
    "dropout = tf.contrib.layers.dropout(hidden_output, keep_prob=0.5)\n",
    "final_output = tf.nn.softmax(tf.add(tf.matmul(dropout, w2), b2), name='final_output')\n",
    "\n",
    "# Cost Function\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(final_output), axis=0))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Run SGD\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training\n",
    "print('Training the model...')\n",
    "losses = {'train':[], 'validation':[]}\n",
    "\n",
    "for e in range(epoch):\n",
    "    idxs = np.random.permutation(train_X.shape[0]) #shuffled ordering\n",
    "    random_X = train_X[idxs]\n",
    "    random_y = train_y[idxs]\n",
    "    for i in range(n_batches):\n",
    "        batch_X = random_X[i * batch_size:(i+1) * batch_size]\n",
    "        batch_y = random_y[i * batch_size:(i+1) * batch_size]\n",
    "        sess.run(optimizer,feed_dict = {X: batch_X, y:batch_y})\n",
    "        \n",
    "        if i % interval == 0:\n",
    "            train_loss = sess.run(loss, feed_dict={X: train_X, y: train_y})\n",
    "            val_loss   = sess.run(loss, feed_dict={X: val_X, y: val_y})\n",
    "            print('Epoch', e, '|',\n",
    "                  'Batch', i, '|',\n",
    "                  'Train Loss:', train_loss , '|',\n",
    "                  'Validation Loss:', val_loss)\n",
    "            losses['train'].append(train_loss)\n",
    "            losses['validation'].append(val_loss)\n",
    "\n",
    "save_path = saver.save(sess, './model/my_test_model',global_step=1000)\n",
    "print(\"Model saved in path: %s\" % save_path)\n",
    "sess.close()\n",
    "\n",
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/my_test_model-1000\n",
      "Accuracy: 0.7583732057416268\n",
      "Confusion Matrix:\n",
      "[[234  38]\n",
      " [ 63  83]]\n"
     ]
    }
   ],
   "source": [
    "# Import model\n",
    "with tf.Session() as sess:    \n",
    "    saver = tf.train.import_meta_graph('./model/my_test_model-1000.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./model'))\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    final_output = graph.get_tensor_by_name(\"final_output:0\")\n",
    "    \n",
    "    success = 0\n",
    "    results = {'actual':[], 'predicted':[]}\n",
    "    for i in range(len(test_X)):\n",
    "        predicted = sess.run(final_output, feed_dict={X: [test_X[i]]})\n",
    "        results['actual'].append(np.argmax(test_y[i]))\n",
    "        results['predicted'].append(np.argmax(predicted))\n",
    "        if np.argmax(test_y[i]) == np.argmax(predicted):\n",
    "            success += 1\n",
    "        #print('Actual:', test_y[i], 'Predicted:', np.rint(predicted))\n",
    "    \n",
    "    print('Accuracy:', success/len(test_X))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(results['actual'], results['predicted']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
