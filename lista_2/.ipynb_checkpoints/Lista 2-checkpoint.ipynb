{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparando conjunto de treino e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processDataset(df):\n",
    "    \"\"\"new_ages = []\n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row['Age'], str):\n",
    "            age = row['Age'].replace(\",\", \".\")\n",
    "            age = float(age)\n",
    "            new_ages.append(age)\n",
    "        else:\n",
    "            new_ages.append(row['Age'])\n",
    "\n",
    "    df['Age'] = new_ages\"\"\"\n",
    "    \n",
    "    # Insere os valores faltando em Age\n",
    "    mean_age = df['Age'].mean()\n",
    "    std_age = df['Age'].std() # desvio padrão\n",
    "    min_value = mean_age - std_age\n",
    "    max_value = mean_age + std_age\n",
    "\n",
    "    size = df['Age'].isnull().sum()\n",
    "\n",
    "    df.loc[df['Age'].isnull(), 'Age'] = np.random.randint(min_value, max_value, size=size)\n",
    "\n",
    "    df['Age'] = df['Age'].astype(int)\n",
    "    \n",
    "    # Insere os valores faltando em Embarked\n",
    "    embarked_values = ['C', 'Q', 'S']\n",
    "    size = df['Embarked'].isnull().sum()\n",
    "    random_values = [embarked_values[i] for i in np.random.randint(0, 3, size=size)]\n",
    "    df.loc[df['Embarked'].isnull(), 'Embarked'] = random_values\n",
    "    \n",
    "    df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "    \n",
    "    #df['Sex'] = df['Sex'].map({'male': 0, 'famale': 1})\n",
    "    new_sex = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Sex'] == 'male':\n",
    "            new_sex.append(0)\n",
    "        else:\n",
    "            new_sex.append(1)\n",
    "\n",
    "    df['Sex'] = new_sex\n",
    "    \n",
    "    # Faz o one hot encode em Sex, Embaked, SibSp e Parch, Pclass\n",
    "    # o  parâmetro'categories' garante que mesmo que o dado esteja faltando será considerado\n",
    "    sex = df['Sex'].astype(CategoricalDtype(categories=[0, 1]))\n",
    "    embarked = df['Embarked'].astype(CategoricalDtype(categories=[0, 1, 2]))\n",
    "    parch = df['Parch'].astype(CategoricalDtype(categories=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "    sibsp = df['SibSp'].astype(CategoricalDtype(categories=[0, 1, 2, 3, 4, 5, 6, 7, 8]))\n",
    "    pclass = df['Pclass'].astype(CategoricalDtype(categories=[1, 2, 3]))\n",
    "    \n",
    "    res = pd.concat([df,pd.get_dummies(sex, prefix='Sex')], axis=1)\n",
    "    res = pd.concat([res,pd.get_dummies(parch, prefix='Parch')], axis=1)\n",
    "    res = pd.concat([res,pd.get_dummies(sibsp, prefix='SibSp')], axis=1)\n",
    "    res = pd.concat([res,pd.get_dummies(embarked, prefix='Embarked')], axis=1)\n",
    "    res = pd.concat([res,pd.get_dummies(pclass, prefix='Pclass')], axis=1)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def clearColumns(df, cols):\n",
    "    df.drop(cols, inplace=True, axis='columns')\n",
    "    return df\n",
    "\n",
    "def addSurvivedCol(df_test, df_full):\n",
    "    survived = []\n",
    "    \n",
    "    names = df_test['Name']\n",
    "    \n",
    "    for name in names:\n",
    "        line = df_full.loc[df_full['Name'] == name]\n",
    "        survived.append(line['Survived'])\n",
    "    \n",
    "    df_test['Survived'] = survived\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "def correlation(df, fields, objective):\n",
    "    correlation_list = {}\n",
    "    for col in fields:\n",
    "        correl = np.corrcoef(df[col], df[objective])\n",
    "        correlation_list[col] = correl[0][1]\n",
    "\n",
    "    correlation_list = sorted(correlation_list.items(), key=lambda x:x[1], reverse=True)\n",
    "    for i in range(len(correlation_list)):\n",
    "        print(str(correlation_list[i][0]) + ': ' + str(correlation_list[i][1]))\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    one_hot = np.zeros([len(x), 2])\n",
    "    for i in enumerate(x):\n",
    "        one_hot[(i[0], i[1])] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_0</th>\n",
       "      <th>Sex_1</th>\n",
       "      <th>Parch_0</th>\n",
       "      <th>...</th>\n",
       "      <th>SibSp_5</th>\n",
       "      <th>SibSp_6</th>\n",
       "      <th>SibSp_7</th>\n",
       "      <th>SibSp_8</th>\n",
       "      <th>Embarked_0</th>\n",
       "      <th>Embarked_1</th>\n",
       "      <th>Embarked_2</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Survived  Sex  Age  SibSp  Parch  Embarked  Sex_0  Sex_1  Parch_0  \\\n",
       "0       1         1    1   29      0      0         2      0      1        1   \n",
       "\n",
       "     ...     SibSp_5  SibSp_6  SibSp_7  SibSp_8  Embarked_0  Embarked_1  \\\n",
       "0    ...           0        0        0        0           0           0   \n",
       "\n",
       "   Embarked_2  Pclass_1  Pclass_2  Pclass_3  \n",
       "0           1         1         0         0  \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.read_csv('titanic.csv', usecols=fields)\n",
    "\n",
    "df_full = processDataset(df_full)\n",
    "\n",
    "df_full.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived: 1.0\n",
      "Sex: 0.5286930913889885\n",
      "Parch: 0.08265957038609868\n",
      "SibSp: -0.02782511923058212\n",
      "Age: -0.047191797033877235\n",
      "Embarked: -0.17650839204362792\n",
      "Pclass: -0.3124693626496761\n"
     ]
    }
   ],
   "source": [
    "correlation(df_full, fields, 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_full = clearColumns(df_full, ['Sex', 'Embarked', 'Pclass', 'Parch', 'SibSp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separa os dados\n",
    "data_X = df_full.drop(['Survived'], inplace=False, axis='columns').values.tolist()\n",
    "data_y = df_full['Survived'].values.tolist()\n",
    "\n",
    "data_X, data_y = shuffle(data_X, data_y, random_state=0)\n",
    "data_X = np.array(data_X)\n",
    "data_y = np.array(data_y)\n",
    "data_y = np.reshape(data_y, (len(data_y), 1))\n",
    "\n",
    "train_size = 800\n",
    "val_size = 200\n",
    "\n",
    "train_X, val_X = data_X[:train_size], data_X[train_size:]\n",
    "train_y, val_y = data_y[:train_size], data_y[train_size:]\n",
    "\n",
    "val_X, test_X = val_X[:val_size], val_X[val_size:]\n",
    "val_y, test_y = val_y[:val_size], val_y[val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 0 | Batch 0 | Train Loss: 3.6537085 | Validation Loss: 3.654295\n",
      "Epoch 1 | Batch 0 | Train Loss: 2.3717203 | Validation Loss: 2.390163\n",
      "Epoch 2 | Batch 0 | Train Loss: 1.4523709 | Validation Loss: 1.46454\n",
      "Epoch 3 | Batch 0 | Train Loss: 0.92528856 | Validation Loss: 0.92378867\n",
      "Epoch 4 | Batch 0 | Train Loss: 0.7051271 | Validation Loss: 0.70179933\n",
      "Epoch 5 | Batch 0 | Train Loss: 0.6514433 | Validation Loss: 0.6508483\n",
      "Epoch 6 | Batch 0 | Train Loss: 0.6311755 | Validation Loss: 0.63661367\n",
      "Epoch 7 | Batch 0 | Train Loss: 0.62309057 | Validation Loss: 0.6325482\n",
      "Epoch 8 | Batch 0 | Train Loss: 0.62105036 | Validation Loss: 0.634135\n",
      "Epoch 9 | Batch 0 | Train Loss: 0.6113569 | Validation Loss: 0.6267489\n",
      "Epoch 10 | Batch 0 | Train Loss: 0.60483336 | Validation Loss: 0.6211956\n",
      "Epoch 11 | Batch 0 | Train Loss: 0.6005437 | Validation Loss: 0.6185219\n",
      "Epoch 12 | Batch 0 | Train Loss: 0.60330564 | Validation Loss: 0.62460685\n",
      "Epoch 13 | Batch 0 | Train Loss: 0.5915348 | Validation Loss: 0.61208034\n",
      "Epoch 14 | Batch 0 | Train Loss: 0.58825445 | Validation Loss: 0.6079617\n",
      "Epoch 15 | Batch 0 | Train Loss: 0.58310205 | Validation Loss: 0.60382646\n",
      "Epoch 16 | Batch 0 | Train Loss: 0.57970136 | Validation Loss: 0.60014725\n",
      "Epoch 17 | Batch 0 | Train Loss: 0.5780132 | Validation Loss: 0.60060376\n",
      "Epoch 18 | Batch 0 | Train Loss: 0.56854475 | Validation Loss: 0.5896798\n",
      "Epoch 19 | Batch 0 | Train Loss: 0.55713433 | Validation Loss: 0.57635045\n",
      "Epoch 20 | Batch 0 | Train Loss: 0.55495787 | Validation Loss: 0.5768173\n",
      "Epoch 21 | Batch 0 | Train Loss: 0.5459454 | Validation Loss: 0.5667957\n",
      "Epoch 22 | Batch 0 | Train Loss: 0.5381581 | Validation Loss: 0.5612482\n",
      "Epoch 23 | Batch 0 | Train Loss: 0.5310161 | Validation Loss: 0.55662507\n",
      "Epoch 24 | Batch 0 | Train Loss: 0.52687275 | Validation Loss: 0.55704683\n",
      "Epoch 25 | Batch 0 | Train Loss: 0.52087027 | Validation Loss: 0.55838275\n",
      "Epoch 26 | Batch 0 | Train Loss: 0.50406027 | Validation Loss: 0.5446299\n",
      "Epoch 27 | Batch 0 | Train Loss: 0.5004518 | Validation Loss: 0.54256576\n",
      "Epoch 28 | Batch 0 | Train Loss: 0.4899092 | Validation Loss: 0.5325619\n",
      "Epoch 29 | Batch 0 | Train Loss: 0.48189223 | Validation Loss: 0.5311291\n",
      "Epoch 30 | Batch 0 | Train Loss: 0.47558153 | Validation Loss: 0.52908087\n",
      "Epoch 31 | Batch 0 | Train Loss: 0.47229326 | Validation Loss: 0.52907753\n",
      "Epoch 32 | Batch 0 | Train Loss: 0.45783067 | Validation Loss: 0.5173297\n",
      "Epoch 33 | Batch 0 | Train Loss: 0.45476067 | Validation Loss: 0.51755667\n",
      "Epoch 34 | Batch 0 | Train Loss: 0.4498172 | Validation Loss: 0.512765\n",
      "Epoch 35 | Batch 0 | Train Loss: 0.4446496 | Validation Loss: 0.50804913\n",
      "Epoch 36 | Batch 0 | Train Loss: 0.44228423 | Validation Loss: 0.5056759\n",
      "Epoch 37 | Batch 0 | Train Loss: 0.44497445 | Validation Loss: 0.50960886\n",
      "Epoch 38 | Batch 0 | Train Loss: 0.43727344 | Validation Loss: 0.50332063\n",
      "Epoch 39 | Batch 0 | Train Loss: 0.4379335 | Validation Loss: 0.50574416\n",
      "Epoch 40 | Batch 0 | Train Loss: 0.44155666 | Validation Loss: 0.51144266\n",
      "Epoch 41 | Batch 0 | Train Loss: 0.43275154 | Validation Loss: 0.5053555\n",
      "Epoch 42 | Batch 0 | Train Loss: 0.4317669 | Validation Loss: 0.5045765\n",
      "Epoch 43 | Batch 0 | Train Loss: 0.4356636 | Validation Loss: 0.50946\n",
      "Epoch 44 | Batch 0 | Train Loss: 0.4291761 | Validation Loss: 0.505622\n",
      "Epoch 45 | Batch 0 | Train Loss: 0.42681813 | Validation Loss: 0.5058294\n",
      "Epoch 46 | Batch 0 | Train Loss: 0.42672676 | Validation Loss: 0.50816953\n",
      "Epoch 47 | Batch 0 | Train Loss: 0.42631423 | Validation Loss: 0.5076493\n",
      "Epoch 48 | Batch 0 | Train Loss: 0.42925262 | Validation Loss: 0.51185083\n",
      "Epoch 49 | Batch 0 | Train Loss: 0.42578983 | Validation Loss: 0.5093838\n",
      "Epoch 50 | Batch 0 | Train Loss: 0.42654204 | Validation Loss: 0.5105136\n",
      "Epoch 51 | Batch 0 | Train Loss: 0.42466733 | Validation Loss: 0.50751054\n",
      "Epoch 52 | Batch 0 | Train Loss: 0.4223047 | Validation Loss: 0.50710106\n",
      "Epoch 53 | Batch 0 | Train Loss: 0.4215938 | Validation Loss: 0.508981\n",
      "Epoch 54 | Batch 0 | Train Loss: 0.42429423 | Validation Loss: 0.51065075\n",
      "Epoch 55 | Batch 0 | Train Loss: 0.42093453 | Validation Loss: 0.5065396\n",
      "Epoch 56 | Batch 0 | Train Loss: 0.42353076 | Validation Loss: 0.5089805\n",
      "Epoch 57 | Batch 0 | Train Loss: 0.41958702 | Validation Loss: 0.5038722\n",
      "Epoch 58 | Batch 0 | Train Loss: 0.4227249 | Validation Loss: 0.50520486\n",
      "Epoch 59 | Batch 0 | Train Loss: 0.4187292 | Validation Loss: 0.5018935\n",
      "Epoch 60 | Batch 0 | Train Loss: 0.41751808 | Validation Loss: 0.50351113\n",
      "Epoch 61 | Batch 0 | Train Loss: 0.4162258 | Validation Loss: 0.503724\n",
      "Epoch 62 | Batch 0 | Train Loss: 0.41643453 | Validation Loss: 0.5036486\n",
      "Epoch 63 | Batch 0 | Train Loss: 0.41667545 | Validation Loss: 0.5029698\n",
      "Epoch 64 | Batch 0 | Train Loss: 0.41508475 | Validation Loss: 0.50347054\n",
      "Epoch 65 | Batch 0 | Train Loss: 0.4136391 | Validation Loss: 0.506946\n",
      "Epoch 66 | Batch 0 | Train Loss: 0.41544932 | Validation Loss: 0.5040586\n",
      "Epoch 67 | Batch 0 | Train Loss: 0.4189104 | Validation Loss: 0.5084189\n",
      "Epoch 68 | Batch 0 | Train Loss: 0.4149288 | Validation Loss: 0.5057669\n",
      "Epoch 69 | Batch 0 | Train Loss: 0.4132605 | Validation Loss: 0.50504065\n",
      "Epoch 70 | Batch 0 | Train Loss: 0.4131461 | Validation Loss: 0.50383496\n",
      "Epoch 71 | Batch 0 | Train Loss: 0.41301402 | Validation Loss: 0.5038979\n",
      "Epoch 72 | Batch 0 | Train Loss: 0.41097108 | Validation Loss: 0.5047766\n",
      "Epoch 73 | Batch 0 | Train Loss: 0.41090217 | Validation Loss: 0.5057916\n",
      "Epoch 74 | Batch 0 | Train Loss: 0.41028222 | Validation Loss: 0.50438523\n",
      "Epoch 75 | Batch 0 | Train Loss: 0.41036928 | Validation Loss: 0.5043954\n",
      "Epoch 76 | Batch 0 | Train Loss: 0.4107204 | Validation Loss: 0.5044032\n",
      "Epoch 77 | Batch 0 | Train Loss: 0.4102126 | Validation Loss: 0.5048041\n",
      "Epoch 78 | Batch 0 | Train Loss: 0.41552117 | Validation Loss: 0.5066585\n",
      "Epoch 79 | Batch 0 | Train Loss: 0.4092282 | Validation Loss: 0.5069954\n",
      "Epoch 80 | Batch 0 | Train Loss: 0.4084813 | Validation Loss: 0.50585496\n",
      "Epoch 81 | Batch 0 | Train Loss: 0.40839672 | Validation Loss: 0.5065725\n",
      "Epoch 82 | Batch 0 | Train Loss: 0.40865275 | Validation Loss: 0.5054671\n",
      "Epoch 83 | Batch 0 | Train Loss: 0.41040915 | Validation Loss: 0.50735015\n",
      "Epoch 84 | Batch 0 | Train Loss: 0.40797234 | Validation Loss: 0.5077469\n",
      "Epoch 85 | Batch 0 | Train Loss: 0.40732723 | Validation Loss: 0.5073789\n",
      "Epoch 86 | Batch 0 | Train Loss: 0.4105612 | Validation Loss: 0.5084521\n",
      "Epoch 87 | Batch 0 | Train Loss: 0.4068731 | Validation Loss: 0.5091992\n",
      "Epoch 88 | Batch 0 | Train Loss: 0.40679726 | Validation Loss: 0.5083366\n",
      "Epoch 89 | Batch 0 | Train Loss: 0.40891403 | Validation Loss: 0.5105455\n",
      "Epoch 90 | Batch 0 | Train Loss: 0.40743315 | Validation Loss: 0.51048493\n",
      "Epoch 91 | Batch 0 | Train Loss: 0.4057301 | Validation Loss: 0.5118503\n",
      "Epoch 92 | Batch 0 | Train Loss: 0.40619713 | Validation Loss: 0.5111887\n",
      "Epoch 93 | Batch 0 | Train Loss: 0.4131601 | Validation Loss: 0.5128203\n",
      "Epoch 94 | Batch 0 | Train Loss: 0.40572232 | Validation Loss: 0.509917\n",
      "Epoch 95 | Batch 0 | Train Loss: 0.41385293 | Validation Loss: 0.512093\n",
      "Epoch 96 | Batch 0 | Train Loss: 0.40571785 | Validation Loss: 0.51121783\n",
      "Epoch 97 | Batch 0 | Train Loss: 0.40496436 | Validation Loss: 0.51029456\n",
      "Epoch 98 | Batch 0 | Train Loss: 0.40450943 | Validation Loss: 0.50888586\n",
      "Epoch 99 | Batch 0 | Train Loss: 0.4039583 | Validation Loss: 0.5086403\n",
      "Epoch 100 | Batch 0 | Train Loss: 0.4070014 | Validation Loss: 0.5092757\n",
      "Epoch 101 | Batch 0 | Train Loss: 0.40361118 | Validation Loss: 0.50976205\n",
      "Epoch 102 | Batch 0 | Train Loss: 0.402876 | Validation Loss: 0.5113995\n",
      "Epoch 103 | Batch 0 | Train Loss: 0.40648055 | Validation Loss: 0.51053095\n",
      "Epoch 104 | Batch 0 | Train Loss: 0.4027892 | Validation Loss: 0.5084079\n",
      "Epoch 105 | Batch 0 | Train Loss: 0.4023487 | Validation Loss: 0.50767225\n",
      "Epoch 106 | Batch 0 | Train Loss: 0.4040724 | Validation Loss: 0.50834084\n",
      "Epoch 107 | Batch 0 | Train Loss: 0.40175807 | Validation Loss: 0.5092977\n",
      "Epoch 108 | Batch 0 | Train Loss: 0.4017495 | Validation Loss: 0.5087452\n",
      "Epoch 109 | Batch 0 | Train Loss: 0.40122104 | Validation Loss: 0.50910807\n",
      "Epoch 110 | Batch 0 | Train Loss: 0.40154988 | Validation Loss: 0.5085281\n",
      "Epoch 111 | Batch 0 | Train Loss: 0.40122548 | Validation Loss: 0.51086295\n",
      "Epoch 112 | Batch 0 | Train Loss: 0.400922 | Validation Loss: 0.50829947\n",
      "Epoch 113 | Batch 0 | Train Loss: 0.40059808 | Validation Loss: 0.50904477\n",
      "Epoch 114 | Batch 0 | Train Loss: 0.40051648 | Validation Loss: 0.51022416\n",
      "Epoch 115 | Batch 0 | Train Loss: 0.40073913 | Validation Loss: 0.5080531\n",
      "Epoch 116 | Batch 0 | Train Loss: 0.40049598 | Validation Loss: 0.5073751\n",
      "Epoch 117 | Batch 0 | Train Loss: 0.3996239 | Validation Loss: 0.5074841\n",
      "Epoch 118 | Batch 0 | Train Loss: 0.39971352 | Validation Loss: 0.50565153\n",
      "Epoch 119 | Batch 0 | Train Loss: 0.39932793 | Validation Loss: 0.5079367\n",
      "Epoch 120 | Batch 0 | Train Loss: 0.39923608 | Validation Loss: 0.50665927\n",
      "Epoch 121 | Batch 0 | Train Loss: 0.39899513 | Validation Loss: 0.50825775\n",
      "Epoch 122 | Batch 0 | Train Loss: 0.3980867 | Validation Loss: 0.5106827\n",
      "Epoch 123 | Batch 0 | Train Loss: 0.39766753 | Validation Loss: 0.50849414\n",
      "Epoch 124 | Batch 0 | Train Loss: 0.39810342 | Validation Loss: 0.5070364\n",
      "Epoch 125 | Batch 0 | Train Loss: 0.39813033 | Validation Loss: 0.50737745\n",
      "Epoch 126 | Batch 0 | Train Loss: 0.3981657 | Validation Loss: 0.51217544\n",
      "Epoch 127 | Batch 0 | Train Loss: 0.3970768 | Validation Loss: 0.5084634\n",
      "Epoch 128 | Batch 0 | Train Loss: 0.39704272 | Validation Loss: 0.50993395\n",
      "Epoch 129 | Batch 0 | Train Loss: 0.39739457 | Validation Loss: 0.50881195\n",
      "Epoch 130 | Batch 0 | Train Loss: 0.3967788 | Validation Loss: 0.5085106\n",
      "Epoch 131 | Batch 0 | Train Loss: 0.3970793 | Validation Loss: 0.51282716\n",
      "Epoch 132 | Batch 0 | Train Loss: 0.3970192 | Validation Loss: 0.50858736\n",
      "Epoch 133 | Batch 0 | Train Loss: 0.3962101 | Validation Loss: 0.5113384\n",
      "Epoch 134 | Batch 0 | Train Loss: 0.3959259 | Validation Loss: 0.50861937\n",
      "Epoch 135 | Batch 0 | Train Loss: 0.39563012 | Validation Loss: 0.5086567\n",
      "Epoch 136 | Batch 0 | Train Loss: 0.3952552 | Validation Loss: 0.510304\n",
      "Epoch 137 | Batch 0 | Train Loss: 0.3949513 | Validation Loss: 0.5101033\n",
      "Epoch 138 | Batch 0 | Train Loss: 0.3957732 | Validation Loss: 0.50816524\n",
      "Epoch 139 | Batch 0 | Train Loss: 0.3949093 | Validation Loss: 0.5107593\n",
      "Epoch 140 | Batch 0 | Train Loss: 0.3952258 | Validation Loss: 0.5078371\n",
      "Epoch 141 | Batch 0 | Train Loss: 0.39467466 | Validation Loss: 0.5088352\n",
      "Epoch 142 | Batch 0 | Train Loss: 0.39523643 | Validation Loss: 0.5146488\n",
      "Epoch 143 | Batch 0 | Train Loss: 0.39395127 | Validation Loss: 0.5114859\n",
      "Epoch 144 | Batch 0 | Train Loss: 0.3936524 | Validation Loss: 0.51159805\n",
      "Epoch 145 | Batch 0 | Train Loss: 0.39397186 | Validation Loss: 0.51001495\n",
      "Epoch 146 | Batch 0 | Train Loss: 0.3936628 | Validation Loss: 0.5106574\n",
      "Epoch 147 | Batch 0 | Train Loss: 0.3937178 | Validation Loss: 0.51063055\n",
      "Epoch 148 | Batch 0 | Train Loss: 0.3935253 | Validation Loss: 0.5094692\n",
      "Epoch 149 | Batch 0 | Train Loss: 0.39299464 | Validation Loss: 0.51057667\n",
      "Epoch 150 | Batch 0 | Train Loss: 0.39313084 | Validation Loss: 0.50938463\n",
      "Epoch 151 | Batch 0 | Train Loss: 0.39322335 | Validation Loss: 0.50912154\n",
      "Epoch 152 | Batch 0 | Train Loss: 0.39289883 | Validation Loss: 0.50932103\n",
      "Epoch 153 | Batch 0 | Train Loss: 0.39274216 | Validation Loss: 0.50951135\n",
      "Epoch 154 | Batch 0 | Train Loss: 0.39268875 | Validation Loss: 0.5089493\n",
      "Epoch 155 | Batch 0 | Train Loss: 0.3948595 | Validation Loss: 0.5096308\n",
      "Epoch 156 | Batch 0 | Train Loss: 0.39338845 | Validation Loss: 0.50937927\n",
      "Epoch 157 | Batch 0 | Train Loss: 0.3929182 | Validation Loss: 0.50923365\n",
      "Epoch 158 | Batch 0 | Train Loss: 0.39238608 | Validation Loss: 0.51039594\n",
      "Epoch 159 | Batch 0 | Train Loss: 0.39248154 | Validation Loss: 0.5113543\n",
      "Epoch 160 | Batch 0 | Train Loss: 0.39254615 | Validation Loss: 0.5093268\n",
      "Epoch 161 | Batch 0 | Train Loss: 0.39205596 | Validation Loss: 0.5121381\n",
      "Epoch 162 | Batch 0 | Train Loss: 0.39220336 | Validation Loss: 0.50947756\n",
      "Epoch 163 | Batch 0 | Train Loss: 0.39264834 | Validation Loss: 0.50929064\n",
      "Epoch 164 | Batch 0 | Train Loss: 0.3917962 | Validation Loss: 0.51026344\n",
      "Epoch 165 | Batch 0 | Train Loss: 0.3918657 | Validation Loss: 0.5092792\n",
      "Epoch 166 | Batch 0 | Train Loss: 0.39192972 | Validation Loss: 0.51127124\n",
      "Epoch 167 | Batch 0 | Train Loss: 0.3919358 | Validation Loss: 0.50999814\n",
      "Epoch 168 | Batch 0 | Train Loss: 0.39253724 | Validation Loss: 0.5092327\n",
      "Epoch 169 | Batch 0 | Train Loss: 0.39061752 | Validation Loss: 0.5106311\n",
      "Epoch 170 | Batch 0 | Train Loss: 0.39156795 | Validation Loss: 0.50976026\n",
      "Epoch 171 | Batch 0 | Train Loss: 0.39083412 | Validation Loss: 0.5105619\n",
      "Epoch 172 | Batch 0 | Train Loss: 0.39027274 | Validation Loss: 0.5100756\n",
      "Epoch 173 | Batch 0 | Train Loss: 0.39043617 | Validation Loss: 0.50951993\n",
      "Epoch 174 | Batch 0 | Train Loss: 0.39071333 | Validation Loss: 0.50900143\n",
      "Epoch 175 | Batch 0 | Train Loss: 0.39066243 | Validation Loss: 0.510407\n",
      "Epoch 176 | Batch 0 | Train Loss: 0.38961467 | Validation Loss: 0.5114234\n",
      "Epoch 177 | Batch 0 | Train Loss: 0.3895228 | Validation Loss: 0.51102585\n",
      "Epoch 178 | Batch 0 | Train Loss: 0.38925448 | Validation Loss: 0.5122119\n",
      "Epoch 179 | Batch 0 | Train Loss: 0.39057776 | Validation Loss: 0.5123875\n",
      "Epoch 180 | Batch 0 | Train Loss: 0.38988832 | Validation Loss: 0.50943285\n",
      "Epoch 181 | Batch 0 | Train Loss: 0.3892967 | Validation Loss: 0.51060045\n",
      "Epoch 182 | Batch 0 | Train Loss: 0.38830215 | Validation Loss: 0.51141167\n",
      "Epoch 183 | Batch 0 | Train Loss: 0.38824815 | Validation Loss: 0.5109658\n",
      "Epoch 184 | Batch 0 | Train Loss: 0.38863477 | Validation Loss: 0.5100316\n",
      "Epoch 185 | Batch 0 | Train Loss: 0.38824773 | Validation Loss: 0.51015073\n",
      "Epoch 186 | Batch 0 | Train Loss: 0.38931042 | Validation Loss: 0.512135\n",
      "Epoch 187 | Batch 0 | Train Loss: 0.38935256 | Validation Loss: 0.510264\n",
      "Epoch 188 | Batch 0 | Train Loss: 0.39026767 | Validation Loss: 0.51621455\n",
      "Epoch 189 | Batch 0 | Train Loss: 0.38812348 | Validation Loss: 0.5098396\n",
      "Epoch 190 | Batch 0 | Train Loss: 0.3890412 | Validation Loss: 0.50885993\n",
      "Epoch 191 | Batch 0 | Train Loss: 0.388974 | Validation Loss: 0.50802183\n",
      "Epoch 192 | Batch 0 | Train Loss: 0.3888536 | Validation Loss: 0.5082546\n",
      "Epoch 193 | Batch 0 | Train Loss: 0.38848105 | Validation Loss: 0.51251733\n",
      "Epoch 194 | Batch 0 | Train Loss: 0.38772833 | Validation Loss: 0.509577\n",
      "Epoch 195 | Batch 0 | Train Loss: 0.38700947 | Validation Loss: 0.5118087\n",
      "Epoch 196 | Batch 0 | Train Loss: 0.39192182 | Validation Loss: 0.5124227\n",
      "Epoch 197 | Batch 0 | Train Loss: 0.38810366 | Validation Loss: 0.51112944\n",
      "Epoch 198 | Batch 0 | Train Loss: 0.3870265 | Validation Loss: 0.51244867\n",
      "Epoch 199 | Batch 0 | Train Loss: 0.38760364 | Validation Loss: 0.5102436\n",
      "Epoch 200 | Batch 0 | Train Loss: 0.38734356 | Validation Loss: 0.51200426\n",
      "Epoch 201 | Batch 0 | Train Loss: 0.38689005 | Validation Loss: 0.511228\n",
      "Epoch 202 | Batch 0 | Train Loss: 0.38739982 | Validation Loss: 0.5091534\n",
      "Epoch 203 | Batch 0 | Train Loss: 0.38651332 | Validation Loss: 0.5118992\n",
      "Epoch 204 | Batch 0 | Train Loss: 0.3863636 | Validation Loss: 0.5125328\n",
      "Epoch 205 | Batch 0 | Train Loss: 0.3869094 | Validation Loss: 0.51206225\n",
      "Epoch 206 | Batch 0 | Train Loss: 0.38680694 | Validation Loss: 0.5114411\n",
      "Epoch 207 | Batch 0 | Train Loss: 0.3866358 | Validation Loss: 0.5114942\n",
      "Epoch 208 | Batch 0 | Train Loss: 0.38660905 | Validation Loss: 0.5120399\n",
      "Epoch 209 | Batch 0 | Train Loss: 0.38577533 | Validation Loss: 0.51315343\n",
      "Epoch 210 | Batch 0 | Train Loss: 0.3856363 | Validation Loss: 0.51248854\n",
      "Epoch 211 | Batch 0 | Train Loss: 0.3863147 | Validation Loss: 0.51101935\n",
      "Epoch 212 | Batch 0 | Train Loss: 0.38587323 | Validation Loss: 0.5121481\n",
      "Epoch 213 | Batch 0 | Train Loss: 0.38546672 | Validation Loss: 0.5128178\n",
      "Epoch 214 | Batch 0 | Train Loss: 0.38707817 | Validation Loss: 0.5183274\n",
      "Epoch 215 | Batch 0 | Train Loss: 0.38584426 | Validation Loss: 0.51076907\n",
      "Epoch 216 | Batch 0 | Train Loss: 0.38580644 | Validation Loss: 0.50957006\n",
      "Epoch 217 | Batch 0 | Train Loss: 0.38574135 | Validation Loss: 0.5106758\n",
      "Epoch 218 | Batch 0 | Train Loss: 0.38482445 | Validation Loss: 0.51181746\n",
      "Epoch 219 | Batch 0 | Train Loss: 0.38424632 | Validation Loss: 0.5132786\n",
      "Epoch 220 | Batch 0 | Train Loss: 0.38446754 | Validation Loss: 0.5119447\n",
      "Epoch 221 | Batch 0 | Train Loss: 0.3842376 | Validation Loss: 0.512568\n",
      "Epoch 222 | Batch 0 | Train Loss: 0.38427857 | Validation Loss: 0.5149458\n",
      "Epoch 223 | Batch 0 | Train Loss: 0.38579652 | Validation Loss: 0.517099\n",
      "Epoch 224 | Batch 0 | Train Loss: 0.3851939 | Validation Loss: 0.51057905\n",
      "Epoch 225 | Batch 0 | Train Loss: 0.38464493 | Validation Loss: 0.51148874\n",
      "Epoch 226 | Batch 0 | Train Loss: 0.38584182 | Validation Loss: 0.51091677\n",
      "Epoch 227 | Batch 0 | Train Loss: 0.38420475 | Validation Loss: 0.511429\n",
      "Epoch 228 | Batch 0 | Train Loss: 0.38428196 | Validation Loss: 0.51314366\n",
      "Epoch 229 | Batch 0 | Train Loss: 0.38401863 | Validation Loss: 0.5119365\n",
      "Epoch 230 | Batch 0 | Train Loss: 0.38369393 | Validation Loss: 0.5141153\n",
      "Epoch 231 | Batch 0 | Train Loss: 0.38314667 | Validation Loss: 0.51402783\n",
      "Epoch 232 | Batch 0 | Train Loss: 0.38362658 | Validation Loss: 0.5144781\n",
      "Epoch 233 | Batch 0 | Train Loss: 0.38332307 | Validation Loss: 0.5146292\n",
      "Epoch 234 | Batch 0 | Train Loss: 0.38500845 | Validation Loss: 0.51712286\n",
      "Epoch 235 | Batch 0 | Train Loss: 0.3837272 | Validation Loss: 0.51525795\n",
      "Epoch 236 | Batch 0 | Train Loss: 0.38339484 | Validation Loss: 0.5125575\n",
      "Epoch 237 | Batch 0 | Train Loss: 0.38398433 | Validation Loss: 0.5106916\n",
      "Epoch 238 | Batch 0 | Train Loss: 0.3830347 | Validation Loss: 0.5151272\n",
      "Epoch 239 | Batch 0 | Train Loss: 0.38257554 | Validation Loss: 0.5144313\n",
      "Epoch 240 | Batch 0 | Train Loss: 0.38238966 | Validation Loss: 0.51446694\n",
      "Epoch 241 | Batch 0 | Train Loss: 0.3827497 | Validation Loss: 0.514472\n",
      "Epoch 242 | Batch 0 | Train Loss: 0.38292778 | Validation Loss: 0.5139754\n",
      "Epoch 243 | Batch 0 | Train Loss: 0.38275158 | Validation Loss: 0.51492006\n",
      "Epoch 244 | Batch 0 | Train Loss: 0.3825757 | Validation Loss: 0.51670164\n",
      "Epoch 245 | Batch 0 | Train Loss: 0.38227654 | Validation Loss: 0.5156786\n",
      "Epoch 246 | Batch 0 | Train Loss: 0.38217232 | Validation Loss: 0.5140683\n",
      "Epoch 247 | Batch 0 | Train Loss: 0.38382044 | Validation Loss: 0.5123977\n",
      "Epoch 248 | Batch 0 | Train Loss: 0.38222703 | Validation Loss: 0.5121323\n",
      "Epoch 249 | Batch 0 | Train Loss: 0.38276035 | Validation Loss: 0.5115895\n",
      "Epoch 250 | Batch 0 | Train Loss: 0.38343498 | Validation Loss: 0.51220816\n",
      "Epoch 251 | Batch 0 | Train Loss: 0.3820854 | Validation Loss: 0.51322156\n",
      "Epoch 252 | Batch 0 | Train Loss: 0.38298187 | Validation Loss: 0.5138496\n",
      "Epoch 253 | Batch 0 | Train Loss: 0.38280585 | Validation Loss: 0.5177001\n",
      "Epoch 254 | Batch 0 | Train Loss: 0.38257962 | Validation Loss: 0.5168938\n",
      "Epoch 255 | Batch 0 | Train Loss: 0.38123038 | Validation Loss: 0.5156723\n",
      "Epoch 256 | Batch 0 | Train Loss: 0.38154212 | Validation Loss: 0.51573044\n",
      "Epoch 257 | Batch 0 | Train Loss: 0.3810192 | Validation Loss: 0.51397985\n",
      "Epoch 258 | Batch 0 | Train Loss: 0.3828633 | Validation Loss: 0.51960486\n",
      "Epoch 259 | Batch 0 | Train Loss: 0.38138658 | Validation Loss: 0.51247257\n",
      "Epoch 260 | Batch 0 | Train Loss: 0.38142502 | Validation Loss: 0.51266676\n",
      "Epoch 261 | Batch 0 | Train Loss: 0.3815442 | Validation Loss: 0.514195\n",
      "Epoch 262 | Batch 0 | Train Loss: 0.38163254 | Validation Loss: 0.51297307\n",
      "Epoch 263 | Batch 0 | Train Loss: 0.38110307 | Validation Loss: 0.5134826\n",
      "Epoch 264 | Batch 0 | Train Loss: 0.3809434 | Validation Loss: 0.5152974\n",
      "Epoch 265 | Batch 0 | Train Loss: 0.3808359 | Validation Loss: 0.5143872\n",
      "Epoch 266 | Batch 0 | Train Loss: 0.3805467 | Validation Loss: 0.5125386\n",
      "Epoch 267 | Batch 0 | Train Loss: 0.38029304 | Validation Loss: 0.5146844\n",
      "Epoch 268 | Batch 0 | Train Loss: 0.38064143 | Validation Loss: 0.5166084\n",
      "Epoch 269 | Batch 0 | Train Loss: 0.38020936 | Validation Loss: 0.514751\n",
      "Epoch 270 | Batch 0 | Train Loss: 0.3801647 | Validation Loss: 0.5185203\n",
      "Epoch 271 | Batch 0 | Train Loss: 0.38056943 | Validation Loss: 0.5178442\n",
      "Epoch 272 | Batch 0 | Train Loss: 0.38168743 | Validation Loss: 0.5121182\n",
      "Epoch 273 | Batch 0 | Train Loss: 0.3802278 | Validation Loss: 0.5133471\n",
      "Epoch 274 | Batch 0 | Train Loss: 0.3803177 | Validation Loss: 0.5150168\n",
      "Epoch 275 | Batch 0 | Train Loss: 0.38037768 | Validation Loss: 0.51375514\n",
      "Epoch 276 | Batch 0 | Train Loss: 0.3800573 | Validation Loss: 0.516278\n",
      "Epoch 277 | Batch 0 | Train Loss: 0.38124987 | Validation Loss: 0.5136398\n",
      "Epoch 278 | Batch 0 | Train Loss: 0.3796621 | Validation Loss: 0.51602495\n",
      "Epoch 279 | Batch 0 | Train Loss: 0.37994015 | Validation Loss: 0.5185711\n",
      "Epoch 280 | Batch 0 | Train Loss: 0.37924942 | Validation Loss: 0.5155102\n",
      "Epoch 281 | Batch 0 | Train Loss: 0.3796196 | Validation Loss: 0.51330584\n",
      "Epoch 282 | Batch 0 | Train Loss: 0.37951562 | Validation Loss: 0.5137637\n",
      "Epoch 283 | Batch 0 | Train Loss: 0.38003027 | Validation Loss: 0.51881045\n",
      "Epoch 284 | Batch 0 | Train Loss: 0.37944114 | Validation Loss: 0.51528746\n",
      "Epoch 285 | Batch 0 | Train Loss: 0.38004872 | Validation Loss: 0.5178959\n",
      "Epoch 286 | Batch 0 | Train Loss: 0.38040873 | Validation Loss: 0.5197073\n",
      "Epoch 287 | Batch 0 | Train Loss: 0.3786888 | Validation Loss: 0.51682633\n",
      "Epoch 288 | Batch 0 | Train Loss: 0.37886453 | Validation Loss: 0.5153632\n",
      "Epoch 289 | Batch 0 | Train Loss: 0.37857628 | Validation Loss: 0.51749873\n",
      "Epoch 290 | Batch 0 | Train Loss: 0.37885094 | Validation Loss: 0.51706064\n",
      "Epoch 291 | Batch 0 | Train Loss: 0.37866357 | Validation Loss: 0.5187174\n",
      "Epoch 292 | Batch 0 | Train Loss: 0.3789534 | Validation Loss: 0.51568705\n",
      "Epoch 293 | Batch 0 | Train Loss: 0.37924442 | Validation Loss: 0.5149345\n",
      "Epoch 294 | Batch 0 | Train Loss: 0.37878168 | Validation Loss: 0.51743215\n",
      "Epoch 295 | Batch 0 | Train Loss: 0.37919796 | Validation Loss: 0.51490176\n",
      "Epoch 296 | Batch 0 | Train Loss: 0.37861508 | Validation Loss: 0.514564\n",
      "Epoch 297 | Batch 0 | Train Loss: 0.37868947 | Validation Loss: 0.5143222\n",
      "Epoch 298 | Batch 0 | Train Loss: 0.37903073 | Validation Loss: 0.51534253\n",
      "Epoch 299 | Batch 0 | Train Loss: 0.37821066 | Validation Loss: 0.5159395\n",
      "Epoch 300 | Batch 0 | Train Loss: 0.37953156 | Validation Loss: 0.5210696\n",
      "Epoch 301 | Batch 0 | Train Loss: 0.37823504 | Validation Loss: 0.51699346\n",
      "Epoch 302 | Batch 0 | Train Loss: 0.3781775 | Validation Loss: 0.51767427\n",
      "Epoch 303 | Batch 0 | Train Loss: 0.3826359 | Validation Loss: 0.5264048\n",
      "Epoch 304 | Batch 0 | Train Loss: 0.37797156 | Validation Loss: 0.5171899\n",
      "Epoch 305 | Batch 0 | Train Loss: 0.3778685 | Validation Loss: 0.5175553\n",
      "Epoch 306 | Batch 0 | Train Loss: 0.37773314 | Validation Loss: 0.5184314\n",
      "Epoch 307 | Batch 0 | Train Loss: 0.37773913 | Validation Loss: 0.5183198\n",
      "Epoch 308 | Batch 0 | Train Loss: 0.37765083 | Validation Loss: 0.52033746\n",
      "Epoch 309 | Batch 0 | Train Loss: 0.37735063 | Validation Loss: 0.51841986\n",
      "Epoch 310 | Batch 0 | Train Loss: 0.37752107 | Validation Loss: 0.51888406\n",
      "Epoch 311 | Batch 0 | Train Loss: 0.3778762 | Validation Loss: 0.516317\n",
      "Epoch 312 | Batch 0 | Train Loss: 0.37738335 | Validation Loss: 0.51811504\n",
      "Epoch 313 | Batch 0 | Train Loss: 0.3773819 | Validation Loss: 0.51819575\n",
      "Epoch 314 | Batch 0 | Train Loss: 0.37766194 | Validation Loss: 0.52054465\n",
      "Epoch 315 | Batch 0 | Train Loss: 0.3777817 | Validation Loss: 0.51680493\n",
      "Epoch 316 | Batch 0 | Train Loss: 0.3775522 | Validation Loss: 0.52060896\n",
      "Epoch 317 | Batch 0 | Train Loss: 0.37702033 | Validation Loss: 0.51771814\n",
      "Epoch 318 | Batch 0 | Train Loss: 0.3769928 | Validation Loss: 0.51771444\n",
      "Epoch 319 | Batch 0 | Train Loss: 0.37764356 | Validation Loss: 0.5174899\n",
      "Epoch 320 | Batch 0 | Train Loss: 0.37713775 | Validation Loss: 0.5196335\n",
      "Epoch 321 | Batch 0 | Train Loss: 0.37746474 | Validation Loss: 0.5224882\n",
      "Epoch 322 | Batch 0 | Train Loss: 0.37721604 | Validation Loss: 0.5178124\n",
      "Epoch 323 | Batch 0 | Train Loss: 0.3771309 | Validation Loss: 0.52271974\n",
      "Epoch 324 | Batch 0 | Train Loss: 0.37703818 | Validation Loss: 0.517617\n",
      "Epoch 325 | Batch 0 | Train Loss: 0.3763808 | Validation Loss: 0.5184877\n",
      "Epoch 326 | Batch 0 | Train Loss: 0.37730315 | Validation Loss: 0.5213625\n",
      "Epoch 327 | Batch 0 | Train Loss: 0.37591267 | Validation Loss: 0.520512\n",
      "Epoch 328 | Batch 0 | Train Loss: 0.37592033 | Validation Loss: 0.5183041\n",
      "Epoch 329 | Batch 0 | Train Loss: 0.37580845 | Validation Loss: 0.52072024\n",
      "Epoch 330 | Batch 0 | Train Loss: 0.37601072 | Validation Loss: 0.5195677\n",
      "Epoch 331 | Batch 0 | Train Loss: 0.37619713 | Validation Loss: 0.5176748\n",
      "Epoch 332 | Batch 0 | Train Loss: 0.3764563 | Validation Loss: 0.5198071\n",
      "Epoch 333 | Batch 0 | Train Loss: 0.37632033 | Validation Loss: 0.52004415\n",
      "Epoch 334 | Batch 0 | Train Loss: 0.37595725 | Validation Loss: 0.51917523\n",
      "Epoch 335 | Batch 0 | Train Loss: 0.37565622 | Validation Loss: 0.51935697\n",
      "Epoch 336 | Batch 0 | Train Loss: 0.3761128 | Validation Loss: 0.5169933\n",
      "Epoch 337 | Batch 0 | Train Loss: 0.37676132 | Validation Loss: 0.5148222\n",
      "Epoch 338 | Batch 0 | Train Loss: 0.37608546 | Validation Loss: 0.51731783\n",
      "Epoch 339 | Batch 0 | Train Loss: 0.37603417 | Validation Loss: 0.5204629\n",
      "Epoch 340 | Batch 0 | Train Loss: 0.37574902 | Validation Loss: 0.51922923\n",
      "Epoch 341 | Batch 0 | Train Loss: 0.37544146 | Validation Loss: 0.519822\n",
      "Epoch 342 | Batch 0 | Train Loss: 0.37622124 | Validation Loss: 0.5221157\n",
      "Epoch 343 | Batch 0 | Train Loss: 0.37831742 | Validation Loss: 0.52579665\n",
      "Epoch 344 | Batch 0 | Train Loss: 0.37572822 | Validation Loss: 0.5175621\n",
      "Epoch 345 | Batch 0 | Train Loss: 0.37566707 | Validation Loss: 0.516626\n",
      "Epoch 346 | Batch 0 | Train Loss: 0.37549838 | Validation Loss: 0.521212\n",
      "Epoch 347 | Batch 0 | Train Loss: 0.3753009 | Validation Loss: 0.5200157\n",
      "Epoch 348 | Batch 0 | Train Loss: 0.37527722 | Validation Loss: 0.520774\n",
      "Epoch 349 | Batch 0 | Train Loss: 0.37579635 | Validation Loss: 0.52029467\n",
      "Epoch 350 | Batch 0 | Train Loss: 0.375083 | Validation Loss: 0.52215\n",
      "Epoch 351 | Batch 0 | Train Loss: 0.37485045 | Validation Loss: 0.52015376\n",
      "Epoch 352 | Batch 0 | Train Loss: 0.37585753 | Validation Loss: 0.52336997\n",
      "Epoch 353 | Batch 0 | Train Loss: 0.37487748 | Validation Loss: 0.51998305\n",
      "Epoch 354 | Batch 0 | Train Loss: 0.37494004 | Validation Loss: 0.51776993\n",
      "Epoch 355 | Batch 0 | Train Loss: 0.37496758 | Validation Loss: 0.5209752\n",
      "Epoch 356 | Batch 0 | Train Loss: 0.37498626 | Validation Loss: 0.51857066\n",
      "Epoch 357 | Batch 0 | Train Loss: 0.37479317 | Validation Loss: 0.5180857\n",
      "Epoch 358 | Batch 0 | Train Loss: 0.37505776 | Validation Loss: 0.52235705\n",
      "Epoch 359 | Batch 0 | Train Loss: 0.37476686 | Validation Loss: 0.5173756\n",
      "Epoch 360 | Batch 0 | Train Loss: 0.37457597 | Validation Loss: 0.51762533\n",
      "Epoch 361 | Batch 0 | Train Loss: 0.3741638 | Validation Loss: 0.51865983\n",
      "Epoch 362 | Batch 0 | Train Loss: 0.3746903 | Validation Loss: 0.51688635\n",
      "Epoch 363 | Batch 0 | Train Loss: 0.37428096 | Validation Loss: 0.51805526\n",
      "Epoch 364 | Batch 0 | Train Loss: 0.37409753 | Validation Loss: 0.5217663\n",
      "Epoch 365 | Batch 0 | Train Loss: 0.37430435 | Validation Loss: 0.51957613\n",
      "Epoch 366 | Batch 0 | Train Loss: 0.37395272 | Validation Loss: 0.5200222\n",
      "Epoch 367 | Batch 0 | Train Loss: 0.37377506 | Validation Loss: 0.5204905\n",
      "Epoch 368 | Batch 0 | Train Loss: 0.37359604 | Validation Loss: 0.5195968\n",
      "Epoch 369 | Batch 0 | Train Loss: 0.37360406 | Validation Loss: 0.5181583\n",
      "Epoch 370 | Batch 0 | Train Loss: 0.3744928 | Validation Loss: 0.51657456\n",
      "Epoch 371 | Batch 0 | Train Loss: 0.37362835 | Validation Loss: 0.5183131\n",
      "Epoch 372 | Batch 0 | Train Loss: 0.37375242 | Validation Loss: 0.52047235\n",
      "Epoch 373 | Batch 0 | Train Loss: 0.37362644 | Validation Loss: 0.52006\n",
      "Epoch 374 | Batch 0 | Train Loss: 0.37395638 | Validation Loss: 0.51984596\n",
      "Epoch 375 | Batch 0 | Train Loss: 0.3753382 | Validation Loss: 0.520634\n",
      "Epoch 376 | Batch 0 | Train Loss: 0.37374648 | Validation Loss: 0.5219136\n",
      "Epoch 377 | Batch 0 | Train Loss: 0.3743788 | Validation Loss: 0.5205026\n",
      "Epoch 378 | Batch 0 | Train Loss: 0.37387475 | Validation Loss: 0.5189369\n",
      "Epoch 379 | Batch 0 | Train Loss: 0.3736298 | Validation Loss: 0.52001405\n",
      "Epoch 380 | Batch 0 | Train Loss: 0.3736944 | Validation Loss: 0.5178\n",
      "Epoch 381 | Batch 0 | Train Loss: 0.37416646 | Validation Loss: 0.5219592\n",
      "Epoch 382 | Batch 0 | Train Loss: 0.37343895 | Validation Loss: 0.52170354\n",
      "Epoch 383 | Batch 0 | Train Loss: 0.373191 | Validation Loss: 0.5201762\n",
      "Epoch 384 | Batch 0 | Train Loss: 0.37338257 | Validation Loss: 0.5184113\n",
      "Epoch 385 | Batch 0 | Train Loss: 0.37297314 | Validation Loss: 0.5208688\n",
      "Epoch 386 | Batch 0 | Train Loss: 0.3739087 | Validation Loss: 0.52567714\n",
      "Epoch 387 | Batch 0 | Train Loss: 0.37285286 | Validation Loss: 0.5215228\n",
      "Epoch 388 | Batch 0 | Train Loss: 0.37336394 | Validation Loss: 0.51917195\n",
      "Epoch 389 | Batch 0 | Train Loss: 0.37289265 | Validation Loss: 0.51900023\n",
      "Epoch 390 | Batch 0 | Train Loss: 0.3733098 | Validation Loss: 0.51864266\n",
      "Epoch 391 | Batch 0 | Train Loss: 0.37391767 | Validation Loss: 0.5184471\n",
      "Epoch 392 | Batch 0 | Train Loss: 0.3735369 | Validation Loss: 0.51814526\n",
      "Epoch 393 | Batch 0 | Train Loss: 0.3743785 | Validation Loss: 0.520426\n",
      "Epoch 394 | Batch 0 | Train Loss: 0.37333947 | Validation Loss: 0.5181002\n",
      "Epoch 395 | Batch 0 | Train Loss: 0.37288103 | Validation Loss: 0.5188675\n",
      "Epoch 396 | Batch 0 | Train Loss: 0.3730398 | Validation Loss: 0.5192327\n",
      "Epoch 397 | Batch 0 | Train Loss: 0.37241042 | Validation Loss: 0.52032316\n",
      "Epoch 398 | Batch 0 | Train Loss: 0.37257165 | Validation Loss: 0.5191584\n",
      "Epoch 399 | Batch 0 | Train Loss: 0.37223992 | Validation Loss: 0.5208809\n",
      "Epoch 400 | Batch 0 | Train Loss: 0.37299496 | Validation Loss: 0.5211028\n",
      "Epoch 401 | Batch 0 | Train Loss: 0.3723837 | Validation Loss: 0.51963073\n",
      "Epoch 402 | Batch 0 | Train Loss: 0.37213635 | Validation Loss: 0.520998\n",
      "Epoch 403 | Batch 0 | Train Loss: 0.37250534 | Validation Loss: 0.517903\n",
      "Epoch 404 | Batch 0 | Train Loss: 0.37282676 | Validation Loss: 0.51889014\n",
      "Epoch 405 | Batch 0 | Train Loss: 0.37260276 | Validation Loss: 0.51732737\n",
      "Epoch 406 | Batch 0 | Train Loss: 0.37221742 | Validation Loss: 0.52129424\n",
      "Epoch 407 | Batch 0 | Train Loss: 0.37303284 | Validation Loss: 0.5242314\n",
      "Epoch 408 | Batch 0 | Train Loss: 0.37194481 | Validation Loss: 0.51955235\n",
      "Epoch 409 | Batch 0 | Train Loss: 0.37184265 | Validation Loss: 0.52056104\n",
      "Epoch 410 | Batch 0 | Train Loss: 0.3716643 | Validation Loss: 0.5210242\n",
      "Epoch 411 | Batch 0 | Train Loss: 0.37144426 | Validation Loss: 0.523332\n",
      "Epoch 412 | Batch 0 | Train Loss: 0.372079 | Validation Loss: 0.5248149\n",
      "Epoch 413 | Batch 0 | Train Loss: 0.37140232 | Validation Loss: 0.52150095\n",
      "Epoch 414 | Batch 0 | Train Loss: 0.37147716 | Validation Loss: 0.52329147\n",
      "Epoch 415 | Batch 0 | Train Loss: 0.37171006 | Validation Loss: 0.52523863\n",
      "Epoch 416 | Batch 0 | Train Loss: 0.3712283 | Validation Loss: 0.5225403\n",
      "Epoch 417 | Batch 0 | Train Loss: 0.3717586 | Validation Loss: 0.52091646\n",
      "Epoch 418 | Batch 0 | Train Loss: 0.37235585 | Validation Loss: 0.52803946\n",
      "Epoch 419 | Batch 0 | Train Loss: 0.37144268 | Validation Loss: 0.5231539\n",
      "Epoch 420 | Batch 0 | Train Loss: 0.3715444 | Validation Loss: 0.52331793\n",
      "Epoch 421 | Batch 0 | Train Loss: 0.37093475 | Validation Loss: 0.52500653\n",
      "Epoch 422 | Batch 0 | Train Loss: 0.37104005 | Validation Loss: 0.5234614\n",
      "Epoch 423 | Batch 0 | Train Loss: 0.37100837 | Validation Loss: 0.5239054\n",
      "Epoch 424 | Batch 0 | Train Loss: 0.37083283 | Validation Loss: 0.5250067\n",
      "Epoch 425 | Batch 0 | Train Loss: 0.37097168 | Validation Loss: 0.52235615\n",
      "Epoch 426 | Batch 0 | Train Loss: 0.37100303 | Validation Loss: 0.5211583\n",
      "Epoch 427 | Batch 0 | Train Loss: 0.37064683 | Validation Loss: 0.524632\n",
      "Epoch 428 | Batch 0 | Train Loss: 0.3707405 | Validation Loss: 0.5245057\n",
      "Epoch 429 | Batch 0 | Train Loss: 0.37091088 | Validation Loss: 0.52465385\n",
      "Epoch 430 | Batch 0 | Train Loss: 0.37052238 | Validation Loss: 0.5244622\n",
      "Epoch 431 | Batch 0 | Train Loss: 0.37071335 | Validation Loss: 0.52376074\n",
      "Epoch 432 | Batch 0 | Train Loss: 0.37122273 | Validation Loss: 0.52741987\n",
      "Epoch 433 | Batch 0 | Train Loss: 0.3705718 | Validation Loss: 0.524073\n",
      "Epoch 434 | Batch 0 | Train Loss: 0.37054798 | Validation Loss: 0.52345186\n",
      "Epoch 435 | Batch 0 | Train Loss: 0.37108567 | Validation Loss: 0.5195417\n",
      "Epoch 436 | Batch 0 | Train Loss: 0.37088233 | Validation Loss: 0.5212268\n",
      "Epoch 437 | Batch 0 | Train Loss: 0.3703307 | Validation Loss: 0.5220244\n",
      "Epoch 438 | Batch 0 | Train Loss: 0.37041187 | Validation Loss: 0.5239228\n",
      "Epoch 439 | Batch 0 | Train Loss: 0.370625 | Validation Loss: 0.5243694\n",
      "Epoch 440 | Batch 0 | Train Loss: 0.37054107 | Validation Loss: 0.5227831\n",
      "Epoch 441 | Batch 0 | Train Loss: 0.37013152 | Validation Loss: 0.5249822\n",
      "Epoch 442 | Batch 0 | Train Loss: 0.3705948 | Validation Loss: 0.52567536\n",
      "Epoch 443 | Batch 0 | Train Loss: 0.37198135 | Validation Loss: 0.52035654\n",
      "Epoch 444 | Batch 0 | Train Loss: 0.37094882 | Validation Loss: 0.52185804\n",
      "Epoch 445 | Batch 0 | Train Loss: 0.37084153 | Validation Loss: 0.52197164\n",
      "Epoch 446 | Batch 0 | Train Loss: 0.37100366 | Validation Loss: 0.52112186\n",
      "Epoch 447 | Batch 0 | Train Loss: 0.37079024 | Validation Loss: 0.52155966\n",
      "Epoch 448 | Batch 0 | Train Loss: 0.37032092 | Validation Loss: 0.52320963\n",
      "Epoch 449 | Batch 0 | Train Loss: 0.3699758 | Validation Loss: 0.5246167\n",
      "Epoch 450 | Batch 0 | Train Loss: 0.37005493 | Validation Loss: 0.52378297\n",
      "Epoch 451 | Batch 0 | Train Loss: 0.37015778 | Validation Loss: 0.52450615\n",
      "Epoch 452 | Batch 0 | Train Loss: 0.37091172 | Validation Loss: 0.52287054\n",
      "Epoch 453 | Batch 0 | Train Loss: 0.37014404 | Validation Loss: 0.5251972\n",
      "Epoch 454 | Batch 0 | Train Loss: 0.370065 | Validation Loss: 0.5228154\n",
      "Epoch 455 | Batch 0 | Train Loss: 0.36983454 | Validation Loss: 0.52313334\n",
      "Epoch 456 | Batch 0 | Train Loss: 0.36970755 | Validation Loss: 0.5237362\n",
      "Epoch 457 | Batch 0 | Train Loss: 0.36948732 | Validation Loss: 0.5259106\n",
      "Epoch 458 | Batch 0 | Train Loss: 0.3698127 | Validation Loss: 0.52329737\n",
      "Epoch 459 | Batch 0 | Train Loss: 0.36935014 | Validation Loss: 0.5253614\n",
      "Epoch 460 | Batch 0 | Train Loss: 0.37009126 | Validation Loss: 0.52597505\n",
      "Epoch 461 | Batch 0 | Train Loss: 0.36947677 | Validation Loss: 0.52758133\n",
      "Epoch 462 | Batch 0 | Train Loss: 0.36971337 | Validation Loss: 0.52777267\n",
      "Epoch 463 | Batch 0 | Train Loss: 0.3693405 | Validation Loss: 0.525696\n",
      "Epoch 464 | Batch 0 | Train Loss: 0.36924076 | Validation Loss: 0.5279177\n",
      "Epoch 465 | Batch 0 | Train Loss: 0.36922485 | Validation Loss: 0.5261412\n",
      "Epoch 466 | Batch 0 | Train Loss: 0.36944678 | Validation Loss: 0.5255761\n",
      "Epoch 467 | Batch 0 | Train Loss: 0.36964476 | Validation Loss: 0.5239257\n",
      "Epoch 468 | Batch 0 | Train Loss: 0.3690906 | Validation Loss: 0.5278165\n",
      "Epoch 469 | Batch 0 | Train Loss: 0.370465 | Validation Loss: 0.53093535\n",
      "Epoch 470 | Batch 0 | Train Loss: 0.3694615 | Validation Loss: 0.5254862\n",
      "Epoch 471 | Batch 0 | Train Loss: 0.36884746 | Validation Loss: 0.5287407\n",
      "Epoch 472 | Batch 0 | Train Loss: 0.36911014 | Validation Loss: 0.52562237\n",
      "Epoch 473 | Batch 0 | Train Loss: 0.36944038 | Validation Loss: 0.5278046\n",
      "Epoch 474 | Batch 0 | Train Loss: 0.36884272 | Validation Loss: 0.5286216\n",
      "Epoch 475 | Batch 0 | Train Loss: 0.36918336 | Validation Loss: 0.5269265\n",
      "Epoch 476 | Batch 0 | Train Loss: 0.36956674 | Validation Loss: 0.5321035\n",
      "Epoch 477 | Batch 0 | Train Loss: 0.36905852 | Validation Loss: 0.52816546\n",
      "Epoch 478 | Batch 0 | Train Loss: 0.36954093 | Validation Loss: 0.52566934\n",
      "Epoch 479 | Batch 0 | Train Loss: 0.36905724 | Validation Loss: 0.52493536\n",
      "Epoch 480 | Batch 0 | Train Loss: 0.3695393 | Validation Loss: 0.5248843\n",
      "Epoch 481 | Batch 0 | Train Loss: 0.36861047 | Validation Loss: 0.52545065\n",
      "Epoch 482 | Batch 0 | Train Loss: 0.3690044 | Validation Loss: 0.5306928\n",
      "Epoch 483 | Batch 0 | Train Loss: 0.36878625 | Validation Loss: 0.52498186\n",
      "Epoch 484 | Batch 0 | Train Loss: 0.36848372 | Validation Loss: 0.5277811\n",
      "Epoch 485 | Batch 0 | Train Loss: 0.36863396 | Validation Loss: 0.52552104\n",
      "Epoch 486 | Batch 0 | Train Loss: 0.36825967 | Validation Loss: 0.5278786\n",
      "Epoch 487 | Batch 0 | Train Loss: 0.36850974 | Validation Loss: 0.5258491\n",
      "Epoch 488 | Batch 0 | Train Loss: 0.36835662 | Validation Loss: 0.5296845\n",
      "Epoch 489 | Batch 0 | Train Loss: 0.36841193 | Validation Loss: 0.52776027\n",
      "Epoch 490 | Batch 0 | Train Loss: 0.36853546 | Validation Loss: 0.52612287\n",
      "Epoch 491 | Batch 0 | Train Loss: 0.3683091 | Validation Loss: 0.5296417\n",
      "Epoch 492 | Batch 0 | Train Loss: 0.3682316 | Validation Loss: 0.5287207\n",
      "Epoch 493 | Batch 0 | Train Loss: 0.36878854 | Validation Loss: 0.5261\n",
      "Epoch 494 | Batch 0 | Train Loss: 0.3685198 | Validation Loss: 0.5259316\n",
      "Epoch 495 | Batch 0 | Train Loss: 0.3682015 | Validation Loss: 0.5289379\n",
      "Epoch 496 | Batch 0 | Train Loss: 0.36844006 | Validation Loss: 0.52814925\n",
      "Epoch 497 | Batch 0 | Train Loss: 0.3683344 | Validation Loss: 0.52864474\n",
      "Epoch 498 | Batch 0 | Train Loss: 0.36896142 | Validation Loss: 0.5260064\n",
      "Epoch 499 | Batch 0 | Train Loss: 0.3679013 | Validation Loss: 0.5282157\n",
      "Epoch 500 | Batch 0 | Train Loss: 0.3678146 | Validation Loss: 0.52749884\n",
      "Epoch 501 | Batch 0 | Train Loss: 0.36814284 | Validation Loss: 0.5242318\n",
      "Epoch 502 | Batch 0 | Train Loss: 0.36770904 | Validation Loss: 0.52691054\n",
      "Epoch 503 | Batch 0 | Train Loss: 0.36796057 | Validation Loss: 0.52782315\n",
      "Epoch 504 | Batch 0 | Train Loss: 0.36793366 | Validation Loss: 0.5260765\n",
      "Epoch 505 | Batch 0 | Train Loss: 0.36808312 | Validation Loss: 0.52521455\n",
      "Epoch 506 | Batch 0 | Train Loss: 0.36757323 | Validation Loss: 0.5277079\n",
      "Epoch 507 | Batch 0 | Train Loss: 0.3673875 | Validation Loss: 0.52856493\n",
      "Epoch 508 | Batch 0 | Train Loss: 0.3677597 | Validation Loss: 0.5294018\n",
      "Epoch 509 | Batch 0 | Train Loss: 0.36731347 | Validation Loss: 0.5291336\n",
      "Epoch 510 | Batch 0 | Train Loss: 0.3673381 | Validation Loss: 0.5309037\n",
      "Epoch 511 | Batch 0 | Train Loss: 0.36752194 | Validation Loss: 0.5305877\n",
      "Epoch 512 | Batch 0 | Train Loss: 0.36846238 | Validation Loss: 0.5309617\n",
      "Epoch 513 | Batch 0 | Train Loss: 0.36765954 | Validation Loss: 0.5311166\n",
      "Epoch 514 | Batch 0 | Train Loss: 0.36789864 | Validation Loss: 0.5324081\n",
      "Epoch 515 | Batch 0 | Train Loss: 0.3675808 | Validation Loss: 0.5303802\n",
      "Epoch 516 | Batch 0 | Train Loss: 0.36764184 | Validation Loss: 0.52652204\n",
      "Epoch 517 | Batch 0 | Train Loss: 0.36784217 | Validation Loss: 0.5248014\n",
      "Epoch 518 | Batch 0 | Train Loss: 0.36781883 | Validation Loss: 0.52565783\n",
      "Epoch 519 | Batch 0 | Train Loss: 0.36750218 | Validation Loss: 0.5272557\n",
      "Epoch 520 | Batch 0 | Train Loss: 0.36770958 | Validation Loss: 0.5284121\n",
      "Epoch 521 | Batch 0 | Train Loss: 0.3669374 | Validation Loss: 0.5311212\n",
      "Epoch 522 | Batch 0 | Train Loss: 0.36712396 | Validation Loss: 0.5278359\n",
      "Epoch 523 | Batch 0 | Train Loss: 0.36706308 | Validation Loss: 0.52888715\n",
      "Epoch 524 | Batch 0 | Train Loss: 0.36689013 | Validation Loss: 0.52980417\n",
      "Epoch 525 | Batch 0 | Train Loss: 0.36684236 | Validation Loss: 0.5291674\n",
      "Epoch 526 | Batch 0 | Train Loss: 0.366783 | Validation Loss: 0.5306566\n",
      "Epoch 527 | Batch 0 | Train Loss: 0.3671021 | Validation Loss: 0.5301438\n",
      "Epoch 528 | Batch 0 | Train Loss: 0.36687425 | Validation Loss: 0.53024673\n",
      "Epoch 529 | Batch 0 | Train Loss: 0.36686116 | Validation Loss: 0.5285812\n",
      "Epoch 530 | Batch 0 | Train Loss: 0.36661446 | Validation Loss: 0.52901244\n",
      "Epoch 531 | Batch 0 | Train Loss: 0.36694416 | Validation Loss: 0.53021395\n",
      "Epoch 532 | Batch 0 | Train Loss: 0.36726516 | Validation Loss: 0.5249684\n",
      "Epoch 533 | Batch 0 | Train Loss: 0.3668649 | Validation Loss: 0.52619934\n",
      "Epoch 534 | Batch 0 | Train Loss: 0.36722633 | Validation Loss: 0.5258038\n",
      "Epoch 535 | Batch 0 | Train Loss: 0.36646682 | Validation Loss: 0.5289734\n",
      "Epoch 536 | Batch 0 | Train Loss: 0.36663437 | Validation Loss: 0.527793\n",
      "Epoch 537 | Batch 0 | Train Loss: 0.36640266 | Validation Loss: 0.53111285\n",
      "Epoch 538 | Batch 0 | Train Loss: 0.36671478 | Validation Loss: 0.52991503\n",
      "Epoch 539 | Batch 0 | Train Loss: 0.36653557 | Validation Loss: 0.5321598\n",
      "Epoch 540 | Batch 0 | Train Loss: 0.36632243 | Validation Loss: 0.53015304\n",
      "Epoch 541 | Batch 0 | Train Loss: 0.36651978 | Validation Loss: 0.5283584\n",
      "Epoch 542 | Batch 0 | Train Loss: 0.36641708 | Validation Loss: 0.5296039\n",
      "Epoch 543 | Batch 0 | Train Loss: 0.36629298 | Validation Loss: 0.5295367\n",
      "Epoch 544 | Batch 0 | Train Loss: 0.36625627 | Validation Loss: 0.53006077\n",
      "Epoch 545 | Batch 0 | Train Loss: 0.36640823 | Validation Loss: 0.5287768\n",
      "Epoch 546 | Batch 0 | Train Loss: 0.36605075 | Validation Loss: 0.5315476\n",
      "Epoch 547 | Batch 0 | Train Loss: 0.36618882 | Validation Loss: 0.52949524\n",
      "Epoch 548 | Batch 0 | Train Loss: 0.36635512 | Validation Loss: 0.53107893\n",
      "Epoch 549 | Batch 0 | Train Loss: 0.3665176 | Validation Loss: 0.529855\n",
      "Epoch 550 | Batch 0 | Train Loss: 0.36647552 | Validation Loss: 0.53171766\n",
      "Epoch 551 | Batch 0 | Train Loss: 0.3662901 | Validation Loss: 0.5298334\n",
      "Epoch 552 | Batch 0 | Train Loss: 0.36738187 | Validation Loss: 0.5347688\n",
      "Epoch 553 | Batch 0 | Train Loss: 0.36576995 | Validation Loss: 0.53304577\n",
      "Epoch 554 | Batch 0 | Train Loss: 0.36583778 | Validation Loss: 0.53262174\n",
      "Epoch 555 | Batch 0 | Train Loss: 0.36651716 | Validation Loss: 0.5301344\n",
      "Epoch 556 | Batch 0 | Train Loss: 0.36654013 | Validation Loss: 0.52748007\n",
      "Epoch 557 | Batch 0 | Train Loss: 0.36634135 | Validation Loss: 0.5277073\n",
      "Epoch 558 | Batch 0 | Train Loss: 0.36650932 | Validation Loss: 0.525583\n",
      "Epoch 559 | Batch 0 | Train Loss: 0.36606273 | Validation Loss: 0.52796465\n",
      "Epoch 560 | Batch 0 | Train Loss: 0.36598387 | Validation Loss: 0.5295727\n",
      "Epoch 561 | Batch 0 | Train Loss: 0.36575451 | Validation Loss: 0.52890533\n",
      "Epoch 562 | Batch 0 | Train Loss: 0.3658435 | Validation Loss: 0.52939683\n",
      "Epoch 563 | Batch 0 | Train Loss: 0.36558 | Validation Loss: 0.5320321\n",
      "Epoch 564 | Batch 0 | Train Loss: 0.36815536 | Validation Loss: 0.5396073\n",
      "Epoch 565 | Batch 0 | Train Loss: 0.36542237 | Validation Loss: 0.53155875\n",
      "Epoch 566 | Batch 0 | Train Loss: 0.3658525 | Validation Loss: 0.52813673\n",
      "Epoch 567 | Batch 0 | Train Loss: 0.36591288 | Validation Loss: 0.529658\n",
      "Epoch 568 | Batch 0 | Train Loss: 0.36533305 | Validation Loss: 0.5307478\n",
      "Epoch 569 | Batch 0 | Train Loss: 0.3661206 | Validation Loss: 0.5288269\n",
      "Epoch 570 | Batch 0 | Train Loss: 0.3656677 | Validation Loss: 0.5297533\n",
      "Epoch 571 | Batch 0 | Train Loss: 0.3653462 | Validation Loss: 0.52901745\n",
      "Epoch 572 | Batch 0 | Train Loss: 0.36563578 | Validation Loss: 0.5299701\n",
      "Epoch 573 | Batch 0 | Train Loss: 0.36572066 | Validation Loss: 0.5287432\n",
      "Epoch 574 | Batch 0 | Train Loss: 0.3652816 | Validation Loss: 0.5318889\n",
      "Epoch 575 | Batch 0 | Train Loss: 0.3652594 | Validation Loss: 0.52996516\n",
      "Epoch 576 | Batch 0 | Train Loss: 0.36514175 | Validation Loss: 0.53192085\n",
      "Epoch 577 | Batch 0 | Train Loss: 0.36552057 | Validation Loss: 0.5306032\n",
      "Epoch 578 | Batch 0 | Train Loss: 0.3652476 | Validation Loss: 0.53169334\n",
      "Epoch 579 | Batch 0 | Train Loss: 0.36534896 | Validation Loss: 0.5322718\n",
      "Epoch 580 | Batch 0 | Train Loss: 0.36493734 | Validation Loss: 0.5336897\n",
      "Epoch 581 | Batch 0 | Train Loss: 0.36529443 | Validation Loss: 0.53151965\n",
      "Epoch 582 | Batch 0 | Train Loss: 0.36538923 | Validation Loss: 0.5315535\n",
      "Epoch 583 | Batch 0 | Train Loss: 0.36528012 | Validation Loss: 0.5335024\n",
      "Epoch 584 | Batch 0 | Train Loss: 0.3659301 | Validation Loss: 0.5329033\n",
      "Epoch 585 | Batch 0 | Train Loss: 0.36501464 | Validation Loss: 0.53526\n",
      "Epoch 586 | Batch 0 | Train Loss: 0.3649616 | Validation Loss: 0.53204453\n",
      "Epoch 587 | Batch 0 | Train Loss: 0.3646744 | Validation Loss: 0.535335\n",
      "Epoch 588 | Batch 0 | Train Loss: 0.36514297 | Validation Loss: 0.5346093\n",
      "Epoch 589 | Batch 0 | Train Loss: 0.3655535 | Validation Loss: 0.5306667\n",
      "Epoch 590 | Batch 0 | Train Loss: 0.3650063 | Validation Loss: 0.5331253\n",
      "Epoch 591 | Batch 0 | Train Loss: 0.36523047 | Validation Loss: 0.5313909\n",
      "Epoch 592 | Batch 0 | Train Loss: 0.36579034 | Validation Loss: 0.5358071\n",
      "Epoch 593 | Batch 0 | Train Loss: 0.36480272 | Validation Loss: 0.53199655\n",
      "Epoch 594 | Batch 0 | Train Loss: 0.3648436 | Validation Loss: 0.53313154\n",
      "Epoch 595 | Batch 0 | Train Loss: 0.36520594 | Validation Loss: 0.5339855\n",
      "Epoch 596 | Batch 0 | Train Loss: 0.36557782 | Validation Loss: 0.5275792\n",
      "Epoch 597 | Batch 0 | Train Loss: 0.36492065 | Validation Loss: 0.5314289\n",
      "Epoch 598 | Batch 0 | Train Loss: 0.36476463 | Validation Loss: 0.53214365\n",
      "Epoch 599 | Batch 0 | Train Loss: 0.36489096 | Validation Loss: 0.53198797\n",
      "Epoch 600 | Batch 0 | Train Loss: 0.36530975 | Validation Loss: 0.5330495\n",
      "Epoch 601 | Batch 0 | Train Loss: 0.36484635 | Validation Loss: 0.5332005\n",
      "Epoch 602 | Batch 0 | Train Loss: 0.36520737 | Validation Loss: 0.53206116\n",
      "Epoch 603 | Batch 0 | Train Loss: 0.36504227 | Validation Loss: 0.5343272\n",
      "Epoch 604 | Batch 0 | Train Loss: 0.36481476 | Validation Loss: 0.53616613\n",
      "Epoch 605 | Batch 0 | Train Loss: 0.36440995 | Validation Loss: 0.5356856\n",
      "Epoch 606 | Batch 0 | Train Loss: 0.36459115 | Validation Loss: 0.53283894\n",
      "Epoch 607 | Batch 0 | Train Loss: 0.36463383 | Validation Loss: 0.5342522\n",
      "Epoch 608 | Batch 0 | Train Loss: 0.36429897 | Validation Loss: 0.53639656\n",
      "Epoch 609 | Batch 0 | Train Loss: 0.36470154 | Validation Loss: 0.53536\n",
      "Epoch 610 | Batch 0 | Train Loss: 0.3640254 | Validation Loss: 0.5368208\n",
      "Epoch 611 | Batch 0 | Train Loss: 0.3646808 | Validation Loss: 0.534033\n",
      "Epoch 612 | Batch 0 | Train Loss: 0.36464906 | Validation Loss: 0.535059\n",
      "Epoch 613 | Batch 0 | Train Loss: 0.3638327 | Validation Loss: 0.53706664\n",
      "Epoch 614 | Batch 0 | Train Loss: 0.36433 | Validation Loss: 0.5321615\n",
      "Epoch 615 | Batch 0 | Train Loss: 0.3642688 | Validation Loss: 0.5327291\n",
      "Epoch 616 | Batch 0 | Train Loss: 0.36405683 | Validation Loss: 0.53357935\n",
      "Epoch 617 | Batch 0 | Train Loss: 0.3642658 | Validation Loss: 0.53405195\n",
      "Epoch 618 | Batch 0 | Train Loss: 0.3642803 | Validation Loss: 0.53388894\n",
      "Epoch 619 | Batch 0 | Train Loss: 0.3638913 | Validation Loss: 0.5320283\n",
      "Epoch 620 | Batch 0 | Train Loss: 0.3643069 | Validation Loss: 0.53483146\n",
      "Epoch 621 | Batch 0 | Train Loss: 0.36400008 | Validation Loss: 0.5339193\n",
      "Epoch 622 | Batch 0 | Train Loss: 0.3643958 | Validation Loss: 0.5363667\n",
      "Epoch 623 | Batch 0 | Train Loss: 0.3638292 | Validation Loss: 0.5334653\n",
      "Epoch 624 | Batch 0 | Train Loss: 0.36370888 | Validation Loss: 0.5332243\n",
      "Epoch 625 | Batch 0 | Train Loss: 0.36471167 | Validation Loss: 0.5345218\n",
      "Epoch 626 | Batch 0 | Train Loss: 0.36398515 | Validation Loss: 0.53250533\n",
      "Epoch 627 | Batch 0 | Train Loss: 0.36417344 | Validation Loss: 0.536274\n",
      "Epoch 628 | Batch 0 | Train Loss: 0.3636566 | Validation Loss: 0.5359999\n",
      "Epoch 629 | Batch 0 | Train Loss: 0.36407188 | Validation Loss: 0.5362297\n",
      "Epoch 630 | Batch 0 | Train Loss: 0.3638599 | Validation Loss: 0.53665525\n",
      "Epoch 631 | Batch 0 | Train Loss: 0.36371407 | Validation Loss: 0.5357994\n",
      "Epoch 632 | Batch 0 | Train Loss: 0.3632828 | Validation Loss: 0.53586686\n",
      "Epoch 633 | Batch 0 | Train Loss: 0.36349458 | Validation Loss: 0.5364549\n",
      "Epoch 634 | Batch 0 | Train Loss: 0.363424 | Validation Loss: 0.5365185\n",
      "Epoch 635 | Batch 0 | Train Loss: 0.3631206 | Validation Loss: 0.53784126\n",
      "Epoch 636 | Batch 0 | Train Loss: 0.3631723 | Validation Loss: 0.53748286\n",
      "Epoch 637 | Batch 0 | Train Loss: 0.362987 | Validation Loss: 0.53547865\n",
      "Epoch 638 | Batch 0 | Train Loss: 0.36297905 | Validation Loss: 0.5362296\n",
      "Epoch 639 | Batch 0 | Train Loss: 0.3630072 | Validation Loss: 0.53573865\n",
      "Epoch 640 | Batch 0 | Train Loss: 0.36301628 | Validation Loss: 0.5366593\n",
      "Epoch 641 | Batch 0 | Train Loss: 0.36326516 | Validation Loss: 0.536007\n",
      "Epoch 642 | Batch 0 | Train Loss: 0.36364287 | Validation Loss: 0.53531957\n",
      "Epoch 643 | Batch 0 | Train Loss: 0.3629468 | Validation Loss: 0.5359107\n",
      "Epoch 644 | Batch 0 | Train Loss: 0.36365426 | Validation Loss: 0.5385406\n",
      "Epoch 645 | Batch 0 | Train Loss: 0.3628695 | Validation Loss: 0.53532684\n",
      "Epoch 646 | Batch 0 | Train Loss: 0.36289042 | Validation Loss: 0.53511\n",
      "Epoch 647 | Batch 0 | Train Loss: 0.3629698 | Validation Loss: 0.5349695\n",
      "Epoch 648 | Batch 0 | Train Loss: 0.36360684 | Validation Loss: 0.5336833\n",
      "Epoch 649 | Batch 0 | Train Loss: 0.36323157 | Validation Loss: 0.53805405\n",
      "Epoch 650 | Batch 0 | Train Loss: 0.36298257 | Validation Loss: 0.5355138\n",
      "Epoch 651 | Batch 0 | Train Loss: 0.36285603 | Validation Loss: 0.53647566\n",
      "Epoch 652 | Batch 0 | Train Loss: 0.36290383 | Validation Loss: 0.53620726\n",
      "Epoch 653 | Batch 0 | Train Loss: 0.36336663 | Validation Loss: 0.53403926\n",
      "Epoch 654 | Batch 0 | Train Loss: 0.36400893 | Validation Loss: 0.53818923\n",
      "Epoch 655 | Batch 0 | Train Loss: 0.36289406 | Validation Loss: 0.53530926\n",
      "Epoch 656 | Batch 0 | Train Loss: 0.36290428 | Validation Loss: 0.534687\n",
      "Epoch 657 | Batch 0 | Train Loss: 0.36277717 | Validation Loss: 0.53546834\n",
      "Epoch 658 | Batch 0 | Train Loss: 0.3627043 | Validation Loss: 0.5362836\n",
      "Epoch 659 | Batch 0 | Train Loss: 0.3630838 | Validation Loss: 0.537344\n",
      "Epoch 660 | Batch 0 | Train Loss: 0.36266032 | Validation Loss: 0.5374892\n",
      "Epoch 661 | Batch 0 | Train Loss: 0.3630084 | Validation Loss: 0.53706783\n",
      "Epoch 662 | Batch 0 | Train Loss: 0.36289054 | Validation Loss: 0.53416204\n",
      "Epoch 663 | Batch 0 | Train Loss: 0.36292854 | Validation Loss: 0.5348216\n",
      "Epoch 664 | Batch 0 | Train Loss: 0.36341703 | Validation Loss: 0.53353274\n",
      "Epoch 665 | Batch 0 | Train Loss: 0.3630613 | Validation Loss: 0.53581\n",
      "Epoch 666 | Batch 0 | Train Loss: 0.3628999 | Validation Loss: 0.5358508\n",
      "Epoch 667 | Batch 0 | Train Loss: 0.36305472 | Validation Loss: 0.5337993\n",
      "Epoch 668 | Batch 0 | Train Loss: 0.36240715 | Validation Loss: 0.53659976\n",
      "Epoch 669 | Batch 0 | Train Loss: 0.36314926 | Validation Loss: 0.5384152\n",
      "Epoch 670 | Batch 0 | Train Loss: 0.3628653 | Validation Loss: 0.5378089\n",
      "Epoch 671 | Batch 0 | Train Loss: 0.36262268 | Validation Loss: 0.5366477\n",
      "Epoch 672 | Batch 0 | Train Loss: 0.36200428 | Validation Loss: 0.53778785\n",
      "Epoch 673 | Batch 0 | Train Loss: 0.36246264 | Validation Loss: 0.5384056\n",
      "Epoch 674 | Batch 0 | Train Loss: 0.36210954 | Validation Loss: 0.5377161\n",
      "Epoch 675 | Batch 0 | Train Loss: 0.36248395 | Validation Loss: 0.537302\n",
      "Epoch 676 | Batch 0 | Train Loss: 0.36199445 | Validation Loss: 0.53609616\n",
      "Epoch 677 | Batch 0 | Train Loss: 0.36198467 | Validation Loss: 0.537536\n",
      "Epoch 678 | Batch 0 | Train Loss: 0.36242592 | Validation Loss: 0.5375704\n",
      "Epoch 679 | Batch 0 | Train Loss: 0.36294585 | Validation Loss: 0.53435457\n",
      "Epoch 680 | Batch 0 | Train Loss: 0.3619445 | Validation Loss: 0.53857744\n",
      "Epoch 681 | Batch 0 | Train Loss: 0.36210427 | Validation Loss: 0.53876525\n",
      "Epoch 682 | Batch 0 | Train Loss: 0.36162388 | Validation Loss: 0.54207087\n",
      "Epoch 683 | Batch 0 | Train Loss: 0.36219084 | Validation Loss: 0.53890455\n",
      "Epoch 684 | Batch 0 | Train Loss: 0.36235207 | Validation Loss: 0.5408722\n",
      "Epoch 685 | Batch 0 | Train Loss: 0.36208054 | Validation Loss: 0.5381961\n",
      "Epoch 686 | Batch 0 | Train Loss: 0.36189416 | Validation Loss: 0.539619\n",
      "Epoch 687 | Batch 0 | Train Loss: 0.3614885 | Validation Loss: 0.53977937\n",
      "Epoch 688 | Batch 0 | Train Loss: 0.36229074 | Validation Loss: 0.54371184\n",
      "Epoch 689 | Batch 0 | Train Loss: 0.36147133 | Validation Loss: 0.54236174\n",
      "Epoch 690 | Batch 0 | Train Loss: 0.36163414 | Validation Loss: 0.54067266\n",
      "Epoch 691 | Batch 0 | Train Loss: 0.36158523 | Validation Loss: 0.5400071\n",
      "Epoch 692 | Batch 0 | Train Loss: 0.36147332 | Validation Loss: 0.53935313\n",
      "Epoch 693 | Batch 0 | Train Loss: 0.36128402 | Validation Loss: 0.5421938\n",
      "Epoch 694 | Batch 0 | Train Loss: 0.36142692 | Validation Loss: 0.5429287\n",
      "Epoch 695 | Batch 0 | Train Loss: 0.36167467 | Validation Loss: 0.542875\n",
      "Epoch 696 | Batch 0 | Train Loss: 0.36255854 | Validation Loss: 0.54467285\n",
      "Epoch 697 | Batch 0 | Train Loss: 0.3617305 | Validation Loss: 0.5403868\n",
      "Epoch 698 | Batch 0 | Train Loss: 0.36184716 | Validation Loss: 0.5405307\n",
      "Epoch 699 | Batch 0 | Train Loss: 0.3616288 | Validation Loss: 0.5422316\n",
      "Epoch 700 | Batch 0 | Train Loss: 0.36157593 | Validation Loss: 0.5399756\n",
      "Epoch 701 | Batch 0 | Train Loss: 0.3616649 | Validation Loss: 0.53798467\n",
      "Epoch 702 | Batch 0 | Train Loss: 0.36234 | Validation Loss: 0.53594255\n",
      "Epoch 703 | Batch 0 | Train Loss: 0.36198723 | Validation Loss: 0.5423912\n",
      "Epoch 704 | Batch 0 | Train Loss: 0.36187145 | Validation Loss: 0.5424502\n",
      "Epoch 705 | Batch 0 | Train Loss: 0.36138716 | Validation Loss: 0.5385235\n",
      "Epoch 706 | Batch 0 | Train Loss: 0.36163327 | Validation Loss: 0.5390416\n",
      "Epoch 707 | Batch 0 | Train Loss: 0.36137825 | Validation Loss: 0.538279\n",
      "Epoch 708 | Batch 0 | Train Loss: 0.3615254 | Validation Loss: 0.5375692\n",
      "Epoch 709 | Batch 0 | Train Loss: 0.36213455 | Validation Loss: 0.5392692\n",
      "Epoch 710 | Batch 0 | Train Loss: 0.36142212 | Validation Loss: 0.53829134\n",
      "Epoch 711 | Batch 0 | Train Loss: 0.36128056 | Validation Loss: 0.5379268\n",
      "Epoch 712 | Batch 0 | Train Loss: 0.36159447 | Validation Loss: 0.53735447\n",
      "Epoch 713 | Batch 0 | Train Loss: 0.36127117 | Validation Loss: 0.5387995\n",
      "Epoch 714 | Batch 0 | Train Loss: 0.36133072 | Validation Loss: 0.53723216\n",
      "Epoch 715 | Batch 0 | Train Loss: 0.36135048 | Validation Loss: 0.5370219\n",
      "Epoch 716 | Batch 0 | Train Loss: 0.36133903 | Validation Loss: 0.5377824\n",
      "Epoch 717 | Batch 0 | Train Loss: 0.3610033 | Validation Loss: 0.54107904\n",
      "Epoch 718 | Batch 0 | Train Loss: 0.36086735 | Validation Loss: 0.5401304\n",
      "Epoch 719 | Batch 0 | Train Loss: 0.36101463 | Validation Loss: 0.5398408\n",
      "Epoch 720 | Batch 0 | Train Loss: 0.36162293 | Validation Loss: 0.5450399\n",
      "Epoch 721 | Batch 0 | Train Loss: 0.36101833 | Validation Loss: 0.5407145\n",
      "Epoch 722 | Batch 0 | Train Loss: 0.36104065 | Validation Loss: 0.53968656\n",
      "Epoch 723 | Batch 0 | Train Loss: 0.36154854 | Validation Loss: 0.5415377\n",
      "Epoch 724 | Batch 0 | Train Loss: 0.3612487 | Validation Loss: 0.53700143\n",
      "Epoch 725 | Batch 0 | Train Loss: 0.36077732 | Validation Loss: 0.53903735\n",
      "Epoch 726 | Batch 0 | Train Loss: 0.36121646 | Validation Loss: 0.53832823\n",
      "Epoch 727 | Batch 0 | Train Loss: 0.36100608 | Validation Loss: 0.5388503\n",
      "Epoch 728 | Batch 0 | Train Loss: 0.36086333 | Validation Loss: 0.5376229\n",
      "Epoch 729 | Batch 0 | Train Loss: 0.36093438 | Validation Loss: 0.5369135\n",
      "Epoch 730 | Batch 0 | Train Loss: 0.3608734 | Validation Loss: 0.53759\n",
      "Epoch 731 | Batch 0 | Train Loss: 0.36139488 | Validation Loss: 0.5392682\n",
      "Epoch 732 | Batch 0 | Train Loss: 0.36083573 | Validation Loss: 0.54205674\n",
      "Epoch 733 | Batch 0 | Train Loss: 0.3607355 | Validation Loss: 0.53621846\n",
      "Epoch 734 | Batch 0 | Train Loss: 0.36075497 | Validation Loss: 0.538414\n",
      "Epoch 735 | Batch 0 | Train Loss: 0.3609249 | Validation Loss: 0.53834724\n",
      "Epoch 736 | Batch 0 | Train Loss: 0.36197013 | Validation Loss: 0.5370205\n",
      "Epoch 737 | Batch 0 | Train Loss: 0.36090946 | Validation Loss: 0.5385856\n",
      "Epoch 738 | Batch 0 | Train Loss: 0.36089733 | Validation Loss: 0.539937\n",
      "Epoch 739 | Batch 0 | Train Loss: 0.36279893 | Validation Loss: 0.544298\n",
      "Epoch 740 | Batch 0 | Train Loss: 0.3614011 | Validation Loss: 0.54191375\n",
      "Epoch 741 | Batch 0 | Train Loss: 0.36105853 | Validation Loss: 0.54302496\n",
      "Epoch 742 | Batch 0 | Train Loss: 0.36075827 | Validation Loss: 0.5407552\n",
      "Epoch 743 | Batch 0 | Train Loss: 0.36110166 | Validation Loss: 0.5427451\n",
      "Epoch 744 | Batch 0 | Train Loss: 0.3615447 | Validation Loss: 0.5420285\n",
      "Epoch 745 | Batch 0 | Train Loss: 0.36060348 | Validation Loss: 0.5395272\n",
      "Epoch 746 | Batch 0 | Train Loss: 0.36058235 | Validation Loss: 0.53968346\n",
      "Epoch 747 | Batch 0 | Train Loss: 0.36059362 | Validation Loss: 0.5383146\n",
      "Epoch 748 | Batch 0 | Train Loss: 0.36062285 | Validation Loss: 0.5374695\n",
      "Epoch 749 | Batch 0 | Train Loss: 0.3614209 | Validation Loss: 0.5398065\n",
      "Epoch 750 | Batch 0 | Train Loss: 0.36068335 | Validation Loss: 0.53676707\n",
      "Epoch 751 | Batch 0 | Train Loss: 0.3614441 | Validation Loss: 0.53545433\n",
      "Epoch 752 | Batch 0 | Train Loss: 0.36156464 | Validation Loss: 0.5343401\n",
      "Epoch 753 | Batch 0 | Train Loss: 0.3604078 | Validation Loss: 0.5387714\n",
      "Epoch 754 | Batch 0 | Train Loss: 0.3611888 | Validation Loss: 0.5363863\n",
      "Epoch 755 | Batch 0 | Train Loss: 0.36095893 | Validation Loss: 0.5429011\n",
      "Epoch 756 | Batch 0 | Train Loss: 0.36038333 | Validation Loss: 0.5386702\n",
      "Epoch 757 | Batch 0 | Train Loss: 0.36041299 | Validation Loss: 0.5365138\n",
      "Epoch 758 | Batch 0 | Train Loss: 0.36041564 | Validation Loss: 0.53967524\n",
      "Epoch 759 | Batch 0 | Train Loss: 0.3605144 | Validation Loss: 0.53769135\n",
      "Epoch 760 | Batch 0 | Train Loss: 0.36058682 | Validation Loss: 0.53722245\n",
      "Epoch 761 | Batch 0 | Train Loss: 0.36042076 | Validation Loss: 0.5409456\n",
      "Epoch 762 | Batch 0 | Train Loss: 0.36052954 | Validation Loss: 0.54266715\n",
      "Epoch 763 | Batch 0 | Train Loss: 0.36061868 | Validation Loss: 0.5416657\n",
      "Epoch 764 | Batch 0 | Train Loss: 0.36019164 | Validation Loss: 0.5419216\n",
      "Epoch 765 | Batch 0 | Train Loss: 0.36048126 | Validation Loss: 0.5405605\n",
      "Epoch 766 | Batch 0 | Train Loss: 0.36072388 | Validation Loss: 0.5378334\n",
      "Epoch 767 | Batch 0 | Train Loss: 0.36034334 | Validation Loss: 0.54095393\n",
      "Epoch 768 | Batch 0 | Train Loss: 0.36075997 | Validation Loss: 0.5376668\n",
      "Epoch 769 | Batch 0 | Train Loss: 0.36029193 | Validation Loss: 0.5386789\n",
      "Epoch 770 | Batch 0 | Train Loss: 0.36090827 | Validation Loss: 0.54161924\n",
      "Epoch 771 | Batch 0 | Train Loss: 0.36081928 | Validation Loss: 0.5425571\n",
      "Epoch 772 | Batch 0 | Train Loss: 0.3601686 | Validation Loss: 0.53935987\n",
      "Epoch 773 | Batch 0 | Train Loss: 0.36016023 | Validation Loss: 0.53909093\n",
      "Epoch 774 | Batch 0 | Train Loss: 0.36035538 | Validation Loss: 0.5415214\n",
      "Epoch 775 | Batch 0 | Train Loss: 0.36056724 | Validation Loss: 0.54190236\n",
      "Epoch 776 | Batch 0 | Train Loss: 0.36013484 | Validation Loss: 0.5398374\n",
      "Epoch 777 | Batch 0 | Train Loss: 0.360081 | Validation Loss: 0.539047\n",
      "Epoch 778 | Batch 0 | Train Loss: 0.35999608 | Validation Loss: 0.5418612\n",
      "Epoch 779 | Batch 0 | Train Loss: 0.35979518 | Validation Loss: 0.5420273\n",
      "Epoch 780 | Batch 0 | Train Loss: 0.35987264 | Validation Loss: 0.5411762\n",
      "Epoch 781 | Batch 0 | Train Loss: 0.35986054 | Validation Loss: 0.5442575\n",
      "Epoch 782 | Batch 0 | Train Loss: 0.35952628 | Validation Loss: 0.543841\n",
      "Epoch 783 | Batch 0 | Train Loss: 0.35964066 | Validation Loss: 0.54453796\n",
      "Epoch 784 | Batch 0 | Train Loss: 0.3595005 | Validation Loss: 0.5445417\n",
      "Epoch 785 | Batch 0 | Train Loss: 0.3597723 | Validation Loss: 0.54382765\n",
      "Epoch 786 | Batch 0 | Train Loss: 0.3597642 | Validation Loss: 0.5405187\n",
      "Epoch 787 | Batch 0 | Train Loss: 0.3596369 | Validation Loss: 0.5397116\n",
      "Epoch 788 | Batch 0 | Train Loss: 0.3593918 | Validation Loss: 0.54249156\n",
      "Epoch 789 | Batch 0 | Train Loss: 0.35954276 | Validation Loss: 0.5471575\n",
      "Epoch 790 | Batch 0 | Train Loss: 0.35932532 | Validation Loss: 0.5447154\n",
      "Epoch 791 | Batch 0 | Train Loss: 0.35937864 | Validation Loss: 0.5392792\n",
      "Epoch 792 | Batch 0 | Train Loss: 0.3593009 | Validation Loss: 0.5404372\n",
      "Epoch 793 | Batch 0 | Train Loss: 0.3594704 | Validation Loss: 0.54060584\n",
      "Epoch 794 | Batch 0 | Train Loss: 0.360346 | Validation Loss: 0.54454255\n",
      "Epoch 795 | Batch 0 | Train Loss: 0.35966355 | Validation Loss: 0.5399654\n",
      "Epoch 796 | Batch 0 | Train Loss: 0.3596164 | Validation Loss: 0.5399176\n",
      "Epoch 797 | Batch 0 | Train Loss: 0.35964707 | Validation Loss: 0.53848815\n",
      "Epoch 798 | Batch 0 | Train Loss: 0.35960343 | Validation Loss: 0.5390653\n",
      "Epoch 799 | Batch 0 | Train Loss: 0.3594791 | Validation Loss: 0.5390673\n",
      "Epoch 800 | Batch 0 | Train Loss: 0.35959437 | Validation Loss: 0.5397483\n",
      "Epoch 801 | Batch 0 | Train Loss: 0.35922173 | Validation Loss: 0.5443454\n",
      "Epoch 802 | Batch 0 | Train Loss: 0.35926747 | Validation Loss: 0.53986716\n",
      "Epoch 803 | Batch 0 | Train Loss: 0.3593749 | Validation Loss: 0.5387554\n",
      "Epoch 804 | Batch 0 | Train Loss: 0.35903 | Validation Loss: 0.5426704\n",
      "Epoch 805 | Batch 0 | Train Loss: 0.35955828 | Validation Loss: 0.54481506\n",
      "Epoch 806 | Batch 0 | Train Loss: 0.35912144 | Validation Loss: 0.5413112\n",
      "Epoch 807 | Batch 0 | Train Loss: 0.35908753 | Validation Loss: 0.54050237\n",
      "Epoch 808 | Batch 0 | Train Loss: 0.35945693 | Validation Loss: 0.53974205\n",
      "Epoch 809 | Batch 0 | Train Loss: 0.3595873 | Validation Loss: 0.5438083\n",
      "Epoch 810 | Batch 0 | Train Loss: 0.35929593 | Validation Loss: 0.54085344\n",
      "Epoch 811 | Batch 0 | Train Loss: 0.35951233 | Validation Loss: 0.540555\n",
      "Epoch 812 | Batch 0 | Train Loss: 0.35970497 | Validation Loss: 0.5433232\n",
      "Epoch 813 | Batch 0 | Train Loss: 0.35937464 | Validation Loss: 0.5412235\n",
      "Epoch 814 | Batch 0 | Train Loss: 0.35909814 | Validation Loss: 0.5417854\n",
      "Epoch 815 | Batch 0 | Train Loss: 0.35904068 | Validation Loss: 0.5440085\n",
      "Epoch 816 | Batch 0 | Train Loss: 0.3594375 | Validation Loss: 0.5409731\n",
      "Epoch 817 | Batch 0 | Train Loss: 0.35900086 | Validation Loss: 0.5407356\n",
      "Epoch 818 | Batch 0 | Train Loss: 0.358714 | Validation Loss: 0.5436444\n",
      "Epoch 819 | Batch 0 | Train Loss: 0.3594574 | Validation Loss: 0.54326195\n",
      "Epoch 820 | Batch 0 | Train Loss: 0.35880044 | Validation Loss: 0.54128957\n",
      "Epoch 821 | Batch 0 | Train Loss: 0.35852945 | Validation Loss: 0.5406761\n",
      "Epoch 822 | Batch 0 | Train Loss: 0.35852683 | Validation Loss: 0.5404579\n",
      "Epoch 823 | Batch 0 | Train Loss: 0.3583746 | Validation Loss: 0.542483\n",
      "Epoch 824 | Batch 0 | Train Loss: 0.3583893 | Validation Loss: 0.54242295\n",
      "Epoch 825 | Batch 0 | Train Loss: 0.35838422 | Validation Loss: 0.54262245\n",
      "Epoch 826 | Batch 0 | Train Loss: 0.35829628 | Validation Loss: 0.5432394\n",
      "Epoch 827 | Batch 0 | Train Loss: 0.35854176 | Validation Loss: 0.54311126\n",
      "Epoch 828 | Batch 0 | Train Loss: 0.35858995 | Validation Loss: 0.5421375\n",
      "Epoch 829 | Batch 0 | Train Loss: 0.35869065 | Validation Loss: 0.54026335\n",
      "Epoch 830 | Batch 0 | Train Loss: 0.3584944 | Validation Loss: 0.54048795\n",
      "Epoch 831 | Batch 0 | Train Loss: 0.3582847 | Validation Loss: 0.5425734\n",
      "Epoch 832 | Batch 0 | Train Loss: 0.358209 | Validation Loss: 0.54160136\n",
      "Epoch 833 | Batch 0 | Train Loss: 0.35819656 | Validation Loss: 0.54273003\n",
      "Epoch 834 | Batch 0 | Train Loss: 0.35854164 | Validation Loss: 0.54095113\n",
      "Epoch 835 | Batch 0 | Train Loss: 0.35828468 | Validation Loss: 0.53988874\n",
      "Epoch 836 | Batch 0 | Train Loss: 0.35825154 | Validation Loss: 0.5431928\n",
      "Epoch 837 | Batch 0 | Train Loss: 0.3584847 | Validation Loss: 0.5401654\n",
      "Epoch 838 | Batch 0 | Train Loss: 0.3583253 | Validation Loss: 0.5416785\n",
      "Epoch 839 | Batch 0 | Train Loss: 0.35863313 | Validation Loss: 0.5444317\n",
      "Epoch 840 | Batch 0 | Train Loss: 0.359031 | Validation Loss: 0.5395591\n",
      "Epoch 841 | Batch 0 | Train Loss: 0.35832 | Validation Loss: 0.54601705\n",
      "Epoch 842 | Batch 0 | Train Loss: 0.35860384 | Validation Loss: 0.54159606\n",
      "Epoch 843 | Batch 0 | Train Loss: 0.3584382 | Validation Loss: 0.54330707\n",
      "Epoch 844 | Batch 0 | Train Loss: 0.35892364 | Validation Loss: 0.5455673\n",
      "Epoch 845 | Batch 0 | Train Loss: 0.35834655 | Validation Loss: 0.54208887\n",
      "Epoch 846 | Batch 0 | Train Loss: 0.35862863 | Validation Loss: 0.53998953\n",
      "Epoch 847 | Batch 0 | Train Loss: 0.358796 | Validation Loss: 0.5406293\n",
      "Epoch 848 | Batch 0 | Train Loss: 0.35856116 | Validation Loss: 0.54173\n",
      "Epoch 849 | Batch 0 | Train Loss: 0.35822326 | Validation Loss: 0.5441947\n",
      "Epoch 850 | Batch 0 | Train Loss: 0.3582167 | Validation Loss: 0.5422074\n",
      "Epoch 851 | Batch 0 | Train Loss: 0.35807893 | Validation Loss: 0.5452378\n",
      "Epoch 852 | Batch 0 | Train Loss: 0.35857245 | Validation Loss: 0.5469177\n",
      "Epoch 853 | Batch 0 | Train Loss: 0.35847935 | Validation Loss: 0.54436225\n",
      "Epoch 854 | Batch 0 | Train Loss: 0.35829487 | Validation Loss: 0.5430597\n",
      "Epoch 855 | Batch 0 | Train Loss: 0.35880294 | Validation Loss: 0.5448379\n",
      "Epoch 856 | Batch 0 | Train Loss: 0.35807288 | Validation Loss: 0.5445068\n",
      "Epoch 857 | Batch 0 | Train Loss: 0.35851 | Validation Loss: 0.5429576\n",
      "Epoch 858 | Batch 0 | Train Loss: 0.35862556 | Validation Loss: 0.54041284\n",
      "Epoch 859 | Batch 0 | Train Loss: 0.3588007 | Validation Loss: 0.53797317\n",
      "Epoch 860 | Batch 0 | Train Loss: 0.3578759 | Validation Loss: 0.5413892\n",
      "Epoch 861 | Batch 0 | Train Loss: 0.35801154 | Validation Loss: 0.54063535\n",
      "Epoch 862 | Batch 0 | Train Loss: 0.3578648 | Validation Loss: 0.5397327\n",
      "Epoch 863 | Batch 0 | Train Loss: 0.35766265 | Validation Loss: 0.54293084\n",
      "Epoch 864 | Batch 0 | Train Loss: 0.3578905 | Validation Loss: 0.54059994\n",
      "Epoch 865 | Batch 0 | Train Loss: 0.35819274 | Validation Loss: 0.539099\n",
      "Epoch 866 | Batch 0 | Train Loss: 0.3583164 | Validation Loss: 0.5385089\n",
      "Epoch 867 | Batch 0 | Train Loss: 0.3581163 | Validation Loss: 0.5392044\n",
      "Epoch 868 | Batch 0 | Train Loss: 0.35788688 | Validation Loss: 0.5404918\n",
      "Epoch 869 | Batch 0 | Train Loss: 0.3579615 | Validation Loss: 0.54109925\n",
      "Epoch 870 | Batch 0 | Train Loss: 0.35807037 | Validation Loss: 0.5400901\n",
      "Epoch 871 | Batch 0 | Train Loss: 0.35817027 | Validation Loss: 0.540377\n",
      "Epoch 872 | Batch 0 | Train Loss: 0.35766503 | Validation Loss: 0.5437177\n",
      "Epoch 873 | Batch 0 | Train Loss: 0.35791183 | Validation Loss: 0.54480577\n",
      "Epoch 874 | Batch 0 | Train Loss: 0.35756448 | Validation Loss: 0.5428427\n",
      "Epoch 875 | Batch 0 | Train Loss: 0.3576271 | Validation Loss: 0.5460566\n",
      "Epoch 876 | Batch 0 | Train Loss: 0.35728002 | Validation Loss: 0.54425347\n",
      "Epoch 877 | Batch 0 | Train Loss: 0.35768396 | Validation Loss: 0.5393566\n",
      "Epoch 878 | Batch 0 | Train Loss: 0.35776198 | Validation Loss: 0.53902453\n",
      "Epoch 879 | Batch 0 | Train Loss: 0.35759202 | Validation Loss: 0.5394173\n",
      "Epoch 880 | Batch 0 | Train Loss: 0.35809308 | Validation Loss: 0.54101515\n",
      "Epoch 881 | Batch 0 | Train Loss: 0.3577366 | Validation Loss: 0.5409006\n",
      "Epoch 882 | Batch 0 | Train Loss: 0.35729027 | Validation Loss: 0.5395235\n",
      "Epoch 883 | Batch 0 | Train Loss: 0.3574659 | Validation Loss: 0.53913426\n",
      "Epoch 884 | Batch 0 | Train Loss: 0.35800773 | Validation Loss: 0.5425403\n",
      "Epoch 885 | Batch 0 | Train Loss: 0.35747704 | Validation Loss: 0.541109\n",
      "Epoch 886 | Batch 0 | Train Loss: 0.35741675 | Validation Loss: 0.5420572\n",
      "Epoch 887 | Batch 0 | Train Loss: 0.35769364 | Validation Loss: 0.54319876\n",
      "Epoch 888 | Batch 0 | Train Loss: 0.35773614 | Validation Loss: 0.5434853\n",
      "Epoch 889 | Batch 0 | Train Loss: 0.35750502 | Validation Loss: 0.543134\n",
      "Epoch 890 | Batch 0 | Train Loss: 0.35729694 | Validation Loss: 0.54484046\n",
      "Epoch 891 | Batch 0 | Train Loss: 0.35811293 | Validation Loss: 0.5498686\n",
      "Epoch 892 | Batch 0 | Train Loss: 0.3573982 | Validation Loss: 0.54354453\n",
      "Epoch 893 | Batch 0 | Train Loss: 0.35781807 | Validation Loss: 0.5409816\n",
      "Epoch 894 | Batch 0 | Train Loss: 0.35734427 | Validation Loss: 0.54212683\n",
      "Epoch 895 | Batch 0 | Train Loss: 0.3574183 | Validation Loss: 0.54637504\n",
      "Epoch 896 | Batch 0 | Train Loss: 0.35735542 | Validation Loss: 0.54470026\n",
      "Epoch 897 | Batch 0 | Train Loss: 0.357699 | Validation Loss: 0.5459479\n",
      "Epoch 898 | Batch 0 | Train Loss: 0.3576617 | Validation Loss: 0.54962\n",
      "Epoch 899 | Batch 0 | Train Loss: 0.35736597 | Validation Loss: 0.54611415\n",
      "Epoch 900 | Batch 0 | Train Loss: 0.357145 | Validation Loss: 0.5441755\n",
      "Epoch 901 | Batch 0 | Train Loss: 0.35711876 | Validation Loss: 0.541723\n",
      "Epoch 902 | Batch 0 | Train Loss: 0.35704002 | Validation Loss: 0.54259866\n",
      "Epoch 903 | Batch 0 | Train Loss: 0.35730916 | Validation Loss: 0.5428841\n",
      "Epoch 904 | Batch 0 | Train Loss: 0.35696578 | Validation Loss: 0.5440252\n",
      "Epoch 905 | Batch 0 | Train Loss: 0.35716715 | Validation Loss: 0.54252344\n",
      "Epoch 906 | Batch 0 | Train Loss: 0.35713378 | Validation Loss: 0.5420832\n",
      "Epoch 907 | Batch 0 | Train Loss: 0.35758042 | Validation Loss: 0.54040855\n",
      "Epoch 908 | Batch 0 | Train Loss: 0.35721216 | Validation Loss: 0.5438864\n",
      "Epoch 909 | Batch 0 | Train Loss: 0.35717392 | Validation Loss: 0.5437278\n",
      "Epoch 910 | Batch 0 | Train Loss: 0.3571761 | Validation Loss: 0.54540193\n",
      "Epoch 911 | Batch 0 | Train Loss: 0.35702837 | Validation Loss: 0.5428535\n",
      "Epoch 912 | Batch 0 | Train Loss: 0.35689995 | Validation Loss: 0.5437155\n",
      "Epoch 913 | Batch 0 | Train Loss: 0.35713607 | Validation Loss: 0.54337496\n",
      "Epoch 914 | Batch 0 | Train Loss: 0.35715902 | Validation Loss: 0.5434266\n",
      "Epoch 915 | Batch 0 | Train Loss: 0.35779324 | Validation Loss: 0.5458398\n",
      "Epoch 916 | Batch 0 | Train Loss: 0.35759515 | Validation Loss: 0.54743105\n",
      "Epoch 917 | Batch 0 | Train Loss: 0.35699028 | Validation Loss: 0.54548377\n",
      "Epoch 918 | Batch 0 | Train Loss: 0.35734773 | Validation Loss: 0.5459388\n",
      "Epoch 919 | Batch 0 | Train Loss: 0.35724923 | Validation Loss: 0.54495335\n",
      "Epoch 920 | Batch 0 | Train Loss: 0.3571795 | Validation Loss: 0.5437093\n",
      "Epoch 921 | Batch 0 | Train Loss: 0.35711235 | Validation Loss: 0.5437824\n",
      "Epoch 922 | Batch 0 | Train Loss: 0.35677677 | Validation Loss: 0.54613703\n",
      "Epoch 923 | Batch 0 | Train Loss: 0.35706988 | Validation Loss: 0.54434866\n",
      "Epoch 924 | Batch 0 | Train Loss: 0.35683158 | Validation Loss: 0.5459677\n",
      "Epoch 925 | Batch 0 | Train Loss: 0.35680535 | Validation Loss: 0.54749954\n",
      "Epoch 926 | Batch 0 | Train Loss: 0.35702905 | Validation Loss: 0.54642695\n",
      "Epoch 927 | Batch 0 | Train Loss: 0.35679427 | Validation Loss: 0.5441868\n",
      "Epoch 928 | Batch 0 | Train Loss: 0.35642907 | Validation Loss: 0.5452482\n",
      "Epoch 929 | Batch 0 | Train Loss: 0.35686448 | Validation Loss: 0.54539514\n",
      "Epoch 930 | Batch 0 | Train Loss: 0.35711434 | Validation Loss: 0.5427741\n",
      "Epoch 931 | Batch 0 | Train Loss: 0.35681903 | Validation Loss: 0.54514647\n",
      "Epoch 932 | Batch 0 | Train Loss: 0.35718173 | Validation Loss: 0.5476605\n",
      "Epoch 933 | Batch 0 | Train Loss: 0.3571189 | Validation Loss: 0.5468494\n",
      "Epoch 934 | Batch 0 | Train Loss: 0.3575344 | Validation Loss: 0.5497159\n",
      "Epoch 935 | Batch 0 | Train Loss: 0.35686764 | Validation Loss: 0.54727685\n",
      "Epoch 936 | Batch 0 | Train Loss: 0.35646805 | Validation Loss: 0.54635674\n",
      "Epoch 937 | Batch 0 | Train Loss: 0.35648537 | Validation Loss: 0.54584295\n",
      "Epoch 938 | Batch 0 | Train Loss: 0.3564177 | Validation Loss: 0.54548067\n",
      "Epoch 939 | Batch 0 | Train Loss: 0.3565033 | Validation Loss: 0.5456781\n",
      "Epoch 940 | Batch 0 | Train Loss: 0.35643342 | Validation Loss: 0.547674\n",
      "Epoch 941 | Batch 0 | Train Loss: 0.35654917 | Validation Loss: 0.54544973\n",
      "Epoch 942 | Batch 0 | Train Loss: 0.35678813 | Validation Loss: 0.54733473\n",
      "Epoch 943 | Batch 0 | Train Loss: 0.35672146 | Validation Loss: 0.5442435\n",
      "Epoch 944 | Batch 0 | Train Loss: 0.35653496 | Validation Loss: 0.5446488\n",
      "Epoch 945 | Batch 0 | Train Loss: 0.35648388 | Validation Loss: 0.54468846\n",
      "Epoch 946 | Batch 0 | Train Loss: 0.3566768 | Validation Loss: 0.5439838\n",
      "Epoch 947 | Batch 0 | Train Loss: 0.35677448 | Validation Loss: 0.54797393\n",
      "Epoch 948 | Batch 0 | Train Loss: 0.3566986 | Validation Loss: 0.5431902\n",
      "Epoch 949 | Batch 0 | Train Loss: 0.3571636 | Validation Loss: 0.54496133\n",
      "Epoch 950 | Batch 0 | Train Loss: 0.3572933 | Validation Loss: 0.54559904\n",
      "Epoch 951 | Batch 0 | Train Loss: 0.35745552 | Validation Loss: 0.542742\n",
      "Epoch 952 | Batch 0 | Train Loss: 0.35733372 | Validation Loss: 0.5413079\n",
      "Epoch 953 | Batch 0 | Train Loss: 0.35744926 | Validation Loss: 0.5409128\n",
      "Epoch 954 | Batch 0 | Train Loss: 0.3568816 | Validation Loss: 0.542465\n",
      "Epoch 955 | Batch 0 | Train Loss: 0.35648513 | Validation Loss: 0.5477431\n",
      "Epoch 956 | Batch 0 | Train Loss: 0.35649487 | Validation Loss: 0.5440389\n",
      "Epoch 957 | Batch 0 | Train Loss: 0.3563559 | Validation Loss: 0.54257035\n",
      "Epoch 958 | Batch 0 | Train Loss: 0.35642838 | Validation Loss: 0.5453096\n",
      "Epoch 959 | Batch 0 | Train Loss: 0.35665062 | Validation Loss: 0.5456783\n",
      "Epoch 960 | Batch 0 | Train Loss: 0.3565713 | Validation Loss: 0.54803485\n",
      "Epoch 961 | Batch 0 | Train Loss: 0.35664323 | Validation Loss: 0.54994434\n",
      "Epoch 962 | Batch 0 | Train Loss: 0.35652164 | Validation Loss: 0.54932517\n",
      "Epoch 963 | Batch 0 | Train Loss: 0.35621196 | Validation Loss: 0.546044\n",
      "Epoch 964 | Batch 0 | Train Loss: 0.35641944 | Validation Loss: 0.54449\n",
      "Epoch 965 | Batch 0 | Train Loss: 0.35677215 | Validation Loss: 0.5415129\n",
      "Epoch 966 | Batch 0 | Train Loss: 0.35678577 | Validation Loss: 0.54090977\n",
      "Epoch 967 | Batch 0 | Train Loss: 0.35689914 | Validation Loss: 0.5418481\n",
      "Epoch 968 | Batch 0 | Train Loss: 0.35655412 | Validation Loss: 0.5430481\n",
      "Epoch 969 | Batch 0 | Train Loss: 0.35614708 | Validation Loss: 0.54612684\n",
      "Epoch 970 | Batch 0 | Train Loss: 0.35621408 | Validation Loss: 0.5452031\n",
      "Epoch 971 | Batch 0 | Train Loss: 0.3564564 | Validation Loss: 0.54232734\n",
      "Epoch 972 | Batch 0 | Train Loss: 0.35605964 | Validation Loss: 0.5440789\n",
      "Epoch 973 | Batch 0 | Train Loss: 0.35619858 | Validation Loss: 0.54553276\n",
      "Epoch 974 | Batch 0 | Train Loss: 0.35623744 | Validation Loss: 0.54772025\n",
      "Epoch 975 | Batch 0 | Train Loss: 0.35616067 | Validation Loss: 0.54655135\n",
      "Epoch 976 | Batch 0 | Train Loss: 0.3560975 | Validation Loss: 0.54586977\n",
      "Epoch 977 | Batch 0 | Train Loss: 0.35609853 | Validation Loss: 0.5466165\n",
      "Epoch 978 | Batch 0 | Train Loss: 0.35635707 | Validation Loss: 0.5468861\n",
      "Epoch 979 | Batch 0 | Train Loss: 0.35675445 | Validation Loss: 0.54752535\n",
      "Epoch 980 | Batch 0 | Train Loss: 0.35592377 | Validation Loss: 0.5456568\n",
      "Epoch 981 | Batch 0 | Train Loss: 0.35572028 | Validation Loss: 0.54667616\n",
      "Epoch 982 | Batch 0 | Train Loss: 0.356139 | Validation Loss: 0.5514622\n",
      "Epoch 983 | Batch 0 | Train Loss: 0.3557799 | Validation Loss: 0.5481603\n",
      "Epoch 984 | Batch 0 | Train Loss: 0.35579842 | Validation Loss: 0.5458335\n",
      "Epoch 985 | Batch 0 | Train Loss: 0.3561261 | Validation Loss: 0.5435803\n",
      "Epoch 986 | Batch 0 | Train Loss: 0.3558983 | Validation Loss: 0.54767805\n",
      "Epoch 987 | Batch 0 | Train Loss: 0.35612786 | Validation Loss: 0.54546946\n",
      "Epoch 988 | Batch 0 | Train Loss: 0.35600555 | Validation Loss: 0.54653186\n",
      "Epoch 989 | Batch 0 | Train Loss: 0.3561242 | Validation Loss: 0.5439056\n",
      "Epoch 990 | Batch 0 | Train Loss: 0.35624474 | Validation Loss: 0.54432034\n",
      "Epoch 991 | Batch 0 | Train Loss: 0.35618424 | Validation Loss: 0.54682076\n",
      "Epoch 992 | Batch 0 | Train Loss: 0.35590214 | Validation Loss: 0.5467864\n",
      "Epoch 993 | Batch 0 | Train Loss: 0.35622206 | Validation Loss: 0.5447294\n",
      "Epoch 994 | Batch 0 | Train Loss: 0.35589385 | Validation Loss: 0.54618806\n",
      "Epoch 995 | Batch 0 | Train Loss: 0.3564142 | Validation Loss: 0.5483366\n",
      "Epoch 996 | Batch 0 | Train Loss: 0.35589767 | Validation Loss: 0.5461056\n",
      "Epoch 997 | Batch 0 | Train Loss: 0.35613587 | Validation Loss: 0.54615295\n",
      "Epoch 998 | Batch 0 | Train Loss: 0.35600555 | Validation Loss: 0.5509719\n",
      "Epoch 999 | Batch 0 | Train Loss: 0.3558779 | Validation Loss: 0.5482249\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch = 1000\n",
    "interval = 100\n",
    "batch_size = 10\n",
    "n_batches = train_X.shape[0]//batch_size\n",
    "\n",
    "# Layer's sizes\n",
    "x_size = train_X.shape[1]   # Number of input nodes: 3 features and 1 bias\n",
    "h_size = 30                 # Number of hidden nodes\n",
    "y_size = train_y.shape[1]   # Number of outcomes (8 classes)\n",
    "\n",
    "# Symbols\n",
    "X = tf.placeholder(tf.float32, shape=[None, x_size])\n",
    "y = tf.placeholder(tf.float32, shape=[None, y_size])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weight initializations\n",
    "w1 = tf.Variable(tf.random_normal(shape=(x_size, h_size)))\n",
    "b1 = tf.Variable(tf.random_normal(shape=[h_size]))\n",
    "w2 = tf.Variable(tf.random_normal(shape=(h_size, y_size)))\n",
    "b2 = tf.Variable(tf.random_normal(shape=[y_size]))\n",
    "\n",
    "# Operations\n",
    "hidden_output = tf.nn.sigmoid(tf.add(tf.matmul(X, w1), b1))\n",
    "dropout = tf.nn.dropout(hidden_output, keep_prob)\n",
    "#final_output = tf.nn.softmax(tf.add(tf.matmul(dropout, w2), b2), name='final_output')\n",
    "final_output = tf.nn.sigmoid(tf.add(tf.matmul(dropout, w2), b2), name='final_output')\n",
    "\n",
    "# Cost Function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(final_output), axis=0))\n",
    "loss = tf.losses.log_loss(y,final_output)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Run SGD\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training\n",
    "print('Training the model...')\n",
    "losses = {'train':[], 'validation':[]}\n",
    "\n",
    "for e in range(epoch):\n",
    "    idxs = np.random.permutation(train_X.shape[0]) #shuffled ordering\n",
    "    random_X = train_X[idxs]\n",
    "    random_y = train_y[idxs]\n",
    "    for i in range(n_batches):\n",
    "        batch_X = random_X[i * batch_size:(i+1) * batch_size]\n",
    "        batch_y = random_y[i * batch_size:(i+1) * batch_size]\n",
    "        sess.run(optimizer,feed_dict = {X: batch_X, y:batch_y, keep_prob:0.8})\n",
    "        \n",
    "        if i % interval == 0:\n",
    "            train_loss = sess.run(loss, feed_dict={X: train_X, y: train_y, keep_prob:1})\n",
    "            val_loss   = sess.run(loss, feed_dict={X: val_X, y: val_y, keep_prob:1})\n",
    "            print('Epoch', e, '|',\n",
    "                  'Batch', i, '|',\n",
    "                  'Train Loss:', train_loss , '|',\n",
    "                  'Validation Loss:', val_loss)\n",
    "            losses['train'].append(train_loss)\n",
    "            losses['validation'].append(val_loss)\n",
    "\n",
    "#save_path = saver.save(sess, './model/my_test_model',global_step=1000)\n",
    "#print(\"Model saved in path: %s\" % save_path)\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAusAAAH0CAYAAACEkWPuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7bf1d35b00>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 373
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7993527508090615\n",
      "Confusion Matrix:\n",
      "[[163  19]\n",
      " [ 43  84]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "success = 0\n",
    "results = {'actual':[], 'predicted':[]}\n",
    "for i in range(len(test_X)):\n",
    "    predicted = sess.run(final_output, feed_dict={X: [test_X[i]], keep_prob:1})\n",
    "    results['actual'].append(test_y[i])\n",
    "    if (test_y[i][0] > threshold and predicted[0][0] > threshold) or (test_y[i][0] < threshold and predicted[0][0] < threshold):\n",
    "        success += 1\n",
    "        results['predicted'].append(test_y[i][0])\n",
    "    else:\n",
    "        results['predicted'].append(not test_y[i][0])\n",
    "    #print('Actual:', test_y[i][0], 'Predicted:', predicted[0][0])\n",
    "    #print('actual:',test_y[i],' | predicted:', predicted)\n",
    "    #print(predicted[0][0])\n",
    "\n",
    "print('Accuracy:', success/len(test_X))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(results['actual'], results['predicted']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
